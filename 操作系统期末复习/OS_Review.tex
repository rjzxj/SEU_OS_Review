% ctexbook专门处理中文排版
\documentclass[12pt, a4paper, oneside]{ctexbook}
% 所需要的包（大部分用于数字符号）
\usepackage{amsmath, amsthm, amssymb, bm, graphicx, titletoc, hyperref, titlesec, mathrsfs, enumitem, verbatim, listings, xcolor, tcolorbox, minted, fancyhdr, inputenc, geometry, setspace, titling, times, booktabs, etoolbox, fontspec, subcaption, tikz, multirow, pgfgantt, adjustbox, ulem}
\usepackage{enumitem} % 自定义编号格式
\setmainfont{Times New Roman}
\geometry{left=3cm, right=3cm, top=3cm, bottom=3cm}
\setlength{\parindent}{2em}
% 数学相关：
% amsmath：高级数学公式;amsthm：定理环境;amssymb：扩展数学符号;bm：粗体数学符号（\bm{}）;mathrsfs：花体字母（\mathscr{}）
% 其他工具：
% graphicx：插图功能;hyperref：超链接支持（自动添加PDF书签）
% 标题、作者、日期、行距（可以理解为全局变量）
% \linespread{1.5}
% \titleformat{\chapter}[block]
%   {\centering\normalfont\huge\bfseries}
%   {Chapter \thechapter}
%   {1em}
%   {}
\renewcommand\thesection{\arabic{section}}
% subsection 和 subsubsection 保持阿拉伯数字形式
\renewcommand\thesubsection{\arabic{section}.\arabic{subsection}}
\renewcommand\thesubsubsection{\arabic{section}.\arabic{subsection}.\arabic{subsubsection}}
\newtheorem{theorem}{定理}[section]
\newtheorem{definition}[theorem]{定义}
\newtheorem{lemma}[theorem]{引理}
\newtheorem{corollary}[theorem]{推论}
\newtheorem{example}{例}
\newtheorem{proposition}[theorem]{命题}
\newtheorem*{vs}{辨析}
\newtheorem*{pt}{注意}
\newtheorem{exam}{考点}
\newtheorem*{pro}{证明}
\newtheorem*{ext}{拓展}
\renewcommand{\theFancyVerbLine}{\normalsize\textbf{\textcolor{black}{\arabic{FancyVerbLine}}}}
\pagestyle{fancy}      % 启用 fancyhdr 页眉页脚样式
\fancyfoot[C]{\thepage} 
\renewcommand{\headrulewidth}{0.4pt} % 设置页眉横线宽度
\patchcmd{\tableofcontents}{\chapter*}{\section*}{}{}
\makeatother
% 共享计数器：所有环境使用theorem计数器（定义、引理等与定理连续编号）；按章节重置：[section]表示在每个章节内重新编号（如"定理1.1"、"定理2.1"
% 设置章节编号深度到subsection（允许显示三级编号）
\setcounter{secnumdepth}{3}
% 设置目录显示深度到subsection（将三级标题写入目录）
\setcounter{tocdepth}{3}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\lstset{basicstyle=\small, breaklines=true, numbers=left, numberstyle=\small\color{mygray}, keywordstyle=\color{blue}, commentstyle=\color{mygreen}, frame=shadowbox, rulesepcolor=\color{black}, stringstyle=\color{mymauve}, escapeinside={(*@}{@*)}, xleftmargin=2em, xrightmargin=2em, aboveskip=0em,}
\begin{comment}
\begin{enumerate}[label=（\arabic*）, leftmargin=3em] % 调整左侧缩进
\item 
\item 
\item 
\item 
\end{enumerate}
\end{comment}
\begin{comment}
\begin{lstlisting}[language=C]
/* thread.c */
#include <stdio.h>
#include <stdlib.h>
#include <common.h>

volatile int counter = 0;
int loops;

void *worker(void *arg) {
 int i;
 for (i = 0; i < loops; i++) {
 counter++;
 }
 return NULL;
}
\end{lstlisting}
\end{comment}
\title{{\Huge{\textbf{操作系统期末复习}}}}
\author{Yilun Zhu}
\date{\today}
% 这里才是文档开始
\begin{document}
		% 生成标题
	\maketitle
	% 标记每一页的页码
	\thispagestyle{empty}
	\newpage
	    \begin{center}
	        {\fontsize{18pt}{16pt}\selectfont \textbf{前言}}
	    \end{center}

本复习资料为针对东南大学操作系统课程的复习资料，仅用于个人在期末考试中的整理复习，几乎涵盖了董恺老师上课提到的所有内容，并加以自己的理解进行的整理。

希望本复习资料能够帮助还未开始“预习”的大家在期末考试的一两天时间内掌握OS的基本内容，祝每一位看到这份资料的小伙伴们都能在期末中取得好成绩！

修订日志：
\begin{itemize}
\item 2025年6月15日：由于时间原因，主播来不及整理Chapter 2、Chapter 8、和Chapter 13的内容了，有机会的话后续会添加。
\end{itemize}

\begin{flushright}
\today
\end{flushright}


        \newpage 
        \thispagestyle{empty}
    \begin{center}
        {\fontsize{18pt}{16pt}\selectfont \textbf{考试注意事项}}
    \end{center}
    \begin{enumerate}[label=\arabic*., leftmargin=3em]{\fontsize{18pt}{13pt}\selectfont}
        \item 答题的时候，遇到要罗列的重要知识点，可以先用中文把关键字写出来。
        \item 考试一定要带尺，可能要画表格。
        \item 考试范围是课本（Operating System Concepts）和PPT的交集，OSTEP的一些内容也是不考的。
        \item 每周三上午去办公室找dk是能够答疑的。
        \item 平时20分、实验20分，期末60分；但是前面的40分不会大部分学生都给满（但是基本上都会给满，也就是40和39.5的区别）
        \item 期末考试千万不能乱考。
        \item 操作系统这个课很抽象，概念很多。
       	\item 判断题20个，每个1分，这个部分会很坑，很多人会错将近一半。董恺认为讲的绝对的一般都是错的，讲的不绝对的一般也都是对的。
       	\item 简答题30分，6个5分题，没有题库，那个非常大（这里出现的内容会相对重要一些，是有机会满分的，不可能有概念完全没听过）
       	\item 注意，简答题考的内容可能不会纯概念，会考一些问答题本来应该出的题目。（mutex等）
       	\item 最后的问答题的题型一定都见过，但是考的不会这么简单。
       	\item 第一题是Process/Thread API，简单来说会问一共创建了多少个进程，考的复杂一点就会把进程和线程搞在一块考。
       	\item 第二题是CPU Scheduling，历史上大家都是满分的。
       	\item 第三题是Deadlock Avoidance，类似银行家算法。
       	\item 第四题可能有变化，一定和page相关，可能考paging（比较复杂，董恺喜欢考，多级页表），也可能考replacement（相对简单），不过董恺不太会出卷。
       	\item 第五题是Semaphore，也不用完全把信号量小书都刷掉，确保initialization写对，后面写点东西，拿个7，8分什么的没那么困难。
       	
    \end{enumerate}
	\newpage
	% 罗马数字
	\pagenumbering{roman}
	\setcounter{page}{1}
	\tableofcontents

        \newpage
        \setcounter{page}{1}
        \pagenumbering{arabic}
        \chapter{Introduction}
		\section{In-class Contents}
                \subsection{What Operating Systems Do}
                \subsubsection{What is an Operating System}
                \begin{definition}
                    Program：是计算机硬件和用户中间的一个软件。
                \end{definition}
        
                Opearing system的目标：
                \begin{enumerate}[label=（\arabic*）, leftmargin=3em]
                    \item 运行用户程序并且让解决用户问题变得更加简单
                    \item 让计算机系统变得容易使用
                    \item 用一个有效的方法去使用计算机硬件
                \end{enumerate}
                
                所以说，我们也可以用easy,convenience和efficiency来评价一个OS的优劣。
                
                \subsubsection{Computer System Structure}
                计算机系统可以被分为以下四个组成部分：
                \begin{enumerate}[label=（\arabic*）, leftmargin=3em]
                \item 硬件：CPU、内存、I/O
                \item OS：作为一个中间媒介来控制和管理硬件的使用，来实现软件的功能
                \item 应用程序：决定解决用户计算问题的方法（compiler解决代码问题、web browsers解决上网问题）
                \item Users：用户、机器和其他电脑
                \end{enumerate}
                
                现在的计算机系统架构一般都支持multi-user和multi-task。
                
                现在，我们将从两个角度来看操作系统，一个是用户的角度，一个是系统的角度。
                
                \subsubsection{User View}
                用户想要的是easy,convenience和good performance（efficiency），并不在意资源的利用率。
                
                \textbf{下面这段内容知道了解也就可以了。}
                
                但实际上吧，资源的分配会很大程度的影响用户的体验，比如说共享的电脑需要分配好资源让所有用户都高兴；专用系统的用户（如工作站）通常有独立的资源，但也会使用服务器的共享资源；手持设备资源有限，操作系统优化重点在于可用性和电池寿命；有些计算机几乎没有用户界面，比如嵌入式系统。
                
                \subsubsection{System View}
                \textbf{注意以下的内容very important。}
             	
             	从操作系统的角度来看：
\begin{exam}
      OS是一个资源分配器，它分配所有的资源，在高效和公平使用资源的相互冲突的请求之间做出决定。与此同时，OS是一个控制程序，控制程序的执行，以防止错误和不当使用计算机。
      
      英文版本：
      
      OS is a resource allocator, it manages all resources and decides between conflicting requests for efficient and fair resource use. Besides, OS is a control program, it controls execution of programs to prevent errors and improper use of the computer.
\end{exam}
        
        最后呢，操作系统没有明确的被广泛接受的定义，Microsoft给出的定义是“当您订购操作系统时，供应商提供的所有产品”，但这是不对的。还有一种定义是“计算机上始终运行的一个程序”，这也是内核的定义，表明了其他一切东西都是系统程序（OS自带）或者应用程序。
        
        \subsubsection{Computer-System Organization}
        \begin{definition}
        Bootstrap program（引导程序）：开机或启动的时候被加载，一般存在ROM或者EPROM中，被称为固件，它可以初始化系统的所有东西，并且加在OS Kernel并开始执行。
        \end{definition}
        
        加载OS kernel的步骤是，首先通过运行Bootstrap program，来加载、启动存在硬盘或者外存的Bootstrap loader，再将OS kernel加载到内存当中，最后把控制权交给OS kernel。
        
        \subsubsection{Von Neumann Model}
        冯诺依曼架构其实就是说了两件事情：
        \begin{enumerate}[label=（\arabic*）, leftmargin=3em]
        \item 一个计算机系统是如何组成的：一个或多个CPU、设备控制器通过公共总线连接，提供对共享内存的访问。
        \item 一个计算机系统是如何运行的：CPU和设备的并发执行竞争内存周期。
        \end{enumerate}
        
        \subsubsection{Computer-System Operation}
        \begin{pt}
        I/O和CPU是可以并发执行的，什么意思呢，就是I/O和CPU可以同时进行不同的任务，如A任务在用CPU，这个时候B任务就可以I/O。
        \end{pt}
        
        那么I/O和CPU是怎么做到并发执行的呢？
        \begin{enumerate}[label=（\arabic*）, leftmargin=3em]
        \item 每个设备控制器负责特定的设备类型。
        
        \item 每个设备控制器都有一个本地缓冲区。
        
        \item CPU将数据从主存储器移动到本地缓冲区。
        
        \item I/O从设备到控制器的本地缓冲区。
        \end{enumerate}
        
        诶，你看是不是大家都不冲突呀（狗头）
        
        好，那现在CPU和I/O可以实现数据沟通了，但是如何合作呢？
        
        Device controller通过\textbf{interrupt}告诉CPU已经完成操作了，你可以回去干活了。
        
        \begin{example}
        假设你让洗衣机洗衣服（这个动作就像CPU把任务交给设备控制器）。你不会一直站在旁边盯着洗衣机（这叫轮询）。相反，洗衣机会在洗完后响铃通知你（这就是中断）。你听到响铃（中断信号），就知道可以去处理下一步工作了。
        \end{example}
        
        \begin{definition}
        Interrupt：中断指的是处理器的输入信号，表示需要立即关注的事件，然后由OS来处理interrupt。
        \end{definition}
        
        那么当一个中断发生的时候，通常会把CPU的控制权交给interrupt service routine，也就是中断服务，通过查找中断向量表（也就是存储每个中端服务地址的表），让CPU去处理一下这个东西。
        
        同时，中断机制必须存储被中断的指令的地址，就像打游戏存了一个副本，之后要找到我存的副本的位置。
        
        \begin{pt}
        Service rountines都存在memory当中，同时中断服务和被interrupt的指令的地址都会被记录。
        \end{pt}
        
        另外，还有两种特殊的interrupt，分别是trap和exception，他们是由软件产生的，一般由错误（除以0）或者是用户需求导致。
        
        \begin{exam}
        An operating system is interrupt driven.（interrupt一般由external devices引起，注意CPU和memory不是external devices，但是GPU是；当然interrupt也可以是软件引起的）
        \end{exam}
        
        怎么去理解这句话呢？其实也很简单，我们举一个例子：
        \begin{example}
        如果我们什么都不做，OS会做什么呢？
        
        答：Just wait and do nothing.
        \end{example}
        
 		那我们会怎么解决这些Interrupts呢？
 		
 		首先OS会保存寄存器和PC的状态来记录中断时CPU的状态，然后通过轮询（效率低）或者中断向量系统（就是中断向量表）来处理traps和exceptions，同时对于每种interrupt都会有一个单独的代码段来决定要采取什么行动。
 		
 	\textbf{以下内容知道就行了。}
 		\begin{pt}
 		中断也是有priority的，低priority的要给高priority的让路；如果两个都是high priority，要借助scause。
 		\end{pt}
 		
 		\subsubsection{Storage Structure}
 		\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
 		\item Main memory：是CPU唯一可以直接访问的较大的存储媒介，random access（不用像tape那样traverse），但一般是易失的。
 		\item Secondary storage（辅存，例如disk）：是main memory的外延，并且提供较大的非易失的存储空间。
 		
 		Hard disks：上面是磁道和扇区，由disk controller管理device和computer之间的联系。
 		
 		Solid-state disks（固态硬盘）：比hard disks要快，而且是非易失的，也越来越受欢迎（虽然考试不考）。
 		\end{enumerate}
 		
 		\subsubsection{I/O Structure}
 		如果我们没有interrupts的话，当要做一个I/O的时候，CPU必须要等I/O完成，这样的效率是很低的，因为CPU需要不断占用内存来查看I/O是不是完成了。而且同时只能处理一个I/O请求。
 		
 		所以我们更多采取Interrupt Driven的I/O structure。
 		
 		啥意思呢？说白了就是、当用户程序发起 I/O 操作后，操作系统会将具体的 I/O 请求交由设备控制器执行，并将 CPU 控制权还给用户程序，使其可以继续执行其他任务而无需等待 I/O 完成。这种机制提高了资源利用率。当 I/O 操作完成时，设备控制器会通过中断通知 CPU，此时控制权转交给操作系统的中断处理程序。操作系统处理完 I/O 相关事务后，可能恢复原来的用户程序，也可能调度其他就绪进程。总体而言，用户程序在正常执行时持有 CPU 控制权，而在 I/O 完成、系统调用或异常等情况下，操作系统接管控制权以执行必要的管理任务。
 		
 		\begin{definition}
 		Device Driver：一个软件，为device controller和kernel提供了一个独有的接口。
 		\end{definition}
 		
 		那么我们如何告诉OS说我要执行I/O了呢（读文件、写数据、打印）？
 		
 		通过System call。
 		
 		\begin{example}
当用户程序需要进行读文件操作时，会调用系统调用 read() 向操作系统发起 I/O 请求。操作系统接收到请求后，会检查设备状态，确保当前可以执行 I/O 操作。随后，设备驱动程序（Device Driver）会向对应的设备控制器（Device Controller）发送指令，启动读操作。在 I/O 操作进行过程中，CPU 的控制权通常会交还给用户程序或调度其他进程继续执行，以提高效率。当 I/O 操作完成后，设备控制器会通过中断机制通知操作系统。CPU 响应中断，转入内核态，由操作系统中的中断处理程序（通常由设备驱动提供）完成后续处理，如读取数据、更新状态或唤醒阻塞的进程。
 		\end{example}
 		
 		\begin{pt}
 		system call和procedure call是不一样的，syscall比如read()，write()，而procedure call是函数调用，比如说def rgl()。
 		\end{pt}
 		
 		我们前面说到OS要查看设备的状态，这个东西被称为Device-status table，其中包含设备类型、设备地址和当前的状态。同时看OS找到I/O状态表的索引去决定device的状态，也可以加入中断标志。
 		
 		\begin{pt}
 		反正就注意一个事情，返回控制权的是Device Driver不是Device Controller。
 		\end{pt}
 		
 		\begin{definition}
 		DMA(Direct Memory Access)：核心思想就是disk和memory通过Device Controller直接交换数据，这个过程当中一般用网卡，不用CPU。
 		\end{definition}
 		
 		在这个过程中，传输每个数据块才产生一个中断，不会每个字节都中断一
 		下。
 		
 		目的：提高数据传输率，加速CPU计算速度，保证设备与CPU之间的高效交互。
 		
 		\subsection{Computer-System Architecture}
 		在过去，许多系统用的还是单通用处理器，当然也会有特殊目的的处理器，但现在流行用多处理器，被称为并行系统或者是紧密耦合系统。
 		
 		这样做有什么好处呢？（有点类似RAID）
 		\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
 		\item 增加吞吐量
 		\item 规模经济：处理器共享资源->降低硬件成本
 		\item 更加可靠：一个处理器故障，其他的好的就行
 		\end{enumerate}
 		
 		有两种类型的multiprocessing：
 		\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
 		\item Asymmetric Multiprocessing（非对称）：由一个主处理器来进行调度和任务分配，而其他处理器都执行\textbf{特定的}任务。
 		\item Symmetric Multiprocessing（对称，more popular）：每个processor做的事情都是一样的。
 		\end{enumerate}
 		
 		现在我们关注另一个话题，为啥那么multicore（一个物理芯片多个核心）要比multichip好？
 		
 		因为在一个芯片上的communication会更快，用更少的能量。
 		
 		\begin{definition}
 		Blade Servers（刀片服务器）：将多个计算单元集成到一个机箱上面，每个刀片就相当于一个独立的服务器，有自己的CPU、内存和存储资源，同时多个server共享电源和散热系统，来减少能源消耗。 
 		\end{definition}
 		
 		\subsection{Operating-System Structure}
 		\textbf{这块是重点了兄弟，好好复习啊！}
 		\begin{definition}
 		Batch system（批处理系统）：处理大量的数据，执行过程完全自动化，不需要人工干预，也就是说，只要用户提交任务，系统就能按照预定的顺序去执行作业。
 		\end{definition}
 		\subsubsection{Multiprogramming}
 		中文名：多道程序设计
 		
 		核心思想：让CPU和I/O都忙起来
 		
 		啥意思呢？就是在内存当中放了一系列的jobs，CPU在执行一个作业的时候，其他作业可以进行I/O操作。那我怎么选取一个job呢？这就和CPU Scheduling有关了，具体看第五章，这里我们用的是\textbf{long term scheduling}。
 		
 		\subsubsection{Timesharing}
 		别名：Multitasking
 		
 		中文名：分时系统
 		
 		核心思想：OS虽然只能同时服务一个user，完成一个job，但是OS可以在jobs之间自由来去，让每个program都感觉自己独占了CPU，也就实现了\textbf{交互式计算}。
 		
 		那么怎么实现分时系统呢？
 		\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
 		\item 第一个指令的响应时间要在1s以内
 		\item 每个用户至少有一个program在内存中运行，那么在运行的program就被称为process啦（打开word和运行Edge就是两个process）
 		\item 如果有许多jobs想要一起运行->CPU scheduling，这个是由\textbf{short term schduling}来决定的（可以让更快的先做或者让更紧急的先做，比如说游戏优先级高，所以用更多的CPU时间片，而后台下载任务的优先级比较低，所以占用的CPU资源应该更少）。
 		\item 如果进程放不进内存来，那就要靠swapping了
 		\item 虚拟内存是操作系统的一项重要技术，它使得程序可以运行，即使它们并没有完全被加载到物理内存中。操作系统通过将部分程序代码或数据保存在硬盘上，并使用内存管理技术（如分页），从而允许程序超出物理内存限制运行。
 		\end{enumerate}
 		
 		\begin{vs}
 		比较一下Long-term scheduling和Short-term scheduling：
 		
 		长时间调度负责决定哪些作业可以进入内存并准备执行；短时间调度负责决定 CPU 将执行哪个作业。
 		\end{vs}
 		
 		\begin{vs}
 		再比较一下multiprogramming和timesharing：
 		
 		多道程序设计（Multiprogramming）更关注提高CPU的利用率，通过加载多个作业到内存中，在作业等待 I/O 操作时切换任务，确保 CPU 始终有任务执行。
 		
 		分时系统（Timesharing）则更关注用户交互和响应时间，通过非常快速的任务切换，使得多个用户或任务可以并发执行，并且每个任务能够得到快速反馈。
 		\end{vs}
 		
 		\subsection{Operating-System Operations}
 		\begin{definition}
 		Interrupt Driven：分为hardware interrupt（比如键盘输入、打印机完成任务，这种情况是不用等程序结束就能中断的）；还有一种是software interrupt(exception or trap)，比如说除以0，或者是请求OS服务，还包括无限循环等问题。
 		\end{definition}
 		
 		\subsubsection{Dual Mode}
 		\begin{definition}
 		Dual-mode（双模式）：指的是User mode和kernel mode双模式，用来让OS保护自己和其他系统组件。
 		\end{definition}
 		
 		\begin{ext}
 		其实现代的OS是有四个模式的，除了以上两个还有VMM(virtual machine manager)，用来管理不同的OS（虚拟机），还有一个monitor（用作security,比如人脸识别）
 		\end{ext}
 		
 		与此同时，硬件还提供了一个mode bit，来表示现在是user mode(0)还是kernel mode(1)。
 		
 		那“保护”是如何体现的呢？一些指令是特权指令，是只能在kernel mode才能运行的指令，比如说set the time。
 		
 		那如果我想要调用一些在kernel mode下才能使用的特权指令呢？啊哈，这个时候就要用到\textbf{System call}啦！它能够把模式切到kernel mode，并且在访问内核结束之后把mode切回去。
 		
 		\subsubsection{Timer}
 		\begin{definition}
 		Timer（定时器）：防止无限循环或进程占用太多的资源。
 		\end{definition}
 		
 		核心思想：定时器保持一个由物理时钟递减的计数器。每当计时器减到零时，触发中断。
 		
 		操作系统会通过设置这个计数器来控制中断的时机，这通常是一个需要特殊权限的操作（即特权指令）。
 		
 		\subsection{Operating-System Functions}
 		在学OS的时候要知道一些关键的idea：
 		\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
 		\item Virtualization：CPU虚拟化（让每个program都觉得自己是在独占CPU而不是共享CPU）、虚存（让program觉得自己有足够的内存）
 		\item Concurrency（并发性）：同时解决一些问题来加速计算，但需要注意可能出现的问题（也就是同步问题等）
 		\item Persistence：保证数据不会丢失
 		\end{enumerate}
 		
 		\newpage
 		\section{Exercise}
 		
 		Consider a multiprogramming environment, two programs $A$ and $B$ share the system simultaneously, and run as follows. 
 		\begin{itemize}
 		 		    \item For every 35 min, A runs on CPU for 15 min, then waits for I/O for 20 min.
 		 		    \item For every 20 min, B runs on CPU for 10 min, then waits for I/O for 10 min.
 		 		\end{itemize}
 		 \begin{enumerate}[label=（\arabic*）, leftmargin=3em]
 		 \item Suppose $B$ runs first, I/O$_A$ and I/O$_B$ are different devices, and the time of switching between $A$ and $B$ can be ignored. Draw the timeline for these two programs, and calculate CPU utilization within 60 minutes. 
 		 		
 		\item What if I/O$_A$ and I/O$_B$ are the same device?
 		\end{enumerate}
 		
 		Keys：
 		\begin{center}
 		\scriptsize
 		Key to Q1: $CPU\ utilization = 50/60 = 83.33\%$\\
 		v1:
 		\begin{adjustbox}{scale={0.8}}
 		\begin{ganttchart}[hgrid, vgrid, inline, x unit=3mm, y unit chart=.8cm, y unit title=0.5cm, include title in canvas=false, bar top shift=0, bar height=1, ]{1}{60} 
 		\ganttgroup{01------05}{01}{05}
 		\ganttgroup{06------10}{06}{10}
 		\ganttgroup{11------15}{11}{15}
 		\ganttgroup{16------20}{16}{20}
 		\ganttgroup{21------25}{21}{25}
 		\ganttgroup{26------30}{26}{30}
 		\ganttgroup{31------35}{31}{35}
 		\ganttgroup{36------40}{36}{40}
 		\ganttgroup{41------45}{41}{45}
 		\ganttgroup{46------50}{46}{50}
 		\ganttgroup{51------55}{51}{55}
 		\ganttgroup{56------60}{56}{60}\\
 		\ganttbar{A}{11}{25}
 		\ganttbar{A}{56}{60}\\
 		\ganttbar{B}{1}{10}
 		\ganttbar{B}{26}{35}
 		\ganttbar{B}{46}{55}\\
 		\ganttbar{I/O$_A$}{26}{45}\\
 		\ganttbar{I/O$_B$}{11}{20}
 		\ganttbar{I/O$_B$}{36}{45}
 		\ganttbar{I/O$_B$}{56}{60}
 		\end{ganttchart}
 		\end{adjustbox}
 		\\
 		v2:
 		\begin{adjustbox}{scale={0.8}}
 		\begin{ganttchart}[hgrid, vgrid, inline, x unit=3mm, y unit chart=.8cm, y unit title=0.5cm, include title in canvas=false, bar top shift=0, bar height=1, ]{1}{60} 
 		\ganttgroup{01------05}{01}{05}
 		\ganttgroup{06------10}{06}{10}
 		\ganttgroup{11------15}{11}{15}
 		\ganttgroup{16------20}{16}{20}
 		\ganttgroup{21------25}{21}{25}
 		\ganttgroup{26------30}{26}{30}
 		\ganttgroup{31------35}{31}{35}
 		\ganttgroup{36------40}{36}{40}
 		\ganttgroup{41------45}{41}{45}
 		\ganttgroup{46------50}{46}{50}
 		\ganttgroup{51------55}{51}{55}
 		\ganttgroup{56------60}{56}{60}\\
 		\ganttbar{A}{11}{25}
 		\ganttbar{A}{46}{60}\\
 		\ganttbar{B}{1}{10}
 		\ganttbar{B}{26}{35}\\
 		\ganttbar{I/O$_A$}{26}{45}\\
 		\ganttbar{I/O$_B$}{11}{20}
 		\ganttbar{I/O$_B$}{36}{45}
 		\end{ganttchart}
 		\end{adjustbox}
 		\\
 		Key to Q2: $CPU\ utilization = 50/60 = 83.33\%$\\
 		\begin{adjustbox}{scale={0.8}}
 		\begin{ganttchart}[hgrid, vgrid, inline, x unit=3mm, y unit chart=.8cm, y unit title=0.5cm, include title in canvas=false, bar top shift=0, bar height=1, ]{1}{60} 
 		\ganttgroup{01------05}{01}{05}
 		\ganttgroup{06------10}{06}{10}
 		\ganttgroup{11------15}{11}{15}
 		\ganttgroup{16------20}{16}{20}
 		\ganttgroup{21------25}{21}{25}
 		\ganttgroup{26------30}{26}{30}
 		\ganttgroup{31------35}{31}{35}
 		\ganttgroup{36------40}{36}{40}
 		\ganttgroup{41------45}{41}{45}
 		\ganttgroup{46------50}{46}{50}
 		\ganttgroup{51------55}{51}{55}
 		\ganttgroup{56------60}{56}{60}\\
 		\ganttbar{A}{11}{25}
 		\ganttbar{A}{46}{60}\\
 		\ganttbar{B}{1}{10}
 		\ganttbar{B}{26}{35}\\
 		\ganttbar{I/O$_A$}{26}{45}
 		\ganttbar{I/O$_B$}{11}{20}
 		\ganttbar{I/O$_B$}{46}{55}
 		\end{ganttchart}
 		\end{adjustbox}
 		\end{center}
 		\newpage
 		\section{Concepts Organization}
 		\begin{enumerate}[label=\arabic*., leftmargin=3em]
 		\item What is the purpose of interrupts?
 		
 		My answer:The purpose of interrupts is to allow the CPU to respond to external events (such as I/O operations) while processing the current task, and to quickly handle these events, thereby improving system efficiency and supporting multitasking.
 		
 		\item How does an interrupt differ from a trap?
 		
 		My answer:An interrupt is typically generated by hardware to signal an external event (such as I/O or a timer), while a trap is usually generated by software to handle exceptions, errors, or system calls.
 		
 		\item \textbf{Can traps be generated intentionally by a user program? If so, for what purpose?}
 		
 		My answer:Yes, traps can be intentionally generated by a user program, typically to make a system call. This allows the program to request the operating system to perform privileged operations, such as file management, memory allocation, or process control.
 		
 		\item Interrupt:generated by hardware, to enable CPU to respond to externel requests, like I/O.
 		\item Trap/Exception:generated by software, to do some system calls.
 		
 		\item Multiprogramming:It allows multiple programs to run in memory and keeps the CPU and I/O devices busy, improving overall system efficiency by switching between tasks when one is waiting for I/O.
 		\item Multitasking(Time-sharing):The CPU time is divided between multiple tasks, giving each task a small time slice to ensure responsiveness, particularly in interactive systems.
 		\item Interrupt driven:Interrupt-driven I/O allows devices to notify the CPU when their I/O operations are complete through an interrupt, rather than the CPU continuously polling the device to check its status. This reduces unnecessary CPU usage and improves overall system efficiency.
 		\item Dual-mode:Dual-mode operation refers to the two modes of CPU operation: user mode and kernel mode. Programs typically execute in user mode, but when they need to perform privileged actions (e.g., accessing hardware or system resources), they must make system calls to transition to kernel mode. The purpose of dual-mode operation is to protect the system's resources and ensure security by preventing user programs from directly accessing or modifying critical system resources.
 		\item Privileged instructions:太简单了，就是kernel mode有但是user mode没有的指令，防止系统资源被破坏或修改。
 		\end{enumerate}
 		
 		
        \chapter{Operating\_System}
        \section{In-class Contents}
        \subsection{System Calls}
        \begin{definition}
        System Call：用户程序和操作系统之间的接口。
        \end{definition}
        \begin{definition}
        Application Programming Interface(API)：API封装了一系列syscall，只需要调用API就行，不用直接调用syscall。
        \end{definition}
        
        用API不直接用system call的一个重要原因就是，每个OS的底层syscall可能不同，但是它们的API可以是一样的。
        
        那么怎么来执行一个system call呢？
        
        Trap, trap-handler and return-from-trap.
        
        那么在一个trap当中，我们运行OS的哪块代码呢？
        
        Trap table对应的trap-handler.
        
        trap之前的信息会存在每个process自己的内核栈当中。
        
        \subsection{Policy \& Mechanisum}
        我们分离了policy（决定我们要做什么）和mechanisum（我们应该如何做这件事情）。以timer为例，中断是我们的目标，但是我们可以通过RR、SJF等mechanisum来实现这个policy。
        
        \newpage
        \section{Concepts Organization}
        \begin{enumerate}[label=\arabic*., leftmargin=3em]
        \item System call: Programming interface to the services provided by the OS.
        \item Policy and mechanisum: Mechanisums determine how to do something, policies decide what will be done.
        \item Simple/ layered/ microkernel/ modules/ hybrid OS structures
        \end{enumerate}
        
        
        \chapter{Processes}
        \section{In-class Contents}
        \subsection{Warm-up}
        我们来看一个例子，来看看四个不同的进程是如何在一个CPU上运行的：
        \vspace*{6pt}
\begin{lstlisting}[language=C]
/* cpu.c */
int main(int argc, char *argv[]) {
	if (argc != 2) {
		fprintf(stderr, "usage: cpu <string>\n");
		exit(1);
	}
	char *str = argv[1];
	while (1) {
		spin(1);
		printf("%s\n",str);
	}
	return 0;
}
\end{lstlisting}
\vspace{6pt}
\begin{lstlisting}[language=sh]
prompt> gcc -o cpu cpu.c -Wall
prompt> ./cpu A & ; ./cpu B & ; ./cpu C & ; ./cpu D &
\end{lstlisting}
\vspace*{6pt}
\begin{lstlisting}[language=sh]
[1] 7353
[2] 7354
[3] 7355
[4] 7356
A
B
D
C
A
B
D
C
A
C
B
D
...
\end{lstlisting}   

我们可以发现输出的ABCD是没有顺序的，随便输出的。

我们来讲讲这个章节主要关注了哪些内容：

首先，如何实现CPU的虚拟化？OS会通过一些低层硬件支持和软件机制（比如中断、特权指令）和一些高层策略（如调度策略）来实现CPU的虚拟化。

我们再来说一下time-sharing机制（上下文切换）+调度策略（优先级调度）：在这个过程当中，一个process正在运行，它会被停掉并且去跑另一个process，以此类推。

\begin{pt}
进程之间是相互独立的，OS的作用是让他们互不干扰。
\end{pt}

最后，process(job)是OS提供给用户的最基本的abstraction之一，process和job在课本里是一样的。

\subsection{Process Concept}
\begin{definition}
Process：一个进程就是在执行的程序，并且进程的执行是按顺序执行的。
\end{definition}

当program执行后，它就会变成进程，进程里面包含代码、REG、内存等。

那么，进程到底由哪些东西组成呢？
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 程序代码，即text section（文本段）
\item 当前的运行状态，包括program counter, instruction pointers, processor registers（存放变量值和临时数据）
\item Stack：包含临时数据，例如函数参数、返回的地址、局部变量
\item Data section:存放全局变量
\item Heap：动态分配的内存
\end{enumerate}

\begin{vs}
对于code来说，只有编译之后才会称为program，program是disk中的可执行文件，而只有program被加载到memory之后，program才会变成process。
\end{vs}

\begin{exam}
Program只有code、data section，而process还有stack和heap。
\end{exam}

\begin{pt}
一个program可以是多个processes（例如服务器）
\end{pt}

\subsubsection{Loading: From Program To Process}
那么，program是怎么从disk加载到memory的呢？
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Loading：分为主动加载(eagerly)（一次性把所有的代码和数据加载到内存当中）和懒加载(lazily)（只加载需要的部分，剩下的按需加载）
\item Paging and swapping：将program分成一个个有固定size的page，同时这些page可以通过swap进行换入换出。
\item program加载到内存之后，需要为它分配stack和heap。
\item IO Initialization（输入输出初始化）:初始化进程的 I/O，例如打开标准输入输出、文件描述符、设备资源等.
\end{enumerate}

现在我们来看看program加载到memory后变成process了是什么情况的，我们看下面的这段代码：
\vspace*{6pt}
\begin{lstlisting}[language=C]
/* main.c */
int a = 0; // data
char *p1;
// 全局变量，不可以定义完之后再对它赋值，也就是说p1="hello"是非法的，但是直接char *p1="hello"是合法的。（全局变量存在data section里面）
int main(int argc, char *argv[]) {
	int b;
	char s[] = "abc"; // stack（由编译器自己管理，会自己释放空间）
	char *p2;
	char *p3 = "123456"; // stack
	p1 = (char *)malloc(10); // heap（由用户管理，所以需要代码来释放空间）
	p2 = (char *)malloc(20); // heap
	return 0;
}
\end{lstlisting}
\begin{table}[]
\centering
\normalsize
\begin{tabular}{r|c|}
\multicolumn{1}{r}{\multirow{2}{*}{max}}& \multicolumn{1}{c}{\quad \quad \quad \quad \quad \quad } \\
\cline{2-2}
& \multirow{3}{*}{\textbf{Stack}} \\
& \\
& \\
\cline{2-2}
 & $\downarrow$ \\
& \\
& $\uparrow$ \\
\cline{2-2}
& \multirow{3}{*}{\textbf{Heap}} \\
& \\
& \\
\cline{2-2}
& \multirow{3}{*}{\textbf{Data}} \\
& \\
& \\
\cline{2-2}
& \multirow{3}{*}{\textbf{Text}} \\
& \\
\multirow{2}*{0}& \\
\cline{2-2}
\end{tabular}
\end{table}

\subsubsection{Process State}
\textbf{重点重点重点重点重点！！！！！！！}、

我们的Process啊，一共有五个主要的状态：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item new:process被创建了
\item running:指令在CPU被运行了
\item waiting:process在等待一些事件发生，比如I/O
\item ready:process等待被分配到处理器，比如等待cin,return,exit,ctrl+c
\item terminated:process完成运行
\end{enumerate}

\textbf{重点重点重点重点重点！！！！！！！}
\begin{pt}
waiting time描述的是ready的时间，表示准备就绪等待调度。
\end{pt}

\textbf{超级无敌重点重点重点！！！！！！！！}
\begin{figure}[H]
\includegraphics[width=\linewidth]{state_transition_diagram.pdf} 
\end{figure}

我们一点点来讲process state的变化：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item new->ready:进程创建完成，进入ready队列，这个过程由long-term scheduler处理（process只要create了，就一定能跑，无非是什么时候让你跑）
\item ready->running:由short-term scheduler进行调度,process被调度程序选中，获得CPU资源并开始运行
\item running->ready:进程A执行了一个时间片后，CPU切到了进程B，A进入了ready（这里是否是short-term scheduler还待考证）
\item running->waiting：主要是内核在工作，通过system call把running的进程安排到waiting，可能需要wait for I/O之类的，short-term scheduler参与完成。
\item waiting->ready:由short-term scheduler完成调度，一般是I/O完成了
\item running->zombee:进程已经跑完了，但还有些信息
\item running->terminated:Process完成执行或者被强制终止
\item ready->swapped out and ready:将整个process给swap out出来，可能是因为系统负载更高，进程被暂时换出
\item waiting->swapped out and waiting:进程在等待I/O，但由于内存紧张，被换出去了
\item memory->disk和disk->memory的swapping操作都是medium-term scheduler完成的。
\item terminated操作也是由short-term scheduler完成的。
\end{enumerate}

\subsubsection{Process Data Structure}
那么问题来了，为什么我们用process而不用program呢？其实很简单，因为program就是一组指令的静态集合，他没有heap、stack啥的。

\begin{example}
那么，一个program可以停止然后再次被运行吗？

答：不可以，除非我们能记录运行的program的机器状态(machine state)，而process就是一个包含machine state的一个running program。
\end{example}

\begin{definition}
Machine state:包含CPU registers，内存状态以及I/O状态。
\end{definition}

那么对于操作系统来说，它有哪些数据结构来追踪每个进程的状态呢？

一个是进程列表Process lists（用于进程调度，决定哪个进程运行），里面有所有ready/running/waiting的processes；那么这个process list里面的内容就是Process Control Block(PCB)，每个process都有一个PCB（包括Program counter，process的state，process id之类的东西，和inode很像），\textbf{PCB就是Process的身份证}。

\subsubsection{Process Execution}
我们先来看一个直接执行协议的例子：

\vspace{6pt}
\begin{center}
\normalsize
\begin{tabular}{l l}
\textbf{OS}&\textbf{Program}\\
\textbf{(kernel mode)}&\\
\hline
create entry for process list&\\
allocate memory for program&\\
load program into memory&\\
set up stack with argc/argv&\\
clear registers&\\
execute call \emph{main}()&\\
&run \emph{main}()\\
&execute return from main\\
free memory of process&\\
remove from process list&\\
\end{tabular}
\end{center}

Direct execution看上去很快，但是有两个问题：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Protection:没法确保program不会乱搞，比如说执行一些我们不想让他执行的东西

应对：使用双模式(dual mode)和system call，也就是说，让program在user mode下运行，防止对系统进行误操作，那么如果program需要一些特殊的调用，就用system call就可以了。

\item Time sharing:我们想要做分时系统，那么就要保证OS能够把一个正在跑的进程切掉，换成另一个进程，所以我们需要有一个timer来做context switch（时间片轮转），但是显然现在没有咯。
\end{enumerate}

\begin{definition}
Context switch:保存和恢复上下文（saving and restoring context）。
\end{definition}

那么要解决Protection的问题，看PPT的19到21页。

对于Time sharing的问题：在processes之间进行切换时，OS通过\textbf{timer interrupt}获得CPU的控制权（hardware interrupt能够做到强制获得CPU的控制权）；同时OS当中的scheduler\textbf{（GPT认为是short-term scheduler）}决定到底换掉哪个process。

补充一个概念：
\begin{definition}
Kernel Stack(k-stack):理论上来说每个process都有一个kernel stack，但董恺认为，kernel stack看作一个更好理解，kernel stack存储的是kernel mode下的临时信息，不包含process的context，而PCB存的东西要比kernel stack是多一些的，在做context switch的时候，比方说CPU要从process A换到process B，就要先把CPU的Reg里的内容（也就是Process A的内容）存到kernel stack当中，然后kernel stack再把这些内容写回process A的PCB里面，再把process B的PCB的东西存到kernel stack里面，kernel stack再把B的数据写到CPU的Reg里面，这样就完成context switch啦！
\end{definition}

所以从上面的例子可以看出来，数据在register、k-stack和PCB里面都有一个备份。

现在有一个比较搞脑子的东西，讲的是两种register saves/resotres:
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Timer interrupt（由OS控制，由hardware实现，通常发生在于user mode下运行program）：在这种情况下，user registers的内容会由hardware隐式存储到kernel stack当中。
\item OS决定switch（可能是一些调度算法之类的，通常发生在于kernel mode下运行program，那可能就是syscall之后进行了一些switch），这个时候OS会把kernel registers里的东西存到kernel stack，然后再从kernel stack存到PCB里面。
\end{enumerate}

那么如果当一个interrupt发生时，另一个interrupt或者trap发生了怎么办，特别是当kernel stack的数据还没有存进PCB时？那很糟糕了兄弟。

但是一般性在处理一个interrupt的时候会disable other interrupts。

当然，要是说每个process有多个program counters呢？就可以实现一个process在多个locations同时运行，也就是有多个执行路径->Threads。那么为了支持多线程，操作系统需要为每个线程保存相关的详细信息，这些信息会存在PCB中。

\subsection{Process Scheduling}
Process Scheduling做的事情就是最大化CPU利用率，实现process之间的快速切换。那么对于process scheduler来说，要在一些processes之中选择下一个在CPU运行的process。

与此同时，操作系统维护着几个不同的队列，用于跟踪进程的状态，帮助调度器选择下一个执行的进程，这样的队列一共有以下三种：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Job queue（有1个）：系统中的所有processes。
\item Ready queue（有好几个）：所有在内存当中，等待执行的Processes（也就是ready状态下的processes）。
\item Device queue（也有好几个）：所有正在等待执行I/O的processes（也就是waiting状态下的Processes）。
\end{enumerate}
\begin{vs}
Job queue里面有所有process，不管你有没有被加载到主存。然后，waiting state的process，也就是说Device queue里的process也是都在主存的。最后，不是说磁盘里只能有program不能有process，如果内存紧张，就可以把process换到磁盘，它不会变成program。
\end{vs}

\subsubsection{Schedulers}
\begin{definition}
Short-term scheduler(CPU scheduler):决定哪个process下一个被执行，并且分配CPU（process的状态由ready到running），因为它经常被调用，所以它需要响应得非常快。
\end{definition}
\begin{definition}
Long-term scheduler(job scheduler):决定哪个process会被放到ready queue（注意是new->ready，不管waiting->ready）,因为它没那么经常被调用，所以会慢一些。但是long-term scheduler控制多道程序设计的程度(degree of multiprogramming)
\end{definition}

诶？啥是多道程序设计的程度呢？
\begin{example}
比如说degree=10，就代表着在main memory中有10个processes，比方说有1个running，5个ready，4个waiting，那么long-term scheduler可以做到增加这个degree，但是不能减少，而short-term不能增加也不能减少->引入medium-term scheduler（见后文）。
\end{example}

对于processes来说，可以分成以下两种：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item I/O-bound process:花大部分时间做I/O而非计算，短时间的CPU burst较多，例如：读取大量数据，此时I/O不占CPU，切到其他进程。
\item CPU-bound process:花大部分时间做计算，只有少量但是时间很长的CPU burst，例如：数学计算、图像处理，因为很少但是时间长，所以中间可能会发生CPU饥饿。
\end{enumerate}

所以对于long-term scheduler来说，他要做的事情就是很好地将这两类process做一个混合。

\begin{definition}
Medium-term scheduler:用来减少degree of multiple programming，它做的事情是将process从memory里面拿走，放到disk里面，或者把process从disk里面拿回来，说的专业一点，叫\textbf{swapping}。
\end{definition}

那为啥要降低多道程序的degree呢？防止thrashing（第九章Virtual Memroy->Thrashing）。

\subsection{Operationgs on Processes}
对于一个系统来说，它一定要具有process creation、process termination等功能。

\subsubsection{Process Creation}
如果一个process想要把任务分配出去，那么他就可以创建一些子进程（它也就被称为父进程），然后这些子进程还可以创建他们的子进程，这样就创建出了一个进程树。

那么在操作系统中如何对进程进行区分呢？我们还记得每个process都有一个PCB对吧，PCB里面有一项叫做pid(process identifier)，它叫进程标识符，就可以区分不同的进程。

那么在资源共享方面，父进程和子进程有以下这些资源共享的策略：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item parent和children共享所有的资源
\item children共享parent的部分资源
\item parent和children不共享任何资源
\end{enumerate}

在进程执行方面，父进程与子进程也有以下两种策略：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item parent和children一起并发执行
\item parent等待children terminate之后再执行
\item 注意不会有children等parent的情况，因为这样不如parent再创建一个子进程
\end{enumerate}

\textbf{下面是重点重点重点！！！}

那么复制完之后的地址空间呢？

对于子进程来说，他会复制父进程的地址空间（包括数据段、堆栈），同时子进程可以继续执行父进程的代码，同时也可以加载并执行新的程序。

\begin{figure}[H]
\centering
\scriptsize
%\includegraphics[width=.3\linewidth]{resources/process_creation.jpg} 
\tikz{
    \path 
        (0,0) node [minimum width=1.5cm,minimum height =0.5cm,ellipse,draw] {\texttt{fork()}}
        (2,-1) node [minimum width=1.5cm,minimum height =0.5cm,ellipse,draw] {\texttt{exec()}}
        (5,-1) node [minimum width=1.5cm,minimum height =0.5cm,ellipse,draw] {\texttt{exit()}}
        (5,1) node [minimum width=1.5cm,minimum height =0.5cm,ellipse,draw] {\texttt{wait()}};
    \draw [->] (0,-0.25) .. controls (0.5,-1) .. node[below=1mm] {\textbf{child}} (1.25,-1);
    \draw (0,0.25) .. controls (0.5,1) .. (1.25,1);
    \draw [->] (1.25,1) -- node[above=1mm] {\textbf{parent}} (4.25,1);
    \draw [->] (2.75,-1) -- (4.25,-1);
    \draw [->] (5,-0.75) -- (5,0.75);
    \draw [->] (5.75,1) -- node[above=1mm] {returns} (7.25,1);
}
\end{figure}

\begin{example}
我们来举一个UNIX架构下的例子，首先有两个system call(API)：fork()（创建新的进程，同时返回子进程的pid）、exec()（中调用fork()之后，用一个新的program来替换当前的内存空间，包括data section、text section、heap、stack等）。

那么在上面这个图里面啊，比方说要发工资了parent干的事情是计算工资，而child做的事情是发工资。那么在fork()之后，因为一开始是复制的，在没有调用exec()之前，child也以为他要做的事情是计算工资，后来才知道他要完成的工作是发钱。然后child运行完exit()之后，父进程才可以继续往下运行。
\end{example}

那么还要知道一个事情就是，当父进程运行了fork()之后，父进程和子进程都会从fork语句的下一句开始运行。我们来看下面的这段代码：
\vspace*{6pt}
\begin{lstlisting}[language=C]
#include <sys/types.h>
#include <stdio.h>
#include <unistd.h>

int main(int argc, char *argv[]) {
	pid_t pid;
	pid = fork();
	// 注意在这个地方，对于父进程来说fork()返回的是子进程的pid，那么对于刚刚创建出来的子进程来说，fork()返回的是0，也就是pid在子进程中是0，所以可以说子进程是不知道自己的pid的。（我们约定创建出来的子进程的pid≥1）
	if (pid < 0) { // error occurred
		fprintf(stderr, "Fork Failed");
		return 1;
	}
	else if (pid == 0) { // child process
		execlp("/bin/ls", "ls", NULL);
	}
	else { // parent process
		wait(NULL);
		printf("Child Complete");
	}
	return 0;
}
\end{lstlisting}

\subsubsection{Process Termination}
那么有进程创建，肯定也会有进程的终止对吧：

子进程的终止也能分为以下两种，一种是自己终止自己，另一种是父进程终止子进程：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 当process执行完了它的最后一个语句的时候，它会调用exit()这个system call来让OS把自己删了（也就是child terminates itself，他自杀了，像是下定了某种决心）。在这之后，父进程可以通过wait()这个system call来获取child的状态数据。最后，子进程的资源会被OS回收。
\item 父进程也会调用abort()这个system call来终止子进程，可能是因为：子进程用超了分配给它的资源、分配给子进程的任务不再被需要了、父进程正在退出，而操作系统不允许子进程在父进程退出后继续运行。
\end{enumerate}

我们再来详细讲讲说操作系统不允许子进程在父进程退出后继续运行这个事情：
\begin{definition}
Cascading termination（级联终止）:父进程的终止会引发子进程（以及所有孙子进程等）的终止。同时，\textbf{子进程的终止不是parent process引起的，是由OS自动杀死的}。
\end{definition}

当然parent process可以等待子进程完成，也就是用wait()这个system call，一般用的代码是pid=wait(\&status)。

最后再讲两个process的特殊状态：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Zombie:父进程没有调用 wait() 来等待子进程的终止，那么子进程在终止后会进入“僵尸”状态。这意味着子进程已经结束，但它的状态信息还没有被父进程收集，导致它仍然占用一些系统资源。
\item Orphan:如果父进程提前终止，但没有调用 wait()，那么子进程就变成了“孤儿进程”。孤儿进程将由系统的初始化进程（也就是init进程）接管并继续执行。
\end{enumerate}
\begin{pt}
zombie状态的进程已经结束了，但是orphan状态的进程是还没有结束的。
\end{pt}

我们来看一个具体的代码来加深理解：
\vspace*{6pt}
\begin{lstlisting}[language=C]
/* p1.c */
int main(int argc, char *argv[]) {
	printf("hello world (pid:%d)\n", (int) getpid());
	int rc = fork();
	if (rc < 0) {
		fprintf(stderr, "fork failed\n");
		exit(1);
	}
	// 子进程运行的代码
	else if (rc == 0) {
		printf ("hello, I am child (pid:%d)\n", (int) getpid());
		char *myargs[3];
		myargs[0] = strdup("wc");
		myargs[1] = strdup("p1.c");
		myargs[2] = NULL;
		execvp(myargs[0], myargs);
		// 注意，子进程运行到上面execvp后，相当于整个进程都被换掉了，后面任何东西都不会执行，所以下面一行printf是不可能执行的。
		printf("this shouldn't print out");
	}
	// 父进程运行的代码
	else {
		int wc = wait(NULL);
		printf("hello, I am parent of %d (wc:%d) (pid:%d)\n", rc, wc, (int) getpid());
		// 第一个传入的参数%d是rc的值，也就是子进程的pid，第二个传入的参数为wc，同样是子进程的pid（因为wc=wait()），最后一个传入的参数是getpid()，也就是父进程的pid。
	}
	return 0;
}
\end{lstlisting}
\vspace{6pt}
\begin{lstlisting}[language=sh]
prompt> ./p1
hello world (pid:29383)
hello, I am child (pid:29384)
	29	107	1030	p1.c
hello, I am parent of 29384 (wc:29384) (pid:29383)
prompt>
\end{lstlisting}

\subsection{Interprocess Communication}
对于一个进程来说，他要么是independent的，要么是cooperating的（当然也可以这两种情况切来切去）：

如果是独立的，那很好理解，大家互不相关，但要是是合作的呢？他们之间是会彼此影响的，包括一些共享的数据，这样多个进程协同工作来完成一个任务，那么这样的cooperating processes需要什么呢？进程间通信（IPC），现在有两种IPC的basic models：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Shared memory
\item Message passing
\end{enumerate}

\subsubsection{Message Passing \& Shared Memory}
下面的东西确实会比较弯弯绕绕：

首先出场的是message-passing model!它给我们带来的是pipe和message queue(Linux)!

\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{screenshot002}
\label{fig:screenshot002}
\end{figure}

那么在message-passing model当中，进程A和进程B之间通过发送和接收消息来进行通信。消息传递可以通过内核（Kernel）来实现，内核在这个过程中充当了一个消息交换的中介。

\begin{definition}
Message Queue:消息队列是一个存储消息的地方，消息可以通过该队列从一个进程传递到另一个进程。例如，进程A可以将消息m0、m1等放入消息队列中，进程B通过内核从队列中读取这些消息（同样遵循先进先出的顺序）。
\end{definition}

这样做的优点是：提供了进程间的隔离，即使进程A和进程B的内存不共享，消息依然可以通过内核传递；但是缺点是：在高频率通信时，需要较多的上下文切换和内核的干预（要调用system call来访问消息队列）。\textbf{进程需要将数据复制一份送到消息队列。}

我们再来看shared-memory model：进程A和进程B共享一个公共的内存区域。这意味着它们可以直接访问和修改该区域的内容，从而实现数据交换。

\begin{definition}
Shared Memory：存在user memory space当中，不在内核，但是这块空间需要向kernel申请。进程A和进程B可以直接读取和写入共享内存中的数据，而不需要通过kernel传递信息，实际的数据交换发生在shared memory当中。
\end{definition}

这样做的优点在于：共享内存的通信方式通常比消息传递更高效，因为它避免了消息的中介传递，进程之间直接通过共享区域交换数据，减少了开销。但缺点在于：共享内存需要进程之间协调和同步，以避免冲突或数据不一致的问题。对于不同进程的内存访问，需要考虑并发控制（如互斥锁、信号量等）。\textbf{进程可以直接在shared memory读取和写入数据。}

\subsubsection{Producer-Consumer Problem}
我们通过一个具体的案例（消费者-生产者问题）来比较一下这两种方案：

说简单点就是一个进程负责生产信息，另一个进程负责消费这些信息，比如说编译器是一个生产者，它将源代码编译为汇编代码；而汇编器是一个消费者，它接收并处理汇编代码，进一步生成机器代码或目标代码。

我们来看一下用shared-memory solution是怎么解决生产者消费者问题的：
\vspace*{6pt}
\begin{lstlisting}[language=C]
/* Solution is correct, but can only use BUFFER_SIZE - 1 elements */
#define BUFFER_SIZE 10
typedef struct { ... } item;
item buffer[BUFFER_SIZE];
int in = 0;
int out = 0;

/* producer */
item next_produced; 
while (true) { 
	while (((in + 1) % BUFFER_SIZE) == out) ; 
	// 表示缓冲区已满，等待
	buffer[in] = next_produced; 
	in = (in + 1) % BUFFER_SIZE; 
} 

/* consumer */
item next_consumed; 
while (true) {
	while (in == out) ;
	// 表示缓冲区已空，等待
	next_consumed = buffer[out]; 
	out = (out + 1) % BUFFER_SIZE;
}
\end{lstlisting}

那么在这个案例当中，0到9只是一个简单的索引，没有说in=9就一定是buffer为满，in=2，out=3也是buffer为满。

Shared memory的数据交流需要由进程自行管理数据访问，而不依赖OS的调度，也就是说，需要自己来编程。同时，面临一个主要的问题就是——同步（数据竞争or数据不一致问题）

接下来讲讲Message Passing的解决方法：
\begin{definition}
Message system：进程之间进行交流但不使用共享变量，也就是说，它最大的优势是不用考虑同步的问题。
\end{definition}

IPC提供了两种操作：send(message)和receive(message)，这里的message的大小可以是固定的也可以是变化的。

那这个代码就会变得极其简单：
\vspace*{6pt}
\begin{lstlisting}[language=C]
/* producer */
message next_produced; 
while (true) {
	 /* produce an item in next produced */ 
	send(next_produced); 
} 

/* consumer */
message next_consumed; 
while (true) {
	receive(next_consumed);
	/* consume the item in next consumed */
}
\end{lstlisting}

\subsubsection{Message Passing Vs. Shared Memory}
这里和第一部分按理说是连起来的，但是中间穿插了一个生产者消费者问题也无伤大雅。

首先我们再来detail讲一下message passing背后到底干了什么事情。\textbf{个人感觉这块地方没有那么重要，知道就行了吧。（指直接通信和间接通信）}

通信分为直接通信(direct communication)和间接通信(indirect communication)

对于直接通信，有两种命名方式：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 对称地址：发送和接收方互相命名：

对于process Q->send(P,message);对于process P->receive(Q,message)

\item 非对称地址：只要发送方指定接收方就可以：

对于process P->send(P,message);对于process Q->receive(id,message)，它能够接收任何process来的message。
\end{enumerate}

直接通信有这三个特质：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 自动建立链接：每对想要进行通信的进程之间会自动建立一个通信链接。

\item 链接关联性：每个通信链接仅与两个进程关联。

\item 每对进程之间只有一个链接：每一对进程之间会存在且只存在一个通信链接。
\end{enumerate}

那么我们再看看间接通信：间接通信的情况下，消息被送到或者从一个叫"mailbox"的地方接收，那么对于两个进程来说，或者多个进程来说，他们的代码就是简单的send(A,message)和receive(A,message)，A是mailbox。

间接通信也有三个特质：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 链接的建立：当一对进程共享一个邮箱时，才会在这两个进程之间建立一个通信链接。

\item 多个进程的链接：一个邮箱可以与多个进程建立关联，支持多个进程间的通信。

\item 多个链接：对于每一对通信进程，可能会存在多个不同的链接，每个链接对应于一个邮箱。
\end{enumerate}

\textbf{下面的内容董恺稍微花了点时间讲：}

对于消息的传输也有两种形式，一种是blocking（阻塞的），另一种是non-blocking（非阻塞的）：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Blocking(synchronous：同步)：做个简单的类比，我们人有两条腿走路，一定是迈完左脚，要block一下，再等待右脚迈出来。

Blocking send（阻塞发送）：发送方被阻塞，直到消息被接收。

Blocking receive（阻塞接收）：接收方阻塞，直到有消息可以供我接收。

\item Non-blocking(asynchronous：异步)：我在操场上跑1000米，我不带停的，所以这样很快。

Non-blocking send（非阻塞发送）：发送方一直在那里发送。

Non-blocking receive（非阻塞接收）：接收方接收到的可能是无效的消息或者是空消息。
\end{enumerate}

当然，排列组合一下就可以发现，一共有四种搭配方法，那么怎么搭配都是可以的，当send和receive都是blocking的时候，我们称它是rendezvous（会面）。

最后一个概念是Buffering，也就是message passing会有一个temporary queue，有以下三种情况：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Zero capacity（刚生产出来就必须消费掉，虽然确保消息的传输，但是想想就知道这样的效率是很低的）
\item Bounded capacity：有界容量
\item Unbounded capacity：无界容量
\end{enumerate}

\textbf{最后的最后啊，我们对message passing和shared memory做一个比较：}

\begin{pt}
记得我们所有的比较都是宏观的比较，不太需要涉及到具体的实现方案。
\end{pt}

我们先来看看Message passing的优势（\textbf{稳定}）：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Message passing适合少量数据的交换，因为不会出现多个进程的冲突
\item 如果想要在不同的计算机之间进行通信，message passing会更容易实现
\item 在多处理器系统中，共享内存可能面临缓存一致性问题，这可能导致性能下降。而消息传递通过明确的消息队列和通信方式，避免了这种问题。
\end{enumerate}

再来看看Shared memory的优势（\textbf{快}）：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 消息传递通常是通过系统调用实现的，这要求内核介入，增加了开销。而在共享内存系统中，系统调用只需要在初始化共享内存区域时进行，后续操作没有内核介入，效率更高。
\end{enumerate}

从未来走向的角度说，如果要实现IPC，message passing会更加流行。

\subsection{Communication in Client-Server Systems}
\begin{definition}
Socket（套接字）：用于网络通信的端点，通常由IP地址和端口号组合而成，是实现网络通信的基础。
\end{definition}

\begin{example}
在UNIX环境下，用pipe()这个system call来实现message passing，管道允许一个进程的输出与另一个进程的输入相连接。管道通常用于连接两个相关的进程，使得一个进程的输出直接作为另一个进程的输入。
\end{example}

管道还分为以下两种：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Ordinary pipes（普通管道）：只能在父子进程之间进行通信的管道。通常情况下，父进程会创建管道，并且子进程通过该管道与父进程通信。普通管道无法在父进程外部访问。

\item Named pipes（命名管道）：与普通管道不同，命名管道可以在没有父子进程关系的情况下进行访问。命名管道是给定名字的管道，允许不同行的进程之间通过文件系统中的路径进行通信。这使得命名管道在某些情况下比普通管道更灵活。
\end{enumerate}

\newpage
\section{Exercise}
\begin{example}
读下面的代码，what is the output?
\end{example}
\begin{lstlisting}[language=C]
/* exercise.c */
int value = 5;
int main(int argc, char *argv[]) {
	pid_t pid;
	pid = fork();
	if (pid == 0) {   
		printf("child process, value1 : %d\n", value);
		value += 15;
		printf("child process, value2 : %d\n", value);
	}
	else if (pid > 0) { 
		printf("parent process, value3 : %d\n", value);
		wait(NULL);
		printf("parent process, value4 : %d\n", value);
	}
	exit(0);
}		
\end{lstlisting}

Keys：
\vspace*{6pt}
\begin{lstlisting}[language=sh]
prompt> ./exercise
child process, value1 : 5
child process, value2 : 20
parent process, value3 : 5
parent process, value4 : 5
prompt>
\end{lstlisting}
\vspace*{6pt}
\begin{lstlisting}[language=sh]
prompt> ./exercise
parent process, value3 : 5
child process, value1 : 5
child process, value2 : 20
parent process, value4 : 5
prompt>
\end{lstlisting}

补充说明：一般不会出现先value1，再value3，再value2，再value4，虽然它是符合逻辑的。		
		
\begin{example}
Including the initial parent process, how many processes are created by the following program?
\end{example}
\vspace*{6pt}
\begin{lstlisting}[language=C]
#include <stdio.h>
#include <unistd.h>

int main() {
	int i;
	for (i = 0; i < 4; i ++)
		fork();
	return 0;
}
\end{lstlisting}

一些小提示，考试的时候可以画树状图。

Keys:16 processes.

\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{screenshot003}
\label{fig:screenshot003}
\end{figure}

\newpage
		\section{Concepts Organization}        
        \begin{enumerate}[label=\arabic*., leftmargin=3em]
        \item Processes:A program in execution; process execution must progress in sequential fashion.
        \item Context switch:A context switch is the process of saving the state of a currently running process and loading the state of the next scheduled process, allowing the CPU to switch between tasks in a multitasking environment.
        \item PCB:A struct that records the machine state of the process.
        \item Long-term scheduling：Select which processes should be brought into the ready queue.
        \item Medium-term scheduling:Used to decrease the degree of multiple programming.
        \item Short-term scheduling:Select which process should be excuted next and allocates CPU.
        \item Shared memory:A region of memory that is shared by cooperating processes is established. Processes can then exchange information by reading and writing data to the shared region.
        \item Message passing:Communication takes place by means of messages exchanged between the cooperating processes.
        \item Pipes:The output of one process is connected to an in-kernel pipe. And the input of another process is connected to that same pipe.
        \end{enumerate}
        
        
\chapter{Threads}
\section{In-class Contents}
\subsection{Warm-up}
\begin{definition}
Thread:process的一部分，共享进程的内存、全局变量等资源；线程之间的通信比进程间更加高效，但容易发生数据竞争、同步等问题，也就是说，如果一个线程寄了，整个进程可能就没了。

本质是一个执行指令序列，允许程序同时进行多个操作。
\end{definition}

我们来考虑一个事情，为什么一些programs会有多个执行点？比方说把两个大数组相加或者是输入一些元素的时候，program可以把task分成几个subtasks，再有multiple threads来完成这些subtasks，听上去这样就可以加快任务的完成。

那么我们还有个问题，为什么用多线程而不用多进程呢？


\subsection{Overview}
现代的应用程序大多都是多线程的，也就是说线程在应用程序中工作，完成不同的工作，比方说word processor有update display、fetch data和spell checking的工作，就会由多个线程一起完成。

那么为什么用multithread不用multiple processes呢？因为multiple threads能够减少开销，公用内存空间。

OS的kernel基本上就是mutithreaded的。

\subsubsection{Multithread}
我们首先来看看single-threaed process和multi-threaded process的区别：

\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{screenshot004}
\label{fig:screenshot004}
\end{figure}

可以看到，一个process里的threads是共享code、data和files的，但他们有自己的stack和registers，存放局部变量和线程的执行状态，例如PC和IR。

那么这样做有什么好处呢？
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Responsiveness（响应性）：当程序的一部分被阻塞时，线程可以允许其他部分继续执行，这在用户界面中尤其重要。这样，程序能够更快地响应用户输入或操作，而不必等待阻塞的部分完成。（只有multiple thread才能这样继续执行）
\item Resource Sharing：线程之间分享资源，比进程之间通过shared memory或者message passing更容易，开销也更小。
\item Economy：创建新的进程只需要stack和register就可以了，创建process就很复杂了，而且在thread switching的开销比context switching要小很多，不用在PCB、Kernel-stack啥里面的搞来搞去。
\item Scalability（可扩展性）：就这样吧，知道就行了；process can take advantage of multiprocessor architectures。
\end{enumerate}

\subsection{Multicore Programming}
\textbf{这个部分dk说没那么重要}

Multicore或者Multiprocessor在编程上有这些挑战：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Dividing activities（分割活动）：将任务分成多个可以并行的小任务，那么要想好怎么去合理分配。
\item Balance（工作负载）：尽量让每个处理器或者核心负载平衡
\item Data splitting：将数据集分割成多个部分，以便在不同的线程或处理器之间共享
\item Data dependency：确保任务按照正确的顺序执行
\item Testing and debugging
\end{enumerate}

两个我认为比较重要的概念：
\begin{definition}
Parallelism（并行）：并行性意味着一个系统能够同时执行多个任务。多核或多处理器系统通过在\textbf{不同的核心或处理器}上执行任务来实现并行处理。例如，在一个四核处理器上，程序可以将\textbf{四个不同的任务}分配到四个核心上，确保每个任务同时执行，从而加快整体执行速度。
\end{definition}
\begin{definition}
Concurrency（并发）：并发性指的是多个任务能够在同一时间段内有进展，但并不一定是同时执行。即使在单核处理器中，操作系统也可以通过\textbf{时间片轮转调度，使得多个任务交替进行}，给人一种“同时进行”的感觉。并发性适用于\textbf{单核处理器或核心}的情况，操作系统调度器通过快速切换任务，让它们看起来在同时执行。
\end{definition}

看上去是不是不太直观？没关系。我们首先看看Concurrency的时候是什么样的：
\begin{figure}[H]
% \includegraphics[width=.8\linewidth]{resources/concurrency_single-core.jpg}
\centering
\tiny
\tikz{
    \path (-1,0) node[minimum width = 1cm, minimum height = 5mm] {single core}
    (0,0) node[minimum width = 5mm, minimum height = 5mm,draw,fill=black!20] {$T_1$}
    (0.5,0) node[minimum width = 5mm, minimum height = 5mm,draw,fill=black!20] {$T_2$}
    (1,0) node[minimum width = 5mm, minimum height = 5mm,draw,fill=black!20] {$T_3$}
    (1.5,0) node[minimum width = 5mm, minimum height = 5mm,draw,fill=black!20] {$T_4$}
    (2,0) node[minimum width = 5mm, minimum height = 5mm,draw,fill=black!20] {$T_1$}
    (2.5,0) node[minimum width = 5mm, minimum height = 5mm,draw,fill=black!20] {$T_2$}
    (3,0) node[minimum width = 5mm, minimum height = 5mm,draw,fill=black!20] {$T_3$}
    (3.5,0) node[minimum width = 5mm, minimum height = 5mm,draw,fill=black!20] {$T_4$}
    (4,0) node[minimum width = 5mm, minimum height = 5mm,draw,fill=black!20] {$T_1$}
    (4.5,0) node[minimum width = 5mm, minimum height = 5mm,draw,fill=black!20] {$\cdots$};
    \draw [->] (0,-0.5) -- node[below=1mm] {time} (4.5,-0.5);
}
\end{figure}

再看看Parallelism是怎么样的：
\begin{figure}[H]
% \includegraphics[width=.48\linewidth]{resources/parallelism_multi-core.jpg}
\centering
\tiny
\tikz{
    \path 
        (-0.7,0.7) node[minimum width = 1cm, minimum height = 5mm] {core 1}
        (0,0.7) node[minimum width = 5mm, minimum height = 5mm,draw,fill=black!20] {$T_1$}
        (0.5,0.7) node[minimum width = 5mm, minimum height = 5mm,draw,fill=black!20] {$T_2$}
        (1,0.7) node[minimum width = 5mm, minimum height = 5mm,draw,fill=black!20] {$T_3$}
        (1.5,0.7) node[minimum width = 5mm, minimum height = 5mm,draw,fill=black!20] {$T_4$}
        (2,0.7) node[minimum width = 5mm, minimum height = 5mm,draw,fill=black!20] {$T_1$}
        (2.5,0.7) node[minimum width = 5mm, minimum height = 5mm,draw,fill=black!20] {$\cdots$};
    \path 
        (-0.7,0) node[minimum width = 1cm, minimum height = 5mm] {core 2}
        (0,0) node[minimum width = 5mm, minimum height = 5mm,draw,fill=black!20] {$T_1$}
        (0.5,0) node[minimum width = 5mm, minimum height = 5mm,draw,fill=black!20] {$T_2$}
        (1,0) node[minimum width = 5mm, minimum height = 5mm,draw,fill=black!20] {$T_3$}
        (1.5,0) node[minimum width = 5mm, minimum height = 5mm,draw,fill=black!20] {$T_4$}
        (2,0) node[minimum width = 5mm, minimum height = 5mm,draw,fill=black!20] {$T_1$}
        (2.5,0) node[minimum width = 5mm, minimum height = 5mm,draw,fill=black!20] {$\cdots$};
    \draw [->] (0,-0.5) -- node[below=1mm] {time} (2.5,-0.5);
}
\end{figure}

Parallelism有两种类型：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Data parallelism（数据并行性）：在多个处理器核心上分配同一数据的不同子集，每个核心执行相同的操作。这样可以提高执行效率，特别适合于需要对大量数据进行相同计算的任务。
\item Task parallelism（任务并行性）：将任务分配到不同的核心，每个核心执行不同的操作。每个线程执行独立的操作，适合于任务之间相互独立的情况。
\end{enumerate}

现在有一个小问题，添加core的数量，我们到底能够提升多少的performance？假设我们有N个proceesing cores。

\subsubsection{Amdahl's Law}
具体看PPT14页，公式如下：
\begin{center}
$$speedup\leq\frac{1}{S+\frac{(1-S)}{N}}$$
\end{center}

\subsection{Multithreading Models}
\subsubsection{User Threads and Kernel Threads}
首先要明确一个事情，或者这个事情能帮我更好去理解这些概念：\textbf{Thread和Process几乎是一样的，只是它们的名字不一样罢了。}

那么如何去支持这些threads呢，又是谁来支持这些threads呢？

别着急，我们先聊一下有哪些thread的类别（说是说类别，其实也就是它们的library的名称不同罢了，本质没啥区别）：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item User threads：由用户级线程库进行管理，而不是由操作系统内核直接管理，不会涉及system call，完全在用户空间完成。但是这样做有一个缺点：如果一个线程阻塞（例如执行 I/O 操作），整个进程可能会被阻塞，因为操作系统看不到独立的线程调度，而只看到一个进程。

比方说，POSIX Pthreads、Windows threads等就支持用户线程

\item Kernel threads：从名字就能看出来，这类threads由操作系统内核直接支持和管理。内核负责创建、销毁、调度线程，并且可以处理线程的并发执行。几乎所有操作系统都支持kernel threads。
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{screenshot005}
\label{fig:screenshot005}
\end{figure}

我们先看看Kernel threads吧，这个看上去比较好理解：

内核线程是由操作系统内核完全管理的。线程的创建、调度、切换、同步等操作都\textbf{需要通过系统调用}来与内核交互，所以左边整张图都表示的是kernel space里面的情况。

同时，有一个process table管理所有process的信息（通常就是PCB啦），当然也会有一个thread table，里面就是所有thread的信息，虽然书上没说，但是GPT认为是TCB。

再看看右边这张图，这里表示的是一些user threads。在用户线程的模型中，进程和线程主要是在用户空间中进行管理。进程表依然在内核空间中，\textbf{但线程表和调度工作主要在用户空间完成}，这种情况下kernel只会提供一些必要的支持，比如I/O，但是线程的一些操作并不是由kernel完全搞定的。

\begin{exam}
进程可以通过以下几个方式管理用户级线程：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item PCB
\item TCB
\item \textbf{内核不会管，编程来管理Threads}\textbf{（重要重要重要！！！）}
\end{enumerate}
\end{exam}

我们现在再来探讨一些问题：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Why we need user threads?没有内核干预，因此用户线程通常会更高效。
\item Why user threads are not enough?这就要说到user thread的缺点了：内核只识别进程，并且只对进程级别进行管理。如果一个用户线程被阻塞（例如等待 I/O 操作），整个进程也会被认为是阻塞状态。因此，同一进程中的所有线程都会被阻塞，即使其他线程并不需要等待。这就降低了系统的效率，特别是当进程内部有多个线程需要并发执行时。
\item Why we need kernel threads?诶嘿，这就要说到kernel thread的优点啦：当一个线程被阻塞时，内核会运行其他线程，而不会影响同一进程中的其他线程。操作系统能够独立调度每个线程，而不仅仅是整个进程（这才是真正的并行管理）。
\item Why kernel threads are not enough?诶嘿（不是哥们，你戏有点太足了啊）很简单，kernel threads比较慢呗。
\end{enumerate}

那么kernel thread和user thread之间有什么关系呢？一共有下面三种关系：

我们首先要知道一件事情，\textbf{对于内核来说，它只能看到内核线程，而内核线程告诉用户线程决定执行的user thread，而内核是不知道运行的user thread的情况的。}

\subsubsection{Many-to-One}
在这种模型中，多个用户级线程会被映射到一个内核级线程上。也就是说，\textbf{操作系统的内核只知道一个线程}，这个线程处理所有用户线程的调度和管理。

\begin{figure}[H]
\centering
\includegraphics[width=0.7\linewidth]{screenshot006}
\label{fig:screenshot006}
\end{figure}


有什么问题呢？有一个user thread block，那就全部block了，为什么呢？因为在这个模型当中，那个kernel thread就像是一个process一样，而kernel thread又只认那个正在运行的thread，所以如果那个thread block了，那整个就垮了，因为内核不知道发生了什么，它也没办法切到其他user thread上面。

由于所有用户线程都在一个内核线程上运行，这意味着即使有多核CPU，多个线程也无法真正地并行运行，除非系统支持其他线程模型。而且这个模型只能用于不支持kernel thread的操作系统，所以它很过时，没人用的。

\subsubsection{One-to-One}
从名字上就可以听出来，每个user thread对应一个kernel thread。

\begin{figure}[H]
\centering
\includegraphics[width=0.7\linewidth]{screenshot014}
\end{figure}

所以说在创建一个user thread的时候，也会创建一个kernel thread。

这样做有什么好处呢？提高了并发性。因为在这种模型下，每个用户线程都可以独立执行，并且内核能够直接管理这些线程，但是代价就是开销有一点大，所以说每个进程能够有的线程数量是比较有限的。

\subsubsection{Many-to-Many}
这就不能不提到我们most flexible的一个模型，顾名思义，many user threads对应many kernel threads，且一般user thread的数量大于kernel thread的数量。（GPT认为，并发运行的线程数量等于kernel thread的数量）

\begin{figure}[H]
\centering
\includegraphics[width=0.7\linewidth]{screenshot008}
\label{fig:screenshot008}
\end{figure}

\subsubsection{Two-Level Model}
你以为这样就结束了吗？并没有，其实还可以把many-to-many和one-to-one合在一起组成一个two-level model。因为如果在many-to-many的模型当中，有三个user thread block了，那么整个模型也没用了，增加几个one-to-one可以保障高并发。

\begin{figure}[H]
\centering
\includegraphics[width=0.7\linewidth]{screenshot009}
\label{fig:screenshot009}
\end{figure}

\subsubsection{Conclusion}
做一个小小的总结，那么这些模型当中，到底哪些在modern systems中更加受欢迎呢？

在服务器中，常用many-to-many，因为它能可以更高效地处理大量并发线程。而在PC（个人计算机）当中，更多用的是one-to-one，因为这样可以更直接的分配系统资源。（这背后有一个小故事，在LinuxThreads当中，NGPT推崇的是M:M，而NPTL推崇的是1:1，最后NPTL因为商业原因，而非技术原因胜出了）

\subsection{Thread Libraries}
\begin{definition}
Thread Library：线程库为程序员提供了创建和管理threads的API。
\end{definition}

有以下两种实现线程库的方式：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 完全在user space的线程库，操作系统不直接管理
\item 由操作系统内核支持的内核级库，操作系统直接管理
\end{enumerate}

这里呢，有三种主要的thread libraries：\textbf{POSIX Pthreads}、Windows、Java，然后我们会重点看一下POSIX Pthreads。

\subsubsection{Pthread}
首先，pthread在user-level和kernel-level都有实现，且其提供的是一个规范（Specification），而不是具体的实现（Implementation）。换句话说，POSIX定义了线程库的行为和功能，但它并不规定如何实现这一功能。不同的操作系统或线程库可以根据该规范来设计和实现自己的Pthreads库。最后Pthreads在UNIX类操作系统中常见。

我们首先来看看Pthread的创建：
\vspace*{6pt}
\begin{lstlisting}[language=C]
#include <pthread.h>
int
pthread_create(	pthread_t * 	thread,
	const	pthread_attr_t *attr,
		void *		(*start_routine) (void*),
		void *		arg);
\end{lstlisting}

我们来一点点看这四个参数：

*thread是指向线程的指针，用来返回创建的线程。

*attr指定线程的属性（比如栈的大小、优先级），那么如果不需要特殊属性的话直接传NULL就行。

(*start\_routine) (void*)这是线程入口函数，该函数将在线程启动时执行。

最后*arg就是传递的一些参数。

看上去很复杂对不对？不要着急，我们来看一个具体的例子就ok：
\vspace{6pt}
\begin{lstlisting}[language=C]
#include <pthread.h>

typedef struct __myarg_t {
	int a;
	int b;
} myarg_t;

void *mythread(void *arg) {
	myarg_t *m = (myarg_t *) arg;
	printf("%d %d\n", m->a, m->b);
	return NULL;
}

int main(int argc, char *argv[]) {
	pthread_t p;
	int rc;
	myarg_t args;
	args.a = 10;
	args.b = 20;
	rc = pthread_create(&p, NULL, mythread, &args);
	...
}
\end{lstlisting}

我们主要关注pthread\_create(\&p, NULL, mythread, \&args)这行代码，可以看出来，我们在这里创建了一个进程p，没有什么特殊的属性，创建完成之后就开始运行*mythread函数，传递的参数是args，其中args.a=10，args.b=10，*mythread函数会输出m->a和m->b，也就是10和20。

那么我们已经知道怎么去创建一个线程了，现在我们再来看看我们如何等待一个线程执行完成：
\vspace*{6pt}
\begin{lstlisting}[language=C]
#include <pthread.h>
int
pthread_join(	pthread_t * 	thread,
		void **		value_ptr);
\end{lstlisting}

那么在这里显然*thread就是我们要等待的线程，**value\_ptr则用来接收线程退出时的返回值。pthread\_create 创建的线程可以通过return语句返回一个值（通常是void *类型），pthread\_join允许获取这个返回值。

\subsubsection{Java Threads}
咱们再顺带提一下Java Threads。在Java中，线程是由Java虚拟机（JVM）管理的，JVM负责线程的创建、调度和生命周期管理。Java程序通过JVM与操作系统提供的底层线程模型进行交互。

那么怎么创建Java Threads呢？有两种方式：继承Thread类或者实现Runnable接口。（不太重要，如果要看的话看PPT35页）

\subsection{Implicit Threading}
随着多核处理器的普及，程序需要使用更多的线程来提高并发性和性能。当线程数目增多时，程序的复杂性也相应增加。尤其是当程序员手动管理大量线程时，正确性和性能优化就变得越来越难。而显式线程管理需要程序员手动创建、同步和管理每个线程，这在处理大规模并发时非常复杂，容易引入竞态条件、死锁等问题。

所以呢，我们引入隐式线程。隐式线程的创建和管理不再依赖于程序员，而是由编译器和运行时库自动完成。程序员只需要指定程序的并行性需求，具体的线程创建、调度和同步交给编译器和运行时库来处理。

\begin{definition}
Thread Pool：预先创建一定数量的线程，并将它们放入一个线程池中，等待执行任务。当任务到来时，线程池中的线程会取出任务进行执行，而不是每次都创建一个新线程。
\end{definition}

用Thread pools有什么好处呢？
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 比创建新线程稍微快：使用线程池可以更快地处理任务，因为线程池中的线程已经创建好，并且准备好执行任务。与每次创建新线程相比，线程池避免了线程创建和销毁的开销。
\item 线程池中线程的数量是有限的，程序可以通过设置池的大小来控制并发的线程数。这能有效避免创建过多线程导致的资源竞争和系统负担。
\item 通过将任务执行与创建任务的机制分离，线程池允许在任务调度上使用不同的策略。例如，线程池可以周期性地执行任务，或者按照其他策略来调度任务。
\end{enumerate}

\subsection{Threading Issues}
\subsubsection{Semantics of fork() and exec()}
现在我们来思考一个问题，如果说有一个thread，要调用fork()，那么它复制的是这个process的所有threads，还是说新的process是单线程的呢？

答：新的process是单线程的，（一般情况下）只会有调用fork()的那个线程的内容。（当然有两个版本的fork()，都是存在的）
\begin{pt}
线程调用fork()，复制出来的是一个进程，不是线程。
\end{pt}

说完了fork()，我们再来看一下exec()：一旦调用 exec()，当前进程空间完全被新程序替代，包括所有线程资源也会被清除，只留下主线程执行新程序。

一个小的讨论：如果说在fork()之后直接调用exec()，那么应该复制process的所有线程还是只复制一个线程。

显而易见啊，你的资源都会被清除，肯定创建一个线程就够了。

\subsubsection{Thread Cancellation}
正常情况下线程会执行完自己的任务后自动退出。线程取消是指在外部主动发起让某个线程提前结束运行的行为。（可能是因为出现异常或者线程运行超时了吧）

有两种cancel thread的方式：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Asynchronous Cancellation（异步取消）：直接把线程杀掉，但是可能会有些资源来不及释放造成deadlock或内存泄漏。
\item Deferred Cancellation（延迟取消）：定期检查一个线程是不是该被杀掉，这样做更加安全，因为线程的资源能来得及释放。
\end{enumerate}

然后呢，调用了thread cancellation也只是请求终止线程，但是真正的终止还要看线程的状态(state)：
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Mode}&\textbf{State}&\textbf{Type}\\
\hline
Off&Disabled&---\\
Deferred&Enabled&Deferred\\
Asynchronous&Enabled&Asynchronous\\
\hline
\end{tabular}
\end{center}

在这个表格当中，Mode（取消模式）：表示线程是否允许被取消，以及如何被取消；State（启用状态）：Disabled：取消请求挂起（pending），线程暂时不会响应；Enabled：线程准备好响应取消；Type（取消类型）：Deferred：线程在特定位置（取消点）才响应取消；Asynchronous：线程几乎立即响应取消请求。

\begin{pt}
对于一个线程来说，不出意外的话它只能是表中三个状态中的一个。（当然，可以是\textbf{Mode为Deferred，State为Disabled，这个时候Type就没有}，这是可以的）
\end{pt}

那么如果Mode为Off的时候，你就没办法把这个线程删掉，除非state后面编程enabled。

默认情况下会选用Deferred的Mode和Type，那么线程只有到达cancellation point才会被cancel掉，这个时候就会释放资源，删掉线程。

最后，在Linux操作系统当中，线程的取消是用signals来完成的。

\subsubsection{Signal Handling}
\begin{definition}
Signal：一种异步通知机制，用于通知进程发生了特定的事件（常见的事件包括ctrl+C、除以0的非法访问等）；signal本质是内核向进程发送的中断请求。
\end{definition}

\begin{definition}
Signal Handler：用来处理信号，一般有两种处理方式：

一种是Default handler（默认处理器）：系统自动为每种信号定义了默认动作；另一种是User-defined handler（用户自定义处理器）：为某些信号编写自定义响应函数。每个signal都会有一个default handler，但如果程序员自定义了一个user-defined signal handler，那它就会覆盖default handler，这很好理解。
\end{definition}

所以说信号的传递过程也非常的简单：某个事件发生，触发信号；信号被送到process；signal handlers（default或者user-defined）来处理这些信号。

\textbf{但是，很遗憾的是，上面只针对单线程的情况，signal直接送到对应的process就结束了。}
\vspace{6pt}
\begin{lstlisting}[language=C]
#include <stdio.h>
#include <signal.h>
#include <stdlib.h>
#include <unistd.h>

// 自定义信号处理函数
void handle_sigint(int sig) {
    printf("\n🚨 收到 SIGINT（信号编号 %d），正在优雅退出...\n", sig);
    exit(0);  // 正常退出程序
}

int main() {
    // 注册 signal handler
    signal(SIGINT, handle_sigint);

    printf("程序运行中，按 Ctrl+C 触发 SIGINT 信号...\n");

    // 无限循环，等待信号到来
    while (1) {
        printf("🟢 程序继续运行中...\n");
        sleep(1);  // 每秒打印一次
    }

    return 0;
}
\end{lstlisting}

那么我们现在还得想想如果是一个multi-threaded的process该怎么办呢？有下面四个方法，由process来决定用哪个方案：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 直接送到信号应用的线程：适用针对某个特定线程触发的信号（比如 pthread\_kill (tid, sig)）
\item 把信号送给所有线程：不太常见
\item 某些线程可接收信号，其它线程忽略，例如通过线程屏蔽 sigprocmask() 控制。
\item 分配一个专门的线程来负责接收和处理所有信号，其他线程则屏蔽信号。（最常见的方法）
\end{enumerate}

还有一个小问题：每个线程能有自己的handler吗？

不可以！Handler是进程级共享的，也就是说进程里的所有threads用的都是一样的signal handler，如果你要搞特殊，你就屏蔽信号就可以了。

\subsubsection{Thread-Local Storage}
那么，我们如何在执行 counter++ 时实现并行性？只要让每个线程分别进行计数，最终汇总结果就可以了。

这时候就可以引入我们的TLS(Thread-local storage)啦，\textbf{它允许每个线程有自己的数据，这些数据不会被其他线程访问}。这是多线程编程中的一个重要概念，特别是当你使用线程池时（无法控制线程的创建过程），TLS 可以确保每个线程都有其独立的数据。

那么TLS和局部变量有什么区别呢？

局部变量只在函数调用期间有效，它们的作用范围仅限于当前函数。\textbf{TLS则是跨函数调用有效的}，它可以保证每个线程在整个生命周期中都有独立的存储空间。

那么TLS和静态数据又有什么区别呢？

虽然TLS和静态数据都存储在程序的特定位置，但TLS是为每个线程单独分配的。每个线程都有自己的TLS数据，而静态数据是所有线程共享的。

\begin{ext}
TLS不在data section，也不在thread的stack里面，一般存在thread的TCB里面。
\end{ext}

\subsubsection{Scheduler Activations}
还有最后一个Issue，再坚持一下呀！

现在还有一个问题，在M:M或者是Two-level模型里，如何维护一个合适数量的kernel threads？答：Communication（communicate的内容包括用户线程要什么时候执行、内核什么时候有资源分配）.但是问题是，谁知道有多少个user threads呢？只有用户自己知道，kernel是不知道的。

\begin{definition}
Lightweight process：是一个内核级进程，因为用户进程不能直接运行在CPU上，所以要绑定一个LWP来实现调度。怎么实现的呢？首先操作系统通过短期调度决定kernel space的哪个kernel thread获得CPU，然后LWP相当于把这些CPU时间带到user sapce里面，分给下面的user thread。
\end{definition}

\begin{pt}
每个LWP对应一个kernel thread。
\end{pt}

\begin{definition}
Scheduler activations（调度激活）：当发生关键事件（如线程阻塞、唤醒、CPU 可用等）时，内核会主动通知用户线程库
\end{definition}

\begin{definition}
Upcall：一种由内核空间的通信机制，这样线程库可以及时做出响应，比如：重新调度用户线程；释放或复用 LWP；创建更多 LWP 等。
\end{definition}

那么有upcall也一定有downcall对吧。
\begin{definition}
Downcall：比方说有两个process A和B，这时候A要B为它服务，就叫了一个procedure call给B，让B来处理，这时候B就会进入kernel space来处理这个问题。（说白了就是system call）
\end{definition}

做一个小总结，我要运行user thread的时候，先把它们绑定到LWP上，实际上是由kernel thread执行的。

\newpage
\section{Exercise}
\begin{example}
阅读下面的代码，输出的结果是什么？
\end{example}
\vspace*{6pt}
\begin{lstlisting}[language=C]
#include <stdio.h>
#include <pthread.h>
#include <assert.h>
#include <stdlib.h>
 
typedef struct __myarg_t {
 int a;
 int b;
} myarg_t;
 
typedef struct __myret_t {
 int x;
 int y;
} myret_t;
 
void *mythread(void *arg) {
 myarg_t *m = (myarg_t *) arg;
 printf("%d %d\n", m->a, m->b);
 myret_t *r = Malloc(sizeof(myret_t));
 r->x = 1;
 r->y = 2;
 return (void *) r;
}

int main(int argc, char *argv[]) {
 int rc;
 // 这里似乎没有用到
 pthread_t p;
 myret_t *m;
 myarg_t args;
 args->a = 10;
 args->b = 20;
 Pthread_create(&p, NULL, mythread, &args);
 Pthread_join(p, (void **) &m);
 printf("returned %d %d\n", m->x, m->y);
 return 0;
}
\end{lstlisting}

Key:

其实很简单，首先Pthread\_create调用mythread函数，输出10 20；之后再有Pthread\_join获取mythread的返回值，也就是*r，输出r->x和r->y，也就是1和2。

所以答案就是：

10 20

returned 1 2

\begin{example}
阅读下面的代码，写出可能的输出（Process和Thread章节的难度天花板，但同时也是一道真题）
\end{example}
\vspace*{6pt}
\begin{lstlisting}[language=C]
/* kai.c */
#include <stdio.h>
#include <pthread.h>
void *helloFunc(void *ptr) {
	int *data; 
	data = (int *) ptr; 
	printf("I'm Thread %d\n", *data);
	return (void *) data; 
} 
int main(int argc, char *argv[]) {
	pthread_t hThread[4]; 
	int *rvals[4];
	for (int i = 0; i < 4; i ++)
		pthread_create(&hThread[i], NULL, helloFunc, (void *) &i);
	for (int i = 0; i < 4; i ++) {
		pthread_join(hThread[i], (void **) &rvals[i]);
		//printf("Thread %d returns %d\n", i, *rvals[i])
	}
	return 0;
}
\end{lstlisting}
\vspace*{6pt}
\begin{lstlisting}[language=sh]
prompt> gcc -o kai kai.c -pthread -Wall
prompt> ./kai
\end{lstlisting}

Key:

虽然说创建了4个Threads是没有问题的，但是pthread\_create(\&hThread[i], NULL, helloFunc, (void *) \&i)中传的i的值可能不是想象中的样子。在线程执行的时候，读取的是“当前”状态下的i的值，当然也可能出现后面创建的线程会先执行，但是因为i都是“当前”状态下的，所以输出的序列一定是升序的，那可能有哪些情况呢？

0,1,2,3;0,1,2,4;0,1,3,4...3,4,4,4;4,4,4,4

\begin{pt}
出现4的情况是，循环的时候i加到4了，但是不再进入下一个循环了。
\end{pt}

\begin{example}
Suppose an I/O-bound application which has five different file-read requests occur simultaneously. At some moment, two kernel threads are allocated to the application.
\begin{enumerate}[label=\arabic*., leftmargin=1em]
\item How many LWPs are created?
\item At some other moment, one kernel thread blocks (waiting for I/O completion)

\textbf{T/F?}This kernel thread makes an upcall, and the attached LWP blocks the current user thread, and schedules another user thread to run.

\textbf{T/F?}The attached LWP also blocks. The kernel makes an upcall and then allocates a new LWP to the application.

\item After returning from upcall handling, how many LWPs now?
\item If we wish to run this application efficiently, how many LWPs are needed for this application.
\end{enumerate}

\end{example}

Keys:
\begin{enumerate}[label=\arabic*., leftmargin=1em]
\item 因为是两个kernel threads，所以显然就是2个LWP啦。
\item F/T

这两道题显然一个是对的一个是错的。那么当一个kernel thread阻塞之后，正确的情况是，对应的LWP也会阻塞。然后kernel就会做一个upcall去和user space说，再分配一个新的LWP。
\item 因为添加了1个，所以现在是3个。
\item 因为有5个任务，所以5个LWP是最好的。
\end{enumerate}





\newpage
\section{Concepts Organization}
\begin{enumerate}[label=\arabic*., leftmargin=3em]
\item Threads: A thread is a unit of execution within a process. Multiple threads can exist within a single process, sharing resources such as memory, data, and files, which allows them to communicate more efficiently compared to separate processes.
\item Concurrency: Supports more than one task making progress.
\item Parallelism: A system can perform more than one task simultaneously.
\item Kernel thread: A thread managed and scheduled by the operating system kernel, interacting directly with hardware.
\item User threads: A thread created and managed in user space, executing tasks through system calls but not interacting directly with hardware.
\item Multithreading models:many-to-one、one-to-one、many-to-many.
\item Thread Pools: A thread pool is a collection of pre-instantiated, idle threads that are ready to perform tasks. 
\item Thread cancellation: Terminate a thread before it has finished.
\item Signal handling: Process signals with default handler or user-defined handler.
\item Thread-local storage: Allows each thread to have its own data.(Not global varaible but available between functions)
\item Lightweight process: LWP is a kernel-level abstraction for managing user threads. It allows user threads to interact with the kernel, which handles scheduling and resource allocation.
\end{enumerate}



\chapter{CPU\_Scheduling}
\section{In-class Contents}
\subsection{Warm-up}
Multiprogramming当中，每个Process在内存中占的空间都是独立的，且只能由自己访问，除非process之间设置了信息交换的通道(message passing, shared memeory)。

开销user thread<kernel thread<process

\begin{exam}
谁决定哪个process下一个运行？CPU Scheduler(in the kernel)

谁来实现context switch(save and restore)？Dispatcher.
\end{exam}

\begin{exam}
CPU调度的最小单位是thread，分配资源的最小单位是process（因为线程不算有独立的资源）。
\end{exam}

\subsection{Basic Concepts}
\subsubsection{CPU Scheduler}
由Short-term scheduler在ready queue中选一个process，并且把CPU分配给它，那这样的ready queue的排序方式就有很多种啦，比如FCFS、SJF等等。

那什么时候需要CPU做出调度决定呢？
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Switch from running to waiting state（一般是要做I/O）

nonpreemptive（非抢占式）：因为是process自愿将CPU让出来的。\textbf{（主动放弃CPU cycle）}
\item Switch from running to ready state（例如timer interrupt）

preemptive（抢占式）：OS强制剥夺了你的CPU，让其他process运行\textbf{（被动放弃CPU cycle）}
\item Switch from waiting to ready（我的I/O做完了，我现在要重新回到ready queue准备运行）

preemptive：可能不是那么明显，可以这样想：对于在running的process来说，它的CPU cycle是不是很可能被这个从waiting到ready的process给抢掉呢？（有点间接不过）
\item Terminate

nonpreemptive：自己结束的那肯定是非抢占了。
\end{enumerate}

那么抢占会有些什么问题呢？
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 数据共享可能出现问题
\item 如果内核模式正在执行系统调用，比如内存分配或文件写入，这时发生抢占可能会非常危险。
\item 在操作系统关键活动（如调度器运行、进程切换）期间如果发生中断，也要谨慎处理
\end{enumerate}

\subsubsection{Dispatcher}
调度器模块做的事情很简单，就是把CPU的控制权交给short-term scheduler选中的process，一共完成了这三个步骤：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item switch context：保存当前process的machine states，并且加载下一个进程的machine states。（\textbf{调度最核心也是最耗时间的部分}）
\item switching to user mode：进程在kernel mode完成调度后，切回用户模式运行用户级代码
\item 跳转到用户程序上次执行中断/暂停的位置继续运行。
\end{enumerate}

\begin{definition}
Dispatch latency（调度延迟）：dispatcher所花的从stop一个Process到让另一个process开始running的时间，影响因素包括context swtich的开销、CPU模式切换的开销以及硬件和OS的工作效率。
\end{definition}

\begin{ext}
Read-time OS可以分为hard real-time（立刻做出反应）和soft real-time（尽快做出反应）
\end{ext}

\subsection{Scheduling Criteria}
\textbf{这里，全是重点！！！！！}

5个scheduling criteria：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item CPU utilization：让CPU越忙越好（现代计算机基本上是100\%）
\item Throughput（吞吐量）：单位时间内完成的进程的数量
\item Turnaround time（周转时间）：一个进程从arrive到complete的时间，包括在ready queue等待的时间(waiting time)、Running的时间和I/O的时间。
\item Waiting time：process呆在ready queue的\textbf{时间总和}（也就是说，如果process从running再到waiting去做I/O，再回到ready state，后面的时间是要一起加上的）
\item Response time：从请求提交到系统第一次给出响应的时间，不管你啥时候做完，也不管你后面会不会再回到ready queue(for time-sharing environment)。比方说，拷贝一个电影，拷贝请求提交到那个拷贝窗口出现的时间，就是respnse time。（时间片轮转RR就能保证较低的响应时间）
\end{enumerate}

\subsection{Simple Scheduling Algorithms}
\textbf{这块部分很重要啊}，我们一点一点来剖析：

首先，我们很不要脸，我们做五个很强的假设：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Each job runs for the same amount of time. A,B,C
\item All jobs arrive at the same time. $T_{arrival}$
\item Once started, each job runs to completion.
\item All jobs only use the CPU (they perform no I/O)
\item The run-time of each job is known.
\end{enumerate}

\subsubsection{FCFS}
First come, first served调度算法（对于进程来说先来先服务），有的时候也被称为FIFO(Firt in, first out)（对于存储来说是这样的）。

\begin{example}
假设说我们有3个进程，A，B，C，几乎同时到达。我们假设A比B比C早来那么一点点（只是做一个区分），同时假设每个任务要10s完成。

现在我们来算一算average turnaround time。

先画一个Gantt Chart：
\begin{center}
\begin{adjustbox}{scale={.8}}
\begin{ganttchart}[hgrid, vgrid, inline, x unit=5mm, y unit chart=.8cm, y unit title=0.5cm, title/.style={draw=none, fill=none}, include title in canvas=false, bar top shift=0, bar height=1, ]{1}{30} 
\gantttitlelist{1,...,30}{1} \\
\ganttbar{A}{1}{10}
\ganttbar{B}{11}{20}
\ganttbar{C}{21}{30}
\end{ganttchart}
\end{adjustbox}
\end{center}

那么就很容易算了啊，A在10s结束，B在20s结束，C在30s结束；average turnaround时间就是(10+20+30)/3=20。
\end{example}

\begin{pt}
这个案例当中，FCFS是Optimal solution。
\end{pt}

现在我们来放宽一下条件：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item \sout{Each job runs for the same amount of time. A,B,C}
\item All jobs arrive at the same time. $T_{arrival}$
\item Once started, each job runs to completion.
\item All jobs only use the CPU (they perform no I/O)
\item The run-time of each job is known.
\end{enumerate}

这个情况下FCFS的表现怎么样呢？什么情况的workload会让FCFS表现更差呢？（其实让慢的先做就是最差的情况）

\begin{example}
现在A要运行10s，B和C只要运行1s，其他都不变，我们再来算算turnaround time：
\begin{center}
\begin{adjustbox}{scale={1}}
\begin{ganttchart}[hgrid, vgrid, inline, x unit=5mm, y unit chart=.8cm, y unit title=0.5cm, title/.style={draw=none, fill=none}, include title in canvas=false, bar top shift=0, bar height=1, ]{1}{12} 
\gantttitlelist{1,...,12}{1} \\
\ganttbar{A}{1}{10}
\ganttbar{B}{11}{11}
\ganttbar{C}{12}{12}
\end{ganttchart}
\end{adjustbox}
\end{center}

turnaround time是(10+11+12)/3=11。
\end{example}

\begin{exam}
Convoy effect（护航效应）：慢的process先做，然后是快的process做，这样显然是不好的。
\end{exam}

那么其实要降低turnaround time，一个直观的想法就是先让B和C做，A最后做。

\subsubsection{SJF}
所以我们就有这样一个新的调度策略啦——Shortest Job First（也是这个情况下最优的）。

那对于上面那个例题，我们就可以再画一个新的Gantt Chart：
\begin{center}
\begin{adjustbox}{scale={1}}
\begin{ganttchart}[hgrid, vgrid, inline, x unit=5mm, y unit chart=.8cm, y unit title=0.5cm, title/.style={draw=none, fill=none}, include title in canvas=false, bar top shift=0, bar height=1, ]{1}{12} 
\gantttitlelist{1,...,12}{1} \\
\ganttbar{A}{3}{12}
\ganttbar{B}{1}{1}
\ganttbar{C}{2}{2}
\end{ganttchart}
\end{adjustbox}
\end{center}

这时候的turnaround time就是(12+1+2)/3=5。

\begin{exam}
另外，SJF算法是一优先级调度算法的一个特例，也就是说shorter job有higher priority。
\end{exam}

\begin{center}
\begin{table}[H]
\centering
\begin{tabular}{c c c}
Procsss&Burst Time&Priority\\
\hline
$P_1$&10&3\\
$P_2$&1&1\\
$P_3$&2&4\\
$P_4$&1&5\\
$P_5$&5&2\\
\end{tabular}
\end{table}
\end{center}

我们来看上面这个表格，画一个Gantt Chart（注意我们认为1比2的priority更高，以此类推）：
\begin{center}
\begin{adjustbox}{scale={1}}}
\begin{ganttchart}[hgrid, vgrid, inline, x unit=5mm, y unit chart=.8cm, y unit title=0.5cm, title/.style={draw=none, fill=none}, include title in canvas=false, bar top shift=0, bar height=1, ]{1}{19} 
\gantttitlelist{1,...,19}{1} \\
\ganttbar{$P_2$}{1}{1}
\ganttbar{$P_5$}{2}{6}
\ganttbar{$P_1$}{7}{16}
\ganttbar{$P_3$}{17}{18}
\ganttbar{$P_4$}{19}{19}
\end{ganttchart}
\end{adjustbox}
\end{center}

这个情况下是turnaround time=(1+6+16+18+19)/5=12.

那这会有什么问题呢？

\begin{definition}
Starvation：低prioriy的process可能永远不会执行。
\end{definition}

\begin{pt}
只要最后这个process能执行，它就不会starvation。
\end{pt}

那么怎么解决这个问题嘞？
\begin{definition}
Aging：随着时间增加，process的priority也会随之增加。
\end{definition}

还有一个SJF的变种：HRRN(Highest Response Ratio Next)，但是吧，它不考，所以想了解的话看PPT22页就行。

现在我们再放宽一下条件：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item \sout{Each job runs for the same amount of time. A,B,C}
\item \sout{All jobs arrive at the same time. $T_{arrival}$}
\item Once started, each job runs to completion.
\item All jobs only use the CPU (they perform no I/O)
\item The run-time of each job is known.
\end{enumerate}

那这个时候会不会让SJF表现的不那么好呢？其实是会的，我们看下面这个例子：

A在T=0的时候到达，需要10s，B和C在T=1的时候到达，需要执行1s。

\begin{center}
\begin{adjustbox}{scale={1}}
\begin{ganttchart}[hgrid, vgrid, inline, x unit=5mm, y unit chart=.8cm, y unit title=0.5cm, title/.style={draw=none, fill=none}, include title in canvas=false, bar top shift=0, bar height=1, ]{1}{12} 
\gantttitlelist{1,...,12}{1} \\
\ganttbar{A}{1}{10}
\ganttbar{B}{11}{11}
\ganttbar{C}{12}{12}
\end{ganttchart}
\end{adjustbox}
\end{center}

这种情况下的turnaround time为(10+(11-1)+(12-1))/3=10.33。

那么为了解决这个问题，我们需要放宽第三个条件：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item \sout{Each job runs for the same amount of time. A,B,C}
\item \sout{All jobs arrive at the same time. $T_{arrival}$}
\item \sout{Once started, each job runs to completion.}
\item All jobs only use the CPU (they perform no I/O)
\item The run-time of each job is known.
\end{enumerate}

\subsubsection{Preemptive SJF}
\begin{definition}
Preemptive: A job can preempt another job.
\end{definition}

所有的现代schedulers都是preemptive的。

现在我们来看看Preemptive Shortest Job First(preemptive SJF) or Shortest Time-to-Completion First(STCF), of Shortest-Remaining-Time First(SRTF)

\begin{pt}
\textbf{一定要关注是SJF还是Preemptive SJF，很重要！！！}
\end{pt}

我们再回顾上面的那个例子：

\begin{center}
\begin{adjustbox}{scale={1}}
\begin{ganttchart}[hgrid, vgrid, inline, x unit=5mm, y unit chart=.8cm, y unit title=0.5cm, title/.style={draw=none, fill=none}, include title in canvas=false, bar top shift=0, bar height=1, ]{1}{12} 
\gantttitlelist{1,...,12}{1} \\
\ganttbar{A}{1}{1}
\ganttbar{B}{2}{2}
\ganttbar{C}{3}{3}
\ganttbar{A}{4}{12}
\end{ganttchart}
\end{adjustbox}
\end{center}

这个时候的average turnaround time=(12+(2-1)+(3-1))/3=5，这显然比前面的10.33表现要很多了对吧。

当然和SJF一样，会有这么一个问题：Starvation。

这个时候我们就不能只把目光放在turnaround time了，我们还要考虑考虑其他的指标，比如说——fairness（也就是让response time能够最少）。

\begin{definition}
Response time：就是一个job arrive到它第一次被调度执行的时间。
\end{definition}

Preemptive SJF在response time做的不好->想到可以使用时间片轮转Round Robin(RR)调度策略，让每个process都能定期获得CPU的时间。

\subsubsection{RR}
RR做的事情就是，它不要求一定要把一个任务完成，每个process轮流使用CPU cycle，RR有的时候被称为time-slicing（时间分片）。

\begin{example}
Assume three jobs A, B, and C arrive at the same time in the system, and that they each wish to run for 5 seconds, the length of time slice is 1 second.
\begin{center}
\begin{adjustbox}{scale={1}}
\begin{ganttchart}[hgrid, vgrid, inline, x unit=5mm, y unit chart=.8cm, y unit title=0.5cm, title/.style={draw=none, fill=none}, include title in canvas=false, bar top shift=0, bar height=1, ]{1}{15} 
\gantttitlelist{1,...,15}{1} \\
\ganttbar{A}{1}{1}
\ganttbar{B}{2}{2}
\ganttbar{C}{3}{3}
\ganttbar{A}{4}{4}
\ganttbar{B}{5}{5}
\ganttbar{C}{6}{6}
\ganttbar{A}{7}{7}
\ganttbar{B}{8}{8}
\ganttbar{C}{9}{9}
\ganttbar{A}{10}{10}
\ganttbar{B}{11}{11}
\ganttbar{C}{12}{12}
\ganttbar{A}{13}{13}
\ganttbar{B}{14}{14}
\ganttbar{C}{15}{15}
\end{ganttchart}
\end{adjustbox}
\end{center}

这种情况下的response time表现很好，是(0+1+2)/3=1，但是也会有一个问题，就是频繁的上下文切换会导致开销有点高。所以这也就说明了，time slice不是越小越好。
\end{example}

那么RR在turnaround time方面做的是不是就没那么好了？确实是这样，和FCFS和SJF相比，turnaround time确实会长一些，那这只能说是response time和turnaround time的trade-off了。

\subsubsection{Incorporating I/O}
现在我们再放开一下条件，加一点I/O进去玩玩：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item \sout{Each job runs for the same amount of time. A,B,C}
\item \sout{All jobs arrive at the same time. $T_{arrival}$}
\item \sout{Once started, each job runs to completion.}
\item \sout{All jobs only use the CPU (they perform no I/O)}
\item The run-time of each job is known.
\end{enumerate}

\begin{example}
Assume two jobs, A and B. A runs for 1 second and then issues an I/O request which also takes 1 second, B simply uses the CPU for 5 seconds and performs no I/O.

那么因为有I/O Requests，我们可以把任务B拆成子任务，Gantt Chart如下图所示：
\begin{center}
\begin{adjustbox}{scale={1}}
\begin{ganttchart}[hgrid, vgrid, x unit=5mm, y unit chart=.8cm, y unit title=0.5cm, title/.style={draw=none, fill=none}, include title in canvas=false, canvas/.style={draw=black, fill=black!20}, bar top shift=0, bar height=1, ]{1}{11} 
\gantttitlelist{1,...,11}{1} \\
\ganttbar{CPU}{1}{1}
\ganttbar[inline]{A}{1}{1}
\ganttbar[inline]{B}{2}{2}
\ganttbar[inline]{A}{3}{3}
\ganttbar[inline]{B}{4}{4}
\ganttbar[inline]{A}{5}{5}
\ganttbar[inline]{B}{6}{6}
\ganttbar[inline]{A}{7}{7}
\ganttbar[inline]{B}{8}{8}
\ganttbar[inline]{A}{9}{9}
\ganttbar[inline]{B}{10}{10}
\ganttbar[inline]{A}{11}{11}\\
\ganttbar{I/O}{2}{2}
\ganttbar[inline]{A}{2}{2}
\ganttbar[inline]{A}{4}{4}
\ganttbar[inline]{A}{6}{6}
\ganttbar[inline]{A}{8}{8}
\ganttbar[inline]{A}{10}{10}
\end{ganttchart}
\end{adjustbox}
\end{center}
\end{example}

最后呢，我们放宽所有条件：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item \sout{Each job runs for the same amount of time. A,B,C}
\item \sout{All jobs arrive at the same time. $T_{arrival}$}
\item \sout{Once started, each job runs to completion.}
\item \sout{All jobs only use the CPU (they perform no I/O)}
\item \sout{The run-time of each job is known.}
\end{enumerate}

那么问题来了，我们没有所有信息，怎么去进行调度呢？我们如何做到fairness和performance的trade-off呢？

如果我们用的是SJF调度算法，那我们需要去“猜”下一个进程的CPU burst需要多久，然后选择最短的那个去执行。那么计算的公式是$\tau_{n+1}=\alpha t_{n}+(1-\alpha)\tau_{n}$，然后$\alpha$的取值一般为0.5.

\subsubsection{Multilevel Queue}
现在我们来讲一下多级队列调度，具体是一种混合型调度策略。

说白了，ready queue会被分成两个部分，一个是foreground（前台，主要做交互式任务，例如移动鼠标），另一个部分是background（后台，面向批处理任务，比如说大型计算）。

\begin{pt}
每个process会被永久分配到一个"队列"，要么是foreground里面，要么是background，不能跨"队列"移动。（但是！\textbf{要注意一个事情是，本质上我们只有这样一个ready queue}）
\end{pt}

每个“队列”有它自己的调度算法：对于foreground来说用的是RR调度算法，它更加公平一些，对于background来说用的是FCFS调度算法。

那么具体我们会怎么做调度呢？
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Fixed priority scheduling：先服务foreground“队列”中的process，然后服务background，那么显然这可能会有starvation的可能。
\item Time slice：为foreground和background分别分配CPU时间，比如说给foreground分配80\%，调度算法是RR，给background分配20\%，调度算法是FCFS。
\end{enumerate}

\begin{example}
我们可以举一个小小的例子，以time slice为例：

假设有三个进程A、B、C（都需要4s才能完成），A、B在foreground queue当中，RR的quantum=2，分配80\%的时间；C在background queue当中，分配20\%的时间。

\begin{center}
\begin{ganttchart}[hgrid, vgrid, inline, x unit=5mm, y unit chart=.8cm, y unit title=0.5cm, 
    title/.style={draw=none, fill=none}, include title in canvas=false, 
    bar top shift=0, bar height=1]{0}{9}
\gantttitlelist{0,...,9}{1} \\

\ganttbar{A}{0}{1}
\ganttbar{B}{2}{3}
\ganttbar{C}{4}{4}
\ganttbar{A}{5}{6}
\ganttbar{B}{7}{8}
\ganttbar{C}{9}{9}

\end{ganttchart}
\end{center}
\end{example}

\subsection{Multilevel Feedback Queue}
我们希望找一个算法，能够在不完全了解每个进程的执行情况的时候也能完成调度，这时候，诶嘿，我们就想到了用Multi-level Feedback Queue(MLFQ)，目的是optimize turnaround time和minimize response time。

当然，在这之前，我们先回顾一下MLQ，它有多个不同的“队列”，并且队列间有不同的优先级。

对于MLQ来说有两个basic rules:
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 如果A的优先级比B的优先级高，则A运行
\item 如果A和B的优先级相同，那么A和B都用RR调度算法
\end{enumerate}

在MLQ里，process的priority是固定的，但这不是我们希望的样子，我们不希望看到starvation，所以我们希望能够调整process的priority，于是我们需要一个"feedback"，比如说如果一个进程占用 CPU 很久，它会被系统降级到低优先级队列。

\begin{pt}
MLFQ是行动驱动的调度策略，通过观察一个进程“是否经常使用完时间片”、“是否经常被抢占”等行为来调整其优先级。
\end{pt}

我们来看看一个新来的任务，MLFQ会怎么进行调度：

首先，一个新的任务来的时候，scheduler不知道它是个long job还是short job，所以scheduler就假定它是一个short job，并给它很高的优先级。同时在运行过程中观察这个job的行为，如果它频繁使用完时间片，那么说明它是个长任务，就会慢慢降低它的优先级；如果很快就完成了，那很好，说明它是一个短任务，也值得很高的优先级。

我们用两张图直观地来看一下Priority的变化：
\begin{pt}
\textbf{再次注意，这实际上是一个Queue！！！}
\end{pt}

对于一个long-running job A:
\begin{center}
\begin{adjustbox}{scale={1}}
\begin{ganttchart}[hgrid, vgrid, x unit=5mm, y unit chart=.8cm, y unit title=0.5cm, title/.style={draw=none, fill=none}, include title in canvas=false, canvas/.style={draw=black, fill=black!20}, bar top shift=0, bar height=1, ]{1}{20} 
\gantttitlelist{1,...,20}{1} \\
\ganttbar{$Q_2$}{1}{2}
\ganttbar[inline]{$A$}{1}{2}\\
\ganttbar{$Q_1$}{3}{4}
\ganttbar[inline]{$A$}{3}{4}\\
\ganttbar{$Q_0$}{5}{5}
\ganttbar[inline]{$A$}{5}{20}
\end{ganttchart}
\end{adjustbox}
\end{center}

那在这之后又来了一个短任务B呢？
\begin{center}
\begin{adjustbox}{scale={1}}
\begin{ganttchart}[hgrid, vgrid, x unit=5mm, y unit chart=.8cm, y unit title=0.5cm, title/.style={draw=none, fill=none}, include title in canvas=false, canvas/.style={draw=black, fill=black!20}, bar top shift=0, bar height=1, ]{1}{20} 
\gantttitlelist{1,...,20}{1} \\
\ganttbar{$Q_2$}{11}{12}
\ganttbar[inline]{$B$}{11}{12}\\
\ganttbar{$Q_1$}{13}{14}
\ganttbar[inline]{$B$}{13}{14}\\
\ganttbar{$Q_0$}{1}{1}
\ganttbar[inline]{$A$}{1}{10}
\ganttbar[inline]{$A$}{15}{20}
\end{ganttchart}
\end{adjustbox}
\end{center}

好，那么好，我们现在来看看MLFQ的Rules（MLQ+Feedback）:
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 如果A的优先级比B高，那么A先跑
\item 如果A和B的优先级一样，那么A和B都用RR跑
\item 如果有一个任务进到系统，那么它会被放在最高的优先级
\item 如果一个任务在跑的时候\textbf{用完了一整个time slice}，那么它的priority会降低
\item 如果一个任务在time slice还没用完时自己放弃CPU，那么它的prioriy就不变
\end{enumerate}

那么这个版本的MLFQ的表现已经达到最好了吗？

显然它是有一些缺陷的，比如说：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Starvation：很好理解啊，如果说一直有新的任务来，那么它们就一直是最高的优先级，那么低优先级的就永远无法执行。
\item Gaming the scheduler:也很好理解，就是在卡bug嘛，我就只用99\%的time slice然后\textbf{主动放弃CPU（可能是去做I/O了）}，这样我的priority就不会掉下去。
\item Changeable program behaviors:process最开始大量计算（CPU 密集），之后等待用户输入选项（变成交互型），但因为它已经被“打入冷宫”，所以响应变慢。（\textbf{所以，还应该有priority上升的策略}）
\end{enumerate}

那么怎么解决这些问题呢？

其实也很简单，就是过S秒之后，把系统中的所有jobs都变成top priority，给所有任务一个机会，这可以解决什么问题呢？

一个是processes不会starve；另一个是如果某个一开始是 CPU 密集型任务，后来行为变成了交互型（I/O 密集），那它也能被重新识别出来并享受交互任务的待遇。

同时，我们为了不让process卡bug，我们在改变优先级的时候，用以下策略：

现在我们不管一个process分成多少次用分配给它的CPU time，只要它用完了，就降级。

这样可以解决一个什么问题呢？我们看下面的Gantt Chart，上面是可以卡bug的情况，下面是更新后的情况，假设time slice=5

\begin{center}
\begin{ganttchart}[hgrid, vgrid, x unit=5mm, y unit chart=.8cm, y unit title=0.5cm, title/.style={draw=none, fill=none}, include title in canvas=false, canvas/.style={draw=black, fill=black!20}, bar top shift=0, bar height=1, ]{1}{30} 
\gantttitlelist{1,...,30}{1} \\
\ganttbar{$Q_2$}{2}{5}
\ganttbar[inline]{$B$}{2}{5}
\ganttbar[inline]{$B$}{7}{10}
\ganttbar[inline]{$B$}{12}{15}
\ganttbar[inline]{$B$}{17}{20}
\ganttbar[inline]{$B$}{22}{25}
\ganttbar[inline]{$B$}{27}{30}\\
\ganttbar{$Q_1$}{40}{40}\\
\ganttbar{$Q_0$}{1}{1}
\ganttbar[inline]{$A$}{1}{1}
\ganttbar[inline]{$A$}{6}{6}
\ganttbar[inline]{$A$}{11}{11}
\ganttbar[inline]{$A$}{16}{16}
\ganttbar[inline]{$A$}{21}{21}
\ganttbar[inline]{$A$}{26}{26}
\end{ganttchart}
\begin{ganttchart}[hgrid, vgrid, x unit=5mm, y unit chart=.8cm, y unit title=0.5cm, title/.style={draw=none, fill=none}, include title in canvas=false, canvas/.style={draw=black, fill=black!20}, bar top shift=0, bar height=1, ]{1}{30} 
\gantttitlelist{1,...,30}{1} \\
\ganttbar{$Q_2$}{2}{2}
\ganttbar[inline]{$B$}{2}{5}
\ganttbar[inline]{$B$}{7}{7}\\
\ganttbar{$Q_1$}{8}{10}
\ganttbar[inline]{$B$}{8}{10}
\ganttbar[inline]{$B$}{12}{13}\\
\ganttbar{$Q_0$}{1}{1}
\ganttbar[inline]{$A$}{1}{1}
\ganttbar[inline]{$A$}{6}{6}
\ganttbar[inline]{$A$}{11}{11}
\ganttbar[inline]{$B$}{14}{15}
\ganttbar[inline]{$A$}{16}{20}
\ganttbar[inline]{$B$}{21}{24}
\ganttbar[inline]{$A$}{25}{29}
\ganttbar[inline]{$B$}{30}{30}
\end{ganttchart}
\end{center}

我们可以发现，更新之后，B就不能卡bug了，它在用完了自己的5秒时间片之后，就降级了。

当然，最后还有一点，对于不同等级的"queue"，在做RR的时候，分配到是time slice可以是不一样的，等级越低，分配的time slice长度越长（因为等级低的大多是CPU-Bound job嘛）。

那么现在我们可以看看MLFQ的最终的五个Rules:
\begin{itemize}
\item \textcolor{blue}{Rule 1}: If Priority(A) $>$ Priority(B), A runs (B doesn't).
\item \textcolor{blue}{Rule 2}: If Priority(A) = Priority(B), A \& B run in RR.
\item \textcolor{blue}{Rule 3}: When a job enters the system, it is placed at the highest priority (the topmost queue).
\item \textcolor{blue}{Rule 4}: Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue).
\item \textcolor{blue}{Rule 5}: After some time period S, move all the jobs in the system to the topmost queue.
\end{itemize}

\subsection{Lottery Scheduling}
\textbf{这个部分不考！！！}

现在呢，我们想要改变我们的目标，我们不再要求最快的turnaround time或者是response time，我们只是想要保证每个job都能得到一定时间的CPU cycle。那这个时候传统的调度算法就不是很好了，所以我们这里采用的是Lottery Scheduling，在这种情况下，我们可以比方说给Linux分配75\%的CPU cycle，然后给Windows25\%的CPU cycle。

\begin{definition}
Proportional Share：Instead of optimizing for turnaround or response time, a scheduler might instead try to guarantee that each job obtain a certain percentage of CPU time.（这个很重要，能够用来管理不同的虚拟机或者是操作系统）
\end{definition}

那么要实现Lottery Scheduling其实也很简单，每个process都有一定数量的lottery，系统抽到谁，谁就获得CPU Cycle.

在抽奖的时候也会有下面三个特性：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Ticket currency（票据货币）：用户或任务组拥有一批票，可以按自定义方式划分这些票给不同的子进程。
\item Ticket transfer（票据转移）：很好玩吧，process之间还可以暂时借tickets噢。
\item Ticket inflation（票据膨胀）：process可以暂时增加或者减少它的票。
\end{enumerate}

那么，这个随机算法怎么去衡量它的公平性呢？我们就引入了这样一个公平性指标U：它表示的是，对于两个有一样彩票，并且运行时间相同的Process，它们实际上第一个任务完成的时间/第二个任务完成的时间的比值。那么我们拍拍脑袋就知道，U=1的时候说明这个算法表现是最好的。

我们可以发现Lottery Scheduling是一个概率性调度算法，只有说long-time jobs才有可能体现公平性，没法保证U=1（有点像大数定律）；那么既然有概率性的调度算法，那肯定也会有确定性的调度算法，也就是能够保证绝对的公平（U=1）。有一个这样的算法叫Stride scheduling，每个job有一个固定的stride（把它理解成和tickets成反比的东西就可以），然后每个job还有一个用来记录的pass value，每次这个process run的时候，它的process value就会增加。最后呢，在进行调度的时候，每次都选择pass最小的那个job去工作。

我们来看下面的这个例子：
\begin{example}
Suppose three processes (A, B and C), with stride values of 100, 200 and 40, and all with pass values initially at 0.
\begin{table}[H]
\normalsize
\centering
\begin{tabular}{c c c|c}
Pass(A)&Pass(B)&Pass(C)&\textbf{Who Runs?}\\
(stride=100)&(stride=200)&(stride=40)&\\
\hline
0&0&0&A\\
100&0&0&B\\
100&200&0&C\\
100&200&40&C\\
100&200&80&C\\
100&200&120&A\\
200&200&120&C\\
200&200&160&C\\
200&200&200&$\cdots$\\
\end{tabular}
\end{table}
\end{example}

那我问你，这个确定性的调度算法不是挺好的嘛，为什么我们还要用有随机性的 Lottery Scheduling呢？

其实是这样的，一方面，每次在调度的时候都要去check一下"pass value"，再决定哪个job下一个运行；还有一个更加关键的问题，如果有一个新的job进来，它的pass value该是多少呢？是0吗？如果是0的话，它不就会独占CPU了吗？

\subsection{Thread Scheduling}
我们首先回顾一下，process是CPU分配资源的最小单位，而thread是CPU调度的最小单位。

同时再回顾一下user-level thread和kernel-level thread的区别：

用户级线程（ULT）：线程由用户空间的线程库管理（如 pthread 库），操作系统内核并不知道这些线程的存在。而内核级线程（KLT）：由操作系统内核直接管理，调度器直接对它们进行调度。

同时，在支持线程的操作系统当中，调度的是kernel-level threads，而不是processes，这个时候内核线程的调度属于system-contention scope，也就是说不管你是哪个进程的，所有的kernel threads一起争用CPU cycles。

但是对于user thread就不一样了，对于OS来说，它只能看到kernel threads和LWP，而user thread的调度是user mode scheduler来完成的（这个调度器在user space当中，其代码就是普通用户程序代码的一部分，由thread library实现）。具体步骤是OS把时间片分给kernel thread/LWP，然后kernel thread来反向激活user mode scheduler，然后再有user mode scheduler决定哪个user thread运行。

\subsection{Multiple-Processor Scheduling}
我们之前讲的东西都是"one queue"，那如果我们有multiple processors/CPUs呢？

我们都知道，Cache这个东西的设计，是依赖局部性的，包括时间局部性和空间局部性。那么在multi-processor架构当中呢，每个processor都会有一个自己的Cache，这时候就有一个叫做Cache Affinity的概念：
\begin{definition}
Cache Affinity(Processor Affinity)：一个process，如果在一个CPU上运行，那么让它继续在这个核心上运行，可以重用缓存数据，从而提高性能。从根本上来说，在执行一个process的时候，需要去memeory里面找它的指令并且执行，那么如果process在一个CPU里面执行过了，那么它的一些指令和数据很可能就会存在Cache里面，那么访问Cache肯定会比访问Memory快，也就是说，在运行过这个process的CPU运行肯定比一个“新”的CPU要快。
\end{definition}

那么针对Multiple-Processor Scheduling，有以下这两个算法：
\subsubsection{Asymmetric Multiprocessing(ASMP)}
在这个算法当中，所有的CPU共用一个ready queue，我们结合一个例子来看：

如果说，我们一开始的CPU运行情况如下所示：
\begin{center}
\begin{ganttchart}[hgrid, vgrid, x unit=5mm, y unit chart=.8cm, y unit title=0.5cm, title/.style={draw=none, fill=none}, include title in canvas=false, canvas/.style={draw=black, fill=black!20}, bar top shift=0, bar height=1, ]{1}{5} 
%\gantttitlelist{1,...,20}{1} \\
\ganttbar{$CPU_0$}{1}{1}
\ganttbar[inline]{$A$}{1}{2}
\ganttbar[inline]{$E$}{3}{4}
\ganttbar[inline]{$D$}{5}{6}
\ganttbar[inline]{$C$}{7}{8}
\ganttbar[inline]{$B$}{9}{10}\\
\ganttbar{$CPU_1$}{1}{1}
\ganttbar[inline]{$B$}{1}{2}
\ganttbar[inline]{$A$}{3}{4}
\ganttbar[inline]{$E$}{5}{6}
\ganttbar[inline]{$D$}{7}{8}
\ganttbar[inline]{$C$}{9}{10}\\
\ganttbar{$CPU_2$}{1}{1}
\ganttbar[inline]{$C$}{1}{2}
\ganttbar[inline]{$B$}{3}{4}
\ganttbar[inline]{$A$}{5}{6}
\ganttbar[inline]{$E$}{7}{8}
\ganttbar[inline]{$D$}{9}{10}\\
\ganttbar{$CPU_3$}{1}{1}
\ganttbar[inline]{$D$}{1}{2}
\ganttbar[inline]{$C$}{3}{4}
\ganttbar[inline]{$B$}{5}{6}
\ganttbar[inline]{$A$}{7}{8}
\ganttbar[inline]{$E$}{9}{10}
\end{ganttchart}
\end{center}

我们认为，执行的早的job和对应的CPU更有亲和力，所以A对CPU0最有亲和力，那么最后4个CPU的运行情况可能是：
\begin{center}
\begin{ganttchart}[hgrid, vgrid, x unit=5mm, y unit chart=.8cm, y unit title=0.5cm, title/.style={draw=none, fill=none}, include title in canvas=false, canvas/.style={draw=black, fill=black!20}, bar top shift=0, bar height=1, ]{1}{5} 
%\gantttitlelist{1,...,20}{1} \\
\ganttbar{$CPU_0$}{1}{1}
\ganttbar[inline]{$A$}{1}{2}
\ganttbar[inline]{$E$}{3}{4}
\ganttbar[inline]{$A$}{5}{10}\\
\ganttbar{$CPU_1$}{1}{1}
\ganttbar[inline]{$B$}{1}{4}
\ganttbar[inline]{$E$}{5}{6}
\ganttbar[inline]{$B$}{7}{10}\\
\ganttbar{$CPU_2$}{1}{1}
\ganttbar[inline]{$C$}{1}{6}
\ganttbar[inline]{$E$}{7}{8}
\ganttbar[inline]{$C$}{9}{10}\\
\ganttbar{$CPU_3$}{1}{1}
\ganttbar[inline]{$D$}{1}{8}
\ganttbar[inline]{$E$}{9}{10}
\end{ganttchart}
\end{center}

但是我们会看到job E就会窜来窜去，就没有很好地利用到Cache的局部性。

\subsubsection{Symmetric Multiprocessing}
这个时候呢，对于每个CPU会有一个自己的ready queue，那我们就可以把任务分给几个queues。比方说每个queue都按照RR来调度，Queue0负责完成任务A和C，Queue1负责完成任务B和任务D，那Gantt Chart会是下面这样的：
\begin{center}
\begin{adjustbox}{scale={.8}}
\begin{ganttchart}[hgrid, vgrid, x unit=5mm, y unit chart=.8cm, y unit title=0.5cm, title/.style={draw=none, fill=none}, include title in canvas=false, canvas/.style={draw=black, fill=black!20}, bar top shift=0, bar height=1, ]{1}{32} 
%\gantttitlelist{1,...,20}{1} \\
\ganttbar{$CPU_0$}{1}{1}
\ganttbar[inline]{$A$}{1}{4}
\ganttbar[inline]{$C$}{5}{8}
\ganttbar[inline]{$A$}{9}{12}
\ganttbar[inline]{$C$}{13}{16}
\ganttbar[inline]{$A$}{17}{20}
\ganttbar[inline]{$C$}{21}{24}
\ganttbar[inline]{$B$}{25}{32}\\
\ganttbar{$CPU_1$}{1}{1}
\ganttbar[inline]{$B$}{1}{4}
\ganttbar[inline]{$D$}{5}{8}
\ganttbar[inline]{$B$}{9}{12}
\ganttbar[inline]{$D$}{13}{16}
\ganttbar[inline]{$B$}{17}{20}
\ganttbar[inline]{$D$}{21}{32}
\end{ganttchart}
\end{adjustbox}
\end{center}

但是这又有一个问题（虽然上面的图已经解决了这个问题）：就是我们不知道每个job的length或者说workload，这个时候只要做一个migration就可以了，在上面的例子中就是把B移到CPU0里面去完成，解决问题！

\subsection{Read-Time CPU Scheduling}
再坚持一下！就最后一点点了！

在实时CPU调度当中，我们也分成两种情况:
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Soft real-time systems:对关键实时任务的调度不提供绝对时间保证，允许任务偶尔错过截止时间，系统整体仍能正常运行。（一般这个任务可以延时高一点）
\item Hard real-time systems:任务必须在绝对截止时间前完成，否则会导致严重后果。（这种情况下任务都可能是非常重要的）
\end{enumerate}

同时也有两个会影响表现的latencies:
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Interrupt Latency:从interrupt arrive到开始service这个Interrupt的延时，和response time很相似
\item Dispatch Latency:scheduling dispatcher从把当前process拿下来，到换成另一个的时间（context switch等花的时间）。

同时，dispatch latency还要考虑一个冲突问题，就是低优先级进程正在执行内核态代码（如操作系统内核函数），高优先级进程无法立即抢占，需等待内核态代码执行完毕。那么怎么解决呢？当低优先级进程阻塞高优先级进程时，临时提升低优先级进程的优先级，使其尽快释放资源。（很像priority inversion）
\end{enumerate}




\newpage
\section{Exercise}
\begin{example}
Consider the following set of processes, with the length of the CPU burst given in milliseconds:（一个小tip：如果priority一样，一般谁先来，谁的priority相对高一点点）
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
Process&Burst Time&Priority\\
\hline
$P_1$&2&2\\
\hline
$P_2$&1&1\\
\hline
$P_3$&8&4\\
\hline
$P_4$&4&2\\
\hline
$P_5$&5&3\\
\hline
\end{tabular}
\end{table}

The processes are assumed to have arrived in the order of P1; P2; P3; P4; P5, all at time 0.

\begin{enumerate}[label=\arabic*., leftmargin=2em]
\item Draw four Gantt charts that illustrate the execution of these processes using the following scheduling algorithms: FCFS, SJF, non-preemptive priority (a larger priority number implies a higher priority), and RR (quantum $=2$).
\item What is the average turnaround time for each of the scheduling algorithms in part 1?
\item What is the waiting time of each process for each of these scheduling algorithms?
\item Which of the algorithms results in the minimum average waiting time (over all processes)?
\end{enumerate}
\end{example}

Keys:

Q1:

\begin{center}
\begin{adjustbox}{scale={1}}
\begin{ganttchart}[hgrid, vgrid, x unit=.6cm, y unit chart=.8cm, y unit title=0.5cm, title/.style={draw=none, fill=none}, include title in canvas=false, canvas/.style={draw=black, fill=black!20}, bar top shift=0, bar height=1, ]{1}{20} 
\gantttitlelist{1,...,20}{1} \\
\ganttbar{$FCFS$}{1}{20}
\ganttbar[inline]{$P_1$}{1}{2}
\ganttbar[inline]{$P_2$}{3}{3}
\ganttbar[inline]{$P_3$}{4}{11}
\ganttbar[inline]{$P_4$}{12}{15}
\ganttbar[inline]{$P_5$}{16}{20}\\
\ganttbar{$SJF$}{1}{20}
\ganttbar[inline]{$P_2$}{1}{1}
\ganttbar[inline]{$P_1$}{2}{3}
\ganttbar[inline]{$P_4$}{4}{7}
\ganttbar[inline]{$P_5$}{8}{12}
\ganttbar[inline]{$P_3$}{13}{20}\\
\ganttbar{$PRIO$}{1}{20}
\ganttbar[inline]{$P_3$}{1}{8}
\ganttbar[inline]{$P_5$}{9}{13}
\ganttbar[inline]{$P_1$}{14}{15}
\ganttbar[inline]{$P_4$}{16}{19}
\ganttbar[inline]{$P_2$}{20}{20}\\
\ganttbar{$RR$}{1}{20}
\ganttbar[inline]{$P_1$}{1}{2}
\ganttbar[inline]{$P_2$}{3}{3}
\ganttbar[inline]{$P_3$}{4}{5}
\ganttbar[inline]{$P_4$}{6}{7}
\ganttbar[inline]{$P_5$}{8}{9}
\ganttbar[inline]{$P_3$}{10}{11}
\ganttbar[inline]{$P_4$}{12}{13}
\ganttbar[inline]{$P_5$}{14}{15}
\ganttbar[inline]{$P_3$}{16}{17}
\ganttbar[inline]{$P_5$}{18}{18}
\ganttbar[inline]{$P_3$}{19}{20}
\end{ganttchart}
\end{adjustbox}
\end{center}

Q2:

\begin{center}
\begin{tabular}{r|cccc}
 & FCFS & SJF & PRIO & RR \\
\hline
\multirow{2}{*}{\begin{tabular}[c]{@{}r@{}}$T_{turnaround}$ (ms)\end{tabular}} & $\frac{2+3+11+15+20}{5}$ & $\frac{1+3+7+12+20}{5}$ & $\frac{8+13+15+19+20}{5}$ & $\frac{2+3+20+13+18}{5}$\\
 & =10.2 & =8.6 & =15 & =11.2\\
\end{tabular}
\end{center}

Q3 \& Q4:

\begin{center}
\begin{tabular}{c|c|ccccc|cc}
&&$P_1$&$P_2$&$P_3$&$P_4$&$P_5$&Avg& \\
\hline
\multirow{4}{*}{\begin{tabular}[c]{@{}r@{}}$T_{waiting}$ (ms)\end{tabular}} 
& FCFS & 0 & 2 & 3 & 11 & 15 & 6.2 &\\
& SJF & 1 & 0 & 12 & 3 & 7 & 4.6 & Minimum\\
& PRIO & 13 & 19 & 0& 15 & 8 & 11 &\\
& RR & 0 & 2 & 12 & 9 & 13 & 7.2 &\\
\end{tabular}
\end{center}

\begin{example}
Given the table below showing the process information of a system, based on multilevel feedback queuing scheduling scheme. Suppose there are five levels in the system and within each level the FCFS scheduling is used. 
If a process is preempted, it will wait for being scheduled at the first place in the FCFS queue.
Draw a Gantt chart to show the time of CPU allocated to each process until all processes are finished using time quantum $q=2^i$, (where $q$ = time allocated for a process to run in its turn, and $i$ ranges from 0 to 4 indicating the $i^{th}$ level queue).
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
Process&Arrival Time&Service Time\\
\hline
$P_1$&0&3\\
\hline
$P_2$&1&5\\
\hline
$P_3$&3&2\\
\hline
$P_4$&9&5\\
\hline
$P_5$&12&5\\
\hline
\end{tabular}
\end{table}
\end{example}

Keys:
\begin{center}
\begin{ganttchart}[hgrid, vgrid, x unit=5mm, y unit chart=.8cm, y unit title=0.5cm, title/.style={draw=none, fill=none}, include title in canvas=false, canvas/.style={draw=black, fill=black!20}, bar top shift=0, bar height=1, ]{1}{20} 
\gantttitlelist{1,...,20}{1} \\
\ganttbar{$Q_0$}{1}{1}
\ganttbar[inline]{$P_1$}{1}{1}
\ganttbar[inline]{$P_2$}{2}{2}
\ganttbar[inline]{$P_3$}{4}{4}
\ganttbar[inline]{$P_4$}{10}{10}
\ganttbar[inline]{$P_5$}{13}{13}\\
\ganttbar{$Q_1$}{3}{3}
\ganttbar[inline]{$P_1$}{3}{3}
\ganttbar[inline]{$P_1$}{5}{5}
\ganttbar[inline]{$P_2$}{6}{7}
\ganttbar[inline]{$P_3$}{8}{8}
\ganttbar[inline]{$P_4$}{11}{12}
\ganttbar[inline]{$P_5$}{14}{15}\\
\ganttbar{$Q_2$}{9}{9}
\ganttbar[inline]{$P_2$}{9}{9}
\ganttbar[inline]{$P_2$}{16}{16}
\ganttbar[inline]{$P_4$}{17}{18}
\ganttbar[inline]{$P_5$}{19}{20}
\end{ganttchart}
\end{center}

\newpage
\section{Concepts Organization}

\begin{enumerate}[label=\arabic*., leftmargin=3em]
\item CPU burst: A period during which a process is executing instructions on the CPU without performing any I/O operations.
\item I/O burst: A period during which a process is waiting for or performing input/output operations, such as reading from disk, writing to a file, or interacting with a device. During this time, the CPU is not used by the process.
\item Scheduling timing: The moments or events at which the operating system decides to evaluate and possibly change the currently running process or thread. 
\item Scheduler: Make the decision of which process runs next.
\item Dispatcher: It gives control of the CPU to the process selected by the short-term scheduler and performs the context switch.
\item Scheduling criteria: CPU utilization, Throughput, turnaround time, waiting time, response time.
\item FCFS: First come, First served.
\item (Preemptive)SJF: Shortest job first, then the next shortest, which is a special case of the general prioriy-scheduling algorithm.(Preemptive SJF:decide whether a job can preempt another job)
\item Priority: A numerical value assigned to a process to determine the order in which processes are selected for execution by the CPU scheduler. A process with higher priority is scheduled before processes with lower priority.
\item RR: Instead of running jobs to completion, RR runs a job for a time slice(or a scheduling quantum) and then switches to the next job in the run queue.
\item MLQ: Ready queue is partitioned into separate queues, such as foreground and background. Besides, process is permanently in a given queue, and each queue has its own scheduling algorithm.
\item MLFQ: A dynamic scheduling algorithm where processes can move between multiple priority queues based on their behavior and execution history. Unlike Multilevel Queue (MLQ), MLFQ allows a process to be promoted or demoted between queues, enabling better responsiveness and fairness for diverse workloads.
\item Gantt chart: A visual representation of the schedule of processes over time in a CPU scheduling scenario. It shows the order and duration in which processes are executed by the CPU.
\item Process/system-contention scope(two completely different concepts): Competiton of CPU cycle is within the process(all threads in system).
\item Symmetric multiprocessing: One queue per CPU, the system divides the jobs to several queues and each queue will likely follow a particular scheduling discipline, such as Round Robin.
\item Processor (cache) affinity: A process, when runs on a particular CPU, builds up a fair bit of state in the caches of the CPU. The next time the process runs, it is often advantageous to run it on the same CPU, as it will run faster if some of its state is already present in the caches on that CPU.
\end{enumerate}


















\chapter{Process\_Synchronization}
\section{In-class Contents}
\subsection{Warm-up}

我们先来看一个程序。
\vspace*{6pt}
\begin{lstlisting}[language=C]
/* thread.c */
#include <stdio.h>
#include <stdlib.h>
#include <common.h>

volatile int counter = 0;
int loops;

void *worker(void *arg) {
 int i;
 for (i = 0; i < loops; i++) {
 counter++;
 // 问题在于，多个线程同时访问counter，造成竞争，可能导致counter的值小于预期值。
 }
 return NULL;
}

int main(int argc, char *argv[]) {
 if (argc != 2) {
 fprintf(stderr, "usage: threads <value>\n");	
 exit(1);
 }
 loops = atoi(argv[1]);
 pthread_t p1, p2;
 printf("Initial value : %d\n", counter);

 Pthread_create(&p1, NULL, worker, NULL);
 Pthread_create(&p2, NULL, worker, NULL);
 Pthread_join(p1, NULL);
 Pthread_join(p2, NULL);
 printf("Final value : %d\n", counter);
 return 0;
}
\end{lstlisting}
\vspace*{6pt}
\begin{lstlisting}[language=sh]
prompt> gcc -o thread thread.c -Wall -pthread
prompt> ./thread 1000
Initial value : 0
Final value : 2000

prompt> ./thread 100000
Initial value : 0
Final value : 143012
prompt> ./thread 100000
Initial value : 0
Final value : 137298
\end{lstlisting}

从上面这个例子可以看出，loop值增加的时候，Final value的值会比两倍的loop小。这背后的原因是出现了并发错误（concurrency）。

\subsection{Background}
我们还是看上面的例子，问题出在"counter++"这行代码，虽然它是一行，但是在汇编当中它是三行代码，所以在这中间如果出现interrupt，那就会导致结果的inconsisitency。

\begin{pt}
不完全是因为multi-processors的问题，只有一个CPU也会出错。
\end{pt}

这个事情的核心在于：我们的调度被kernel control而不是被user control，kernel不知道user想要的结果是什么样的。

那么其实有一种方式可以解决这个问题，如果我们有一个指令可以把自加操作合并成一个原子操作，那就没事了，但是这样做的代价是指令的执行会很慢，例如，将自加指令合并为\textcolor{blue}{``memory-add 0x8049a1c, \$0x1''}。

\begin{definition}
Race condition（竞争条件）:\textbf{最终的输出是不确定的。}多个processes(threads)同时竞争和控制同样的数据，那么这个数据的结果会取决于processes竞争这个数据的顺序。
\end{definition}

\subsection{The Critical-Section Problem}
我们首先来对这个问题做一个定义：

假设我们的系统有n个processes：$P_0, P_1, \cdots, P_{n-1}$。每个Process当中有这样几个代码部分(segments)：entry、critical section（涉及变量的改变、写文件等）、exit、remain。那么理论上，我们在critical section运行process的时候，其他的process不应该在自己的critical section运行。

\begin{definition}
Critical Section:有可能导致不确定输出(race condition)的代码片段，在前面的例子中就是"count++"。
\end{definition}

\begin{definition}
Critical section problem:我们做一些规定来避免race condition。
\end{definition}

具体来说，还是以上面的count++为例，它可以拆成三条指令，我们希望这三条指令要么一起运行，要么都不运行。所以每个process在entry section的时候要询问是否能够进入critical section，同时在执行完critical section的代码之后进入exit section（告诉别的进程我用完这个critical section了，其他process可以进到它们的critical section了），最后是remainder section（在这里做一些其他的事情）。

现在我们来看一个例子，看看它能不能解决critical section problem。假设我们有两个process，$P_i$ 和 $P_j$。
\vspace*{6pt}
\begin{lstlisting}[language=C]
// Pi
do {
    while (turn == j); 
        critical section 
    turn = j; 
        remainder section 
} while (true);
\end{lstlisting}
\vspace*{6pt}
\begin{lstlisting}[language=C]
// Pj
do {
    while (turn == i); 
        critical section 
    turn = i; 
        remainder section 
} while (true);
\end{lstlisting}

看上去没有任何问题，能够满足每次只有一个process进到critical section(mutual exclusion)，也满足有界等待(bounded waiting)，因为process只会等待一次就能进入critical section。但是问题在于\textbf{如果这两个process一个不想运行了，那另一个肯定也运行不了了(progress)}。

\subsubsection{Solution to Critical-Section Problem}
\textbf{下面的内容very important!}

现在我们来讲讲三个判定一个solution是否correct的标准：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Mutual Exclusion（互斥）:很好理解，一个process在执行critical section代码的时候，别的process不给执行。
\item Progress（前进、空闲让进）:保证当critical section没人执行的时候，如果有process想要执行critical section，就保证能够执行，不能大家都堵死，\textbf{也就是不能出现选择一个process进入critical section的这个动作被无限延迟，等待的时间是有限的}。
\item Bounded Waiting（有界等待）:从一个进程做出请求进入critical section到这个请求被满足为止，其他process进到critical section的次数有限。\textbf{保证等待的次数有限，自己不会被饿死。}注意在判断这个的时候，只要找到一个调度序列，也就是代码的某一行被interrupt，就能导致一个process进不去就行。
\end{enumerate}

同时，在设计内核的时候，有两种情况，一种是preemptive的，可以强制某个进程释放CPU，避免出现不想主动结束或者是阻塞的问题，但会出现race condition的情况；还有一种是non-preemptive的，它虽然不会有race condition，但是它的响应能力太差。在实际当中，我们用的都是抢占式内核。


\subsection{Mutex Locks}
那么在解决同步问题的时候，我们需要的是Hardware synchronization primitives（硬件支持同步语句）来实现将一串代码变成“原子操作”，但实际上一个lock本质就是一个变量。

在这里我们有两个函数lock()和unlock()。

如果没人占用这把锁，调用lock()就可以获得这个锁，别人进不来，别人调用lock()也没用。同时呢，如果锁当前占有者unlock()了，那这下锁就又可以被使用了，但是下一个谁进去是没有保障的，取决于其他东西（比如说有这样一个队列），和锁本身无关。

\begin{pt}
lock()和unlock()一般是非原子性的，取决于OS的实现，test and set这种就是原子性的。
\end{pt}


那么我们怎么构建一个锁呢？有下面两类方法（都是实现lock的功能）：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 第一类是没special hardware support的情况：Dekker's and Peterson's Algorithms、Lamport's Bakery Algorithm。
\item 第二类是有hardware support的情况：Controlling Interrupts、The test-and-set instruction、The compare-and-swap instruction等等。 
\end{enumerate}

\subsubsection{Controlling Interrupts}
一种很简单的想法就是，直接不允许interrupt就行：
\vspace*{6pt}
\begin{lstlisting}[language=C]
void lock() {
	DisableInterrupts();
}
void unlock() {
	EnableInterrupts();
}
\end{lstlisting}


\indent 这里的DisableInterrupts和EnableInterrupts是两个special hardware instructions。

那么这是一个正确的解决critical section problem的思路吗？
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Mutual exclusion:显然是的。
\item Progress:只要enableinterrupt，就可以有process进去。
\item Bounded waiting:不满足，比如说有两个process A和B，万一每次都是A抢到了critical section呢？没法保证B有机会执行。
\end{enumerate}

它很简单、很快、也很方便，但是不是一个正确的解法，也会有下面这些问题（了解）：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 关闭中断是一个特权操作，如果任何线程都能关闭中断，那这是很危险的，一个process占用了一个锁之后不放了，一直占用CPU你不傻了吗。
\item diableinterrupts只能在当前的processor使用，而不同的CPU处理自己的interrupt，所以这么做没法阻止其他CPU interrupt。
\item 把中断关掉了，CPU就没办法及时处理重要的中断请求了。
\item 效率低。
\end{enumerate}

\subsubsection{Peterson's Solution}
\textbf{Important!一定要知道这个方法的所有details！}虽然现代OS不会用，但是考试会考，不需要hardware support。

\begin{pt}
这是一个两进程互斥问题的解决方案。
\end{pt}


当然在这个方案里面有一个假设，也就是假设"load"和"store"的操作是原子操作，也就是它们不能被中断（从代码上看就是turn=j的这种赋值是原子操作）。

这两个processes共享两个变量：int turn和Boolean flag[2]。其中turn标识现在轮到谁进入critical section，flag数组则用来表示一个进程是否准备好进到critical section，比方说flag[i]=true就代表着$P_i$已经准备好进到critical section了。

初始化的时候注意flag[i]和flag[j]都为false。

\begin{pt}
\textbf{三个变量已经是最简单的情况了，如果变量的数量少于三个，一定出错！}
\end{pt}


\textbf{下面的内容，very very very important!}

Peterson's Solution的核心思想就是：首先我想进，但是我先问问你想不想进，如果你不想进，那我就进去了，如果你想进，那你先来，最后记得告诉我你不想进了，我才好进去。


从代码上来看：
\vspace*{6pt}
\begin{lstlisting}[language=C]
// Pi的代码
do {
 flag[i] = true;
 turn = j;
 while (flag[j] && turn == j);
 critical section
 flag[i] = false;
 remainder section
} while (true);
\end{lstlisting}
\vspace*{6pt}
\begin{lstlisting}[language=C]
// Pj的代码
do {
 flag[j] = true;
 turn = i;
 while (flag[i] && turn == i);
 critical section
 flag[j] = false;
 remainder section
} while (true);
\end{lstlisting}

好，那么好，我们现在来证明一下我们这样做是一个正确的解法，也就是满足那三个标准：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Mutual Exclusion:\textbf{证明方法：假设一个Process进去了，看另一个process能不能进去。}假设$P_i$进去了，那么意味着要么flag[j]=false，要么turn=i。如果turn=i，那么在$P_j$里面，肯定已经执行过flag[j]=true，所以$P_j$要卡在while那个死循环那里。如果说flag[j]=false，那对于$P_j$，它一句都还没执行，肯定进不到critical section。所以满足互斥。
\item Progress:\textbf{证明方法：分情况讨论，一种情况是两种都从零开始，就是没人进去的情况；另一种情况是假设一个进程刚退出，看另一个进程能不能进去；还可以考虑一个进程执行完之后再也不动的情况。}如果两个进程都从零开始执行，因为turn只会有一个值，所以肯定有一个Process能进去。如果$P_i$刚运行完出来了，这个时候就会有flag[i]=false，所以这个时候$P_j$就能进去，当然如果说$P_i$又绕了一圈回去，运行到flag[i]=true和turn=j，这个时候$P_j$还是能够进去。
\item Bounded Waiting:我们看如果$P_i$进了一次，那下一次只要$P_j$运行到死循环的那个地方，那就是$P_j$进去。
\end{enumerate}

\begin{exam}
\textbf{做题的时候有一个小技巧，不满足互斥情况的往往满足前进；不满足前进情况的往往满足互斥和有界等待，当然也会有例外，比如bakery算法。}
\end{exam}

\textbf{下面有几个例子，就不在这里赘述了，直接看PPT就好，但记得一定要看！}

\subsubsection{Bakery Algorithm}
那么现在问题也就出现了，我们之前的算法是针对两个process来的，如果我现在有n个processes呢？

\begin{exam}
什么是Bakery Algorithm?

简单来说就是：顾客进入店里 → 进程想进入临界区；他们从门口的取号机拿到一个号码（比如 1, 2, 3...）；所有人都按号码从小到大排队，谁号小谁先服务；正在被服务的人（临界区）之后，才轮到下一个；出来后“释放号码” → 表示可以让下一个进程继续。
\end{exam}

但是这里可能会有一个问题，就是可能产生两个进程取的号是一样的情况，那这个时候我们决定下一个服务谁的时候就不能仅仅根据号来确定，而是要根据一个pair(number, index of the process)，如果number相同的时候，谁的index小，就先服务谁。同时，number的数组一定是递增的，虽然它可能不是严格递增的，例如：1,2,3,3,3,4,5...

那么在这个算法当中，我们有两个shared data：
\vspace*{6pt}
\begin{lstlisting}[language=C]
boolean choosing[n];	// initialized to false
int number[n]; 	// initialized to 0
\end{lstlisting}
\vspace*{6pt}
\begin{lstlisting}[language=C]
do { 
	choosing[i] = true;	// 表示我当前在取号，还没有取完
	number[i] = max(number[0], number[1], (*@$\cdots$@*), number [n - 1]) + 1;
	// 看看别人都拿了哪些号，然后在别人拿的号的最大值的基础上加1，确保号是递增的，解决bounded waiting的问题。
	// 但是问题就在于可能i和j抢的是一样的号，因为这句代码不是原子操作
	choosing[i] = false; // 取号完成
	// 有这一步的意义就是，告诉别的进程，我现在的number为0到底是因为我本身就不想取号(choosing[i]=false)还是说我现在还在取号，你们等等我(choosing[i]=true)。
	for (j = 0; j < n; j ++) {
		while (choosing[j]);  // 有人还没取完，就得等等
		while ((number[j] != 0) && ((number[j], j) < (number[i], i))); 
		// number[j]要是为0，那就说明这个进程不想进去；同时后面一句话表示的是号小的先进去。
	}
		critical section
	number[i] = 0;
		remainder section
} while (true);
\end{lstlisting}

那么choosing[]这个数组有什么用呢？

用来表示process是否在取号的过程中，如果没有choosing[]这个数组，就无法满足mutual exclution。

为什么呢？比方说process i理论上要拿一个3的号，但是它一直卡在第三行取号的地方，刚做完max()，号还没打印出来，但是这个时候process j(i<j)也进去了，并且也打印了一个3的号，它走的比较快，发现自己是最小的号，就进去了，然后process i这时候把号打印出来，发现自己也是最小的（因为number i=number j，但是i<j），所以process i也进去了，就不满足mutual exclusion了。

\begin{pt}
\textbf{不过Progress和Bounded waiting都是满足的！}
\end{pt}

但是呢，现在的主流还是硬件锁（LOL）。

\subsubsection{Test And Set}
我们先看看这段代码有什么问题？
\vspace{6pt}\\
\begin{lstlisting}[language=C]
typedef struct __lock_t { int flags; } lock_t;

void init(lock_t *mutex) {
	mutex->flag = 0;	// 0 -> lock is available, 1 -> held
}

void lock(lock_t *mutex) {
	while (mutex->flag == 1)	//TEST the flag
		;		// spin-wait (do nothing)
	mutex->flag = 1;	// now SET it!
}

void unlock(lock_t *mutex) {
	mutex->flag = 0;
}
\end{lstlisting}

可以发现这段代码不满足mutual exclusion和bounded waiting，问题出在while(mutex->flag==1)这句话。所以这个时候我们就想到了用hardware support来完成这个事情。

\vspace{6pt}
\begin{lstlisting}[language=C]
int TestAndSet(int *old_ptr, int new) {
	int old = *old_ptr;
	*old_ptr = new;
	return old;
}
// 这里相当于在做一件事情，就是test old_ptr的值，如果是我要的值，就把它改掉，改成new的值。
\end{lstlisting}


这个时候我们再改一下前面我们说出错的地方，改成下面这个样子：
\begin{lstlisting}[language=C]
typedef struct __lock_t { int flag; } lock_t;

void init(lock_t *lock) {
	lock->flag = 0;	// 0 -> available, 1 -> held
}

void lock(lock_t *lock) {
	while (TestAndSet(&lock->flag, 1) == 1);
	// spin-waiting
}

void unlock(lock_t *lock) {
	lock->flag = 0;
}
\end{lstlisting}

但是啊但是，这样的修改只能解决mutual exclusion的问题，也就是说，现在满足了mutual exclusion和progress，但是bounded waiting的问题还没有得到解决。

\begin{exam}
Mutex lock考察的天花板就是如何使用test and set满足bounded waiting的问题。
\end{exam}

\subsubsection{Compare And Swap}
现在我们来讲讲compare and swap，其实它和test and set很像，compare就是在test的过程，swap就是set的另一种形式。
\vspace*{6pt}
\begin{lstlisting}[language=C]
int CompareAndSwap(int *ptr, int expected, int new) {
	int actual = *ptr;
	if (actual == expected)
		*ptr = new;
	return actual;
}
\end{lstlisting}

和test and set的代码一样，可以这样来实现mutual exclusion:
\vspace*{6pt}
\begin{lstlisting}[language=C]
void lock(lock_t *lock) {
	while (CompareAndSwap(&lock->flag, 0, 1) == 1);	// spin-waiting
}
\end{lstlisting}

同样的，这个方法也是满足mutual exclusion、progress，但是不满足bounded waiting。

但是！有一个不一样的地方：compare and swap它更强大，它可以用来实现一个没有锁的同步！

这里补充一个内容，就是说我们看到代码里面有spin-waiting，那么对于一个需要spin-waiting的process来说，就算时间片分给它了，它也干不了啥，就只能傻等着，这样的performance是比较差的。

我们来看看compare and swap是如何实现lock-free synchronization的：

首先，给一个compare and swap的代码，我们希望是实现无锁的加法的功能：
\vspace{6pt}
\begin{lstlisting}[language=C]
int CompareAndSwap(int *ptr, int expected, int new) {
	int actual = *ptr;
	if (actual == expected) {
	 *ptr = new;
	 return 1;
	}
	return 0;
}
\end{lstlisting}

然后是执行加法的函数：
\vspace*{6pt}
\begin{lstlisting}[language=C]
void AtomicIncrement(int *counter, int amount) {
	do {
		int old = *counter;
	} while (CompareAndSwap(counter, old, old+amount) == 0);
}
\end{lstlisting}

那么这是一个原子操作，一定能在上下文切换之前完成，所以就不会出现切换到一个进程的时间片之后，他只能spin waiting的情况，也不会出现deadlock。

如果在完成old=*counter的赋值之后，发现counter被改掉了，那就进行下一次循环，这个时候old又赋值为改之后的counter了，那其实一般情况就不会有问题哩。

\subsubsection{Load-Linked and Store-Conditional}
这个部分书上貌似没有，暂时跳过一下~

\subsubsection{Test And Set——Satisfying Bounded Waiting}
\textbf{这个部分非常非常重要！要能理解并且把代码背出来}

\vspace*{6pt}
\begin{lstlisting}[language=C]
/* C-like pseudo code */
Initially Boolean waiting[i] = false; lock = false;(false在这里也就是0)

lock() {
	waiting[i] = true; // 表示一个进程是否在尝试获得锁
	while (waiting[i] && (TestAndSet(lock, 1) == 1));
	// 那么开锁的条件就是lock=false或者waiting[i]=false。
	waiting[i] = false;
}

unlock() {
	j = (i + 1) % n; // 循环查找下一个等待锁的进程
	while ((j != i) && !waiting[j])
		j = (j + 1) % n;
	if (j == i)
		lock = false;
		// 如果大家都不要锁，饶了一圈回到我自己了，那就释放锁
	else
		waiting[j] = false;
		// 如果有进程在等待锁，那就把锁传过去
}
\end{lstlisting}

现在又两个问题：一个是为什么这段代码就能够满足bounded waiting了呢？另一个问题是什么时候锁被释放了呢？

因为对于一个想要获取锁的进程来说，他只需要等一圈就可以了；只有所有进程都不要这个锁的时候，锁才会被释放，不然只是锁的传递。


\subsubsection{Fetch And Add}
是Bakery算法的硬件实现，更加聪明一些（把取号变成一个原子操作，也就是把取号的过程保护住），但是内容其实不在书上。
\vspace*{6pt}
\begin{lstlisting}[language=C]
int FetchAndAdd(int *ptr) {
	int old = *ptr;
	*ptr = old + 1;
	return old;
}

/* ticket lock */
typedef struct __lock_t { int ticket; int turn; } lock_t;
void lock_init(lock_t *lock) {
	lock->ticket = 0;
	lock->turn = 0;
}
void lock(lock_t *lock) {
	int myturn = FetchAndAdd(&lock->ticket);
	while (lock->turn != myturn);
}
void unlock(lock_t *lock) {
	lock->turn = lock->turn + 1;
}
\end{lstlisting}


\subsubsection{Spin-Waiting}
现在我们来讲讲怎么解决Spin-waiting的问题：

可以用到一些OS support（不是hardware support），或者说提供一些API，让user能够在写代码的时候避开需要wait的那些thread，尽可能减少时间片的浪费。

比如说有一个syscall叫做yield：process自己把自己从running state搞到ready state去。这样做确实解决单核CPU的大部分时间浪费问题，但是\textbf{进程要切来切去还是很麻烦的}。

也就是说，现在是scheduler决定哪个process/thread下一个运行，这种情况下线程随机获得锁，没办法保证公平性，所以我们想对scheduling做一些控制，将等待锁的进程都放在一个队列里面，如果锁可用，就把队列的第一个进程放进来running。

一个解决的方法是用park()和unpark()，那么因为这个部分同样书上没有，同时也比较复杂，所以我们先不管。

\begin{exam}
我们要注意，不管怎么做，spin都不是能够完全avoid掉的，只能说可以把它限制一下。
\end{exam}

\textbf{下面的内容very important!}

spin-waiting这个东西并不是说它一直都是不好的，比方说这种情况：在多处理器系统当中，一个thread在一个processor运行的时候，另一个thread在另一个processor上可以做一会spin waiting来等第一个thread运行完它的critical section，这样对于一个thread来说它就不需要被做context switch了，只要稍微等等就好。

最后讲了一个优先级反转和优先级继承的东西，有点像是mutex lock的特性：讲了两个概念，一个是priority inversion（优先级反转:A higher-priority thread waiting for a lock held by lower-priority thread）：发生在具有\textbf{两个以上优先级（至少三个）}的系统中；另一个就是解决这个问题的priority-inheritance（优先级继承）：低优先级的process换到更高的优先级，把资源释放掉之后，再恢复自己之前的优先级。

\subsection{Locked Data Structures}
dk讲的很快很快，或许，不是很重要吧。

\subsection{Condition Variables}
我们前面学了这些东西，是不是以为mutex lock能解决所有问题呢？实际上不是这样的，我们先来看一下下面这个例子：
\vspace*{6pt}
\begin{lstlisting}[language=C]
void *child(void *arg) {
	pintf("child\n");
	// Problem #1: how to indicate we are done?
	// 我们没有任何机制能够通知主线程说我们已经完成了子进程的thread
	return NULL;
}

int main(int argc, char *argv[]) {
	printf("parent: begin\n");
	pthread_t c;
	pthread_create(&c, NULL, child, NULL);
	// Problem #2: how to wait for child?
	// 因为子线程没有通知我，所以我也不知道子线程是做完了还是没有做完。
	printf("parent: end\n");
	return 0;
}
\end{lstlisting}

那么这个问题能用互斥锁来解决吗？很明显不可以（因为这里都没有critical section），这时候我们想到了可以用一个shared variable来做这个事情：
\vspace*{6pt}
\begin{lstlisting}[language=C]
void *child(void *arg) {
	pintf("child\n");
	done = 1;
	return NULL;
}

int main(int argc, char *argv[]) {
	printf("parent: begin\n");
	pthread_t c;
	pthread_create(&c, NULL, child, NULL);
	while (done == 0);
	printf("parent: end\n");
	return 0;
}
\end{lstlisting}

这个方法看上去可以，而且确实是可以，但是它很低效，因为对于父进程来说我得一直spin-waiting去check done的值。

这时候我们希望能够用条件变量(condition variable)，它是一个显式的排序队列，有两种接口：wait()（让它自己睡去吧）和signal()（唤醒一个满足条件的thread），同时在用条件变量的时候，我们也会用到mutex。

\vspace*{6pt}
\begin{lstlisting}[language=C]
pthread_cond_wait(pthread_cond_t *c, pthread_mutex_t *m);
pthread_cond_signal(pthread_cond_t *c);
\end{lstlisting}

我们来看看如何从condition variable解决我们上面说到的问题：
\vspace{6pt}
\begin{lstlisting}[language=C]
int done = 0;	// 共享变量，表示子进程是否已经完成
pthread_mutex_m = PTHREAD_MUTEX_INITIALIZER;
pthread_cond_t c = PTHREAD_COND_INITIALIZER;

void thr_exit() {
 pthread_mutex_lock(&m);	// 加锁
 done = 1;					// 表示子进程完成了
 pthread_cond_signal(&c);	// 发出信号告诉主进程我已经完成了
 pthread_mutex_unlock(&m);	// 解锁
}

void *child(void *arg) {
 printf("child\n");
 thr_exit();		// 通知主进程我已经完成嘞
 return NULL;
}

void thr_join() {
 pthread_mutex_lock(&m);	// 加锁
 while (done == 0)
  pthread_cond_wait(&c, &m);// 如果done一直为0，那也就说明子进程还没完成，运行这行代码的时候会释放锁m，同时我也会堵在这里等待子进程signal一下，如果c的值被signal了，主进程能重新获得锁m，最后再解锁就可以顺利出去了。
 pthread_mutex_unlock(&m);
}

int main(int argc, char *argv[]) {
 printf("parent: begin\n");
 pthread_t p;
 pthread_create(&p, NULL, child, NULL);
 thr_join();
 printf("parent: end\n");
 return 0;
}
\end{lstlisting}

从上面的例子中，我们也可以看到，condition variable会有一个lock(m)和一个flag(done)。我们来分析一下如果两个少了一个该怎么办：

\begin{pt}
\textbf{Condition Variable不可以先signal再wait！}
\end{pt}

\vspace*{6pt}
\begin{lstlisting}[language=C]
void thr_exit() {
 pthread_mutex_lock(&m);
 pthread_cond_signal(&c);
 pthread_mutex_unlock(&m);
}

void thr_join() {
 pthread_mutex_lock(&m);
 pthread_cond_wait(&c, &m);
 pthread_mutex_unlock(&m);
}

/* broken code #1 */
\end{lstlisting}

这里犯的错误就是可能子线程先signal，父线程再wait，那么父线程永远不能被唤醒，就寄了。

再看看这个情况：
\vspace*{6pt}
\begin{lstlisting}[language=C]
void thr_exit() {
 done = 1;
 pthread_cond_signal(&c);
}


void thr_join() {
 if (done == 0)
  pthread_cond_wait(&c);
}


/* broken code #2 */
\end{lstlisting}

这个情况也是一样，可能先if(done==0)，再子线程done=1，并且signal，这下父进程wait，就没办法再被signal了，又寄了。


\subsubsection{The Producer-Consumer Problem}
一个人不停生产，喂给消费者作为输入，中间有一个缓冲区。

我们先来看一个简单的情况，只有一个producer，一个consumer和buffer\_size为1的情况（这是一个correct solution）：
\vspace*{6pt}
\begin{lstlisting}[language=C]
/* Let's begin with only 1 producer, 1 consumer, buffer_size = 1 */

cond_t cond;
mutex_t mutex
void *producer(void *arg) {
	for (int i = 0; i < loops; i ++) {
		pthread_mutex_lock(&mutex);
		if (count == 1)	// 表示buffer里面有东西了，等着
			pthread_cond_wait(&cond, &mutex);
		put();			// produce
		pthread_cond_signal(&cond);
		pthread_mutex_unlock(&mutex);
	}
}
void *consumer(void *arg) {
	for (int i = 0; i < loops; i ++) {
		pthread_mutex_lock(&mutex);
		if (count == 0)	// 表示buffer里面没东西了，同样等着
			pthread_cond_wait(&cond, &mutex);
		get();			// consume
		pthread_cond_signal(&cond);
		pthread_mutex_unlock(&mutex);
	}
}
\end{lstlisting}

但是这个代码就不适用有很多consumer的情况啦（很好理解，直接看PPT就行）问题本质在于，我们只check了一次count的数值(if语句)，所以一个很自然的想法就是把if换成while（我们以一个producer和两个consumer为例）：
\vspace*{6pt}
\begin{lstlisting}[language=C]
/* Still broken */

cond_t cond;
mutex_t mutex
void *producer(void *arg) {
	for (int i = 0; i < loops; i ++) {
		pthread_mutex_lock(&mutex);
		while (count == 1)	// use "while" instead of "if"
			pthread_cond_wait(&cond, &mutex);
		put();			// produce
		pthread_cond_signal(&cond);
		pthread_mutex_unlock(&mutex);
	}
}
void *consumer(void *arg) {
	for (int i = 0; i < loops; i ++) {
		pthread_mutex_lock(&mutex);
		while (count == 0)	// use "while" instead of "if"
			pthread_cond_wait(&cond, &mutex);
		get();			// consume
		pthread_cond_signal(&cond);
		pthread_mutex_unlock(&mutex);
	}
}
\end{lstlisting}

但是很遗憾它还是错的，其实原因也很简单，producer和consumer用的是同样的condition，那么比方说我一个consumer消耗了一个资源，理论上我想要唤起的是producer，但是不小心唤醒了另一个consumer，那就炸了呀兄弟。

那么同样很自然的，我们可以想到用两个conditon variables来解决这个问题，producer和consumer用的应该是不一样的：
\vspace*{6pt}
\begin{lstlisting}[language=C]
/* buffer_size = 1 */

cond_t empty, fill;			// two condition variables
mutex_t mutex
void *producer(void *arg) {
	for (int i = 0; i < loops; i ++) {
		pthread_mutex_lock(&mutex);
		while (count == 1)
			pthread_cond_wait(&empty, &mutex);
		put();			// produce
		pthread_cond_signal(&fill);		//只signal fill，说明现在buffer是满的
		pthread_mutex_unlock(&mutex);
	}
}
void *consumer(void *arg) {
	for (int i = 0; i < loops; i ++) {
		pthread_mutex_lock(&mutex);
		while (count == 0)
			pthread_cond_wait(&fill, &mutex);
		get();			// consume
		pthread_cond_signal(&empty);	// 只signal empty，说明现在buffer是空的
		pthread_mutex_unlock(&mutex);
	}
}
\end{lstlisting}

在解决了这个问题之后，我们很自然地会去想，如果buffer的size不止是1呢？
\vspace{6pt}
\begin{lstlisting}[language=C]
/* dealing with bounded buffer instead of single buffer */

cond_t empty, fill;			// still two condition variables
mutex_t mutex
void *producer(void *arg) {
	for (int i = 0; i < loops; i ++) {
		pthread_mutex_lock(&mutex);
		while (count == MAX)	// remember: always use "while"
			pthread_cond_wait(&empty, &mutex);
		put();			// produce
		pthread_cond_signal(&fill);		// 注意这里的fill不是说真的满了，只是说现在你consumer可以去消耗了
		pthread_mutex_unlock(&mutex);
	}
}
void *consumer(void *arg) {
	for (int i = 0; i < loops; i ++) {
		pthread_mutex_lock(&mutex);
		while (count == 0)
			pthread_cond_wait(&fill, &mutex);
		get();			// consume
		pthread_cond_signal(&empty);	// 同样注意这里的empty不是空了，而是告诉producer你可以继续生产
		pthread_mutex_unlock(&mutex);
	}
}
\end{lstlisting}

最后呢，我们讲一个好玩的情况（也算是condition variable的一个特性）：

比如说现在我们没有空间了，Ta想要100B的空间，Tb想要10B的空间，Tc刚好要释放50B的空间，那么我们会唤醒哪一个线程呢？我们会用pthread\_cond\_broadcast把所有线程全部唤醒，再让他们看自己是不是符合条件。

\subsection{Semaphores}
Semaphore比condition variable牛逼在哪里呢，condition variable不能应对先signal再wait的情况，但是semaphore可以，它能解决所有mutex和condition variable的问题。对于semaphore的操作也是wait()和signal()。

\begin{pt}
这里的wait和signal和condition variable的是不一样的，注意区分。wait的时候，信号量-1，signal的时候，信号量+1。同时wait()和signal()都是原子操作。
\end{pt}

我们来看这样一个例子：
\vspace*{6pt}
\begin{lstlisting}[language=C]
typedef struct {
	int value;
	struct process *list;
} semaphore;

wait(semaphore *s) {
	s->value --;
	if (s->value < 0) {
		add this process to s->list;	// 进队列，waiting to be signaled，记得如果s->value为0的时候，刚好可以运行，不用进队列。
		block();
	}
}

signal(semaphore *s) {
	s->value ++;
	if (s->value <= 0) {
		remove a process P from s->list;	// remove一个进程，然后把这个进程wake up
		wakeup(P);	// 类似不释放锁，直接把锁传递给下一个进程
	}
}
\end{lstlisting}

所以说，semaphore的功能很强大：
$$Semaphore = \textcolor{red}{Mutex} + Integer + \textcolor{blue}{Condition Variable}$$

它最牛逼的地方还是在于，它可以先signal()再wait()，condition variable就不可以这么做。

\subsubsection{Signaling}
直接看PPT就行，非常简单兄弟。

\subsubsection{Rendezvous}
仍然很简单，还是看PPT。

\subsubsection{Mutex}
依然很简单，自己解决。

注意一下semaphore的初始值很关键。

\subsubsection{Multiplex}

比方说我们最多允许MAX个进程访问critical section，那么我们的代码应该这样写，同时multiplex这个信号量的初始值应该设为MAX：
\vspace*{6pt}
\begin{lstlisting}[language=C]
wait(multiplex);
count = count + 1;	// critical section
signal(multiplex);
\end{lstlisting}

\begin{pt}
Semaphore更像是一种资源，如果是condition variable，那初始值就是0，代表现在没有资源；如果是mutex，那初始值就是1，也就是只有一张门卡；如果像上面那样，就说明有MAX张这样的门卡。
\end{pt}


\subsubsection{Barrier}
这里开始就有点难了哈（但还是我们能轻松理解的最后一个问题了），现在要求必须n个线程都到这个地方之后，一起进去，不能有一个先跑进去这样的。

如果只是想要完成这个任务，那么代码是比较简单的，\textbf{一定要背出来这些代码}：
\vspace*{6pt}
\begin{lstlisting}[language=C]
/* initialization */
int count = 0;
semaphore mutex = 1;
semaphore barrier = 0;

wait(mutex);
count = count + 1;
signal(mutex);
if (count == n)
	signal(barrier);
wait(barrier);
signal(barrier);
critical point;
\end{lstlisting}

在这个例子当中，如果n=10，也就是说，当有10个进程到这里的时候大家才能一起走，一共会有11次signal(barrier)，最后barrier的值为1。所以说这么做虽然是correct，但好像不完全是correct的。

那我们之前也说了，这样做的barrier是一次性的，我们想让这样的barrier可以被重复利用（\textbf{虽然有一个很赖皮的做法，就是如果我要10个barrier，我不复用barrier，我直接搞10个也是可以的，但是考试的时候不能写的太简单}）：
\vspace*{6pt}
\begin{lstlisting}[language=C]
// 这个代码不需要背出来，但是要能体会（现场写能写出来）
// 注意：真正意义的barrier就是有n为多少，就有多少个线程，不会有多出来的，现在解决的是之前进去barrier然后又绕回来冲进去的问题。
semaphore barrier1 = 0, barrier2 = 0, mutex = 1;

rendezvous;
wait(mutex);
count += 1;
if (count == n)
	for (int i = 0; i < n; i ++)
		signal(barrier1);
signal(mutex);
wait(barrier1);
critical point;
wait(mutex);
count -= 1;
if (count == 0)
	for (int i = 0; i < n; i ++)
		signal(barrier2);
signal(mutex);
wait(barrier2);
\end{lstlisting}

\subsubsection{Pairing}
这能算是考试难度的天花板了。（比如说两个氢和一个氧怎么配对呢？）

这个问题是这样的：在舞会当中，一个领舞来的时候，看看有没有伴舞在等着，如果有就去跳舞，如果没有就等着。对于伴舞也是一样，看看有没有领舞，如果有就跳舞，没有就等着。

解决方法如下：
\vspace*{6pt}
\begin{lstlisting}[language=C]
/* initialization */
int num_l = 0, num_f = 0;
semaphore leader = 0, follower = 0, pairing = 0, mutex = 1;
\end{lstlisting}
\vspace*{6pt}
\begin{lstlisting}[language=C]
/* leader */
wait(mutex);
if (num_f > 0) {
	num_f --;
	signal(leader);
}
else {
	num_l ++;
	signal(mutex);
	wait(follower);
}
dance();
wait(pairing);
signal(mutex);
\end{lstlisting}
\vspace*{6pt}
\begin{lstlisting}[language=C]
/* follower */
wait(mutex);
if (num_l > 0) {
	num_l --;
	signal(follower);
}
else {
	num_f ++;
	signal(mutex);
	wait(leader);
}
dance();
signal(pairing);
	// no signal(mutex);
\end{lstlisting}

值得注意的是，在leader和follower的最后两行，对于leader来说，它会被pairing卡住，对于follower来说，它会被mutex卡住。（保证不会有两个follower进场，也不会有两个leader进场）（获得两次锁，也会释放两次锁，一定要自己能够理解）

有一个问题，怎么就保证leader和follower成对跳舞呢？

还是依靠最后两行的代码，不能一个地方signal两个semaphore。

最后我们来讲五个很经典的问题，每个都需要全面掌握。
\subsubsection{The Producer-Consumer Problem}
\begin{lstlisting}[language=C]
semaphore empty = MAX, full = 0, mutex = 1;
// MAX表示size of buffer

// empty+full永远是MAX
// 消耗一个empty，生产资源

// 在这里答案是用mutex的想法做的，当然也可以用condition variable做
void *producer(void *args) {
	for (int i = 0; i < loops; i ++) {
		wait(empty);
		wait(mutex);
		put();
		signal(mutex);
		signal(full);
	}
}

// 消耗资源，生产一个empty
void *consumer(void *args) {
	for (int i = 0; i < loops; i ++) {
		wait(full);
		wait(mutex);
		get();
		signal(mutex);
		signal(empty);
	}
}
\end{lstlisting}

\subsubsection{The Dining Philosophers}
There are five ``philosophers'' sitting around a table. Between each pair of philosophers is a single chopstick (and thus, five total). The philosophers each have times where they think, and don't need any chopsticks, and times where they eat. In order to eat, a philosopher needs two chopsticks, both the one on their left and the one on their right. The basic loop of each philosopher is as follows, assuming each has a unique identifier $p\in [0,4]$:

首先我们看一个不那么对的解决方法，但能够给我们一些启发：
\vspace*{6pt}
\begin{lstlisting}[language=C]
/* a failed solution, why failed? */
// 因为大家如果都先去左边找筷子，大家都只能找到一支->deadlock

int left(int p) { return p; }
int right(int p) {return (p + 1) % 5; }

void putchopsticks() {
	signal(chopsticks[left(p)]);
	signal(chopsticks[right(p)]);
}

void getchopsticks() {
	wait(chopsticks[left(p)]);
	wait(chopsticks[right(p)]);
}
\end{lstlisting}

所以我们有下面的解决方案：一个人从右手拿，其他人正常从左手拿：
\vspace*{6pt}
\begin{lstlisting}[language=C]
/* a correct solution */

int left(int p) { return p; }
int right(int p) {return (p + 1) % 5; }

void putchopsticks() {
	signal(chopsticks[left(p)]);
	signal(chopsticks[right(p)]);
}

void getchopsticks() {
	// 这个地方就非常有意思了
	if (p == 4) {
		wait(chopsticks[right(p)]);
		wait(chopsticks[left(p)]);
	}
	else {
		wait(chopsticks[left(p)]);
		wait(chopsticks[right(p)]);
	}
}
\end{lstlisting}

\subsubsection{The Readers-Writers Problem}
写者可以修改数据，但只能有一个，但是读者是可以有很多个的，读者不能修改数据。

第一类读者写者问题（读者优先）：读者不能等着，除非写者已经获得了修改的权限。
\vspace*{6pt}
\begin{lstlisting}[language=C]
semaphore write_mutex = 1;
semaphore readcount_mutex = 1;
int read_count = 0;

// 写者很简单，就是去竞争写的锁
// 考试的时候do, while没写不会扣分
void write() {
 do {
  wait(write_mutex);
  /* writing */
  signal(write_mutex);
 }
 while (true);
}

void read() {
  entry(read,write)
  /* reading */
  exit(read,write)
}
\end{lstlisting}

当然在这种情况下，写者可能会饿死，因为writer都没有资格和reader抢一个锁：
\vspace*{6pt}
\begin{lstlisting}[language=C]
semaphore readcount_mutex= 1;
semaphore writemutex=1;
int read_count = 0;
semaphore readmutex =1;

void write() {
  // 和下一个要来的读者抢一下锁，让后来的reader都无法进入
  wait(readmutex);
  // 把后面来的那个读者给比下去了，现在就等着读者全部出去之后，就可以写了
  wait(writemutex);
  /* writing */
  signal(writemutex);
  signal(readmutex);
}

void read() {
  // 同样和writer去竞争这个锁
  wait(readmutex);
  signal(readmutex);
  entry(read,write)
  /* reading */
  exit(read,write)
}
\end{lstlisting}


第二类读者写者问题（写者优先）：一旦读者已经就位了，他必须要尽快写。
\vspace*{6pt}
\begin{lstlisting}[language=C]
semaphore readcount_mutex= 1;
semaphore writemutex=1;
int write_count = read_count = 0;
semaphore writecount_mutex= 1;
semaphore readmutex=1;

entry(A,B){
	wait(Acount_mutex);
	A_count++;
	if(A_count == 1) wait(B_mutex);
	signal(Acount_mutex);
}

exit(A,B){
	wait(Acount_mutex);
	A_count--;
	if(A_count == 0) signal(B_mutex);
	signal(Acount_mutex)
}


void write() {
  entry(write,read);
  wait(writemutex);
  /* writing */
  signal(writemutex);
  exit(write,read)
}

void read() {
  wait(readmutex);
  entry(read,write);
  signal(readmutex);
  /* reading */
  exit(read,write);
}
\end{lstlisting}
\subsection{Monitors}





\newpage
\section{Concepts Organization}

\begin{enumerate}[label=\arabic*., leftmargin=3em]
\item Race condition: Several processes (threads) access and manipulate the same data concurrently and the outcome of the execution depends on the particular order in which the access takes place. Result indeterminate.
\item Critical section: Multiple threads executing a segment of code, which can result in a race condition.
\item Critical section problem(three requirements): Mutual exclusion, progress, bounded waiting.
\item Peterson's Solution: A software-based algorithm for addressing critical section problems between two processes.
\item Interrupt masks: A technique where the CPU disables interrupts to prevent context switches during the execution of critical sections.
\item Test-and-set: An atomic hardware instruction used to implement mutual exclusion.(For more details, plz read the main text)
\item Compare-and-swap: An atomic hardware instruction used to implement mutual exclusion.(For more details, plz read the main text)
\item Spin-waiting: A thread repeatedly checks a condition (typically a lock variable) in a loop without relinquishing the processor(release the control of CPU cycle).
\item Mutex: A synchronization primitive used to prevent multiple threads from accessing a shared resource at the same time. 
\item Semaphores: A counter that tracks the number of available units of a resource.
\item Condition variables: A synchronization primitive that allows threads to wait for certain conditions to become true.
\item Monitors:
\end{enumerate}






\chapter{Deadlocks}
\section{In-class Contents}
\subsection{Warm-up}
我们先来看看OS中的两个类型的bugs。
\subsubsection{Non-deadlock Bugs}
非死锁型错误：绝大多数这类错误是atomicity violation（原子性违例）或者是order violation（顺序违例），解决也很简单，用mutex lock或者是signal就可以解决。

\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Atomicity violation bugs:某个操作序列应该是原子的，但实际上在运行的时候被从中间打断了。

\begin{center}
\begin{lstlisting}[language=C]
/* Thread 1 */
if (thd->proc_info) {
 (*@$\cdots$@*)
 // 假如说运行到这里的时候突然thread 2开始运行了，就把proc_info的内容置空了。
 fputs(thd->proc_info, (*@$\cdots$@*));
 (*@$\cdots$@*)
}
\end{lstlisting}
\end{center}

\begin{center}
\begin{lstlisting}[language=C]
/* Thread 2 */
thd->proc_info = NULL;
// 这个操作相当于破坏了thread 1里的atomicity
//
\end{lstlisting}
\end{center}

\item Order violation Bugs:因为执行顺序的问题而出的错误
\begin{center}
\begin{lstlisting}[language=C]
/* Thread 1 */
void init() {
 (*@$\cdots$@*)
 mThread = PR_CreateThread(mMain, (*@$\cdots$@*));
  // 按理说应该创建完mThread之后再运行mMain
 (*@$\cdots$@*)
}
\end{lstlisting}
\vspace{6pt}
\begin{lstlisting}[language=C]
/* Thread 2 */
void mMain((*@$\cdots$@*)) {
 (*@$\cdots$@*)
 mState = mThread->State;
 // Thread 1还没来得及初始化mThread，Thread 2就运行这个，那肯定获取不到mThread的State呀
 (*@$\cdots$@*)
}
//
\end{lstlisting}
\end{center}
\end{enumerate}

\subsubsection{Deadlock Bugs}
那么为什么Deadlock会发生呢？
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 一个原因是dependencies（相互依赖，比如模块A调用模块B，同时模块B也会调用模块A）：比如说虚拟内存和文件系统互相等待对方。
\item 另一个原因是封装的问题：比如说Java Vector类里面的AddAll()，函数，可以看下面的代码：v1在调用AddAll的时候，会想要保护自己而给自己上锁，然后试图去开v2的锁，同时v2也给自己上锁，并且想开v1的锁，那么就死锁啦。
\begin{center}
\begin{lstlisting}[language=C]
/* Thread 1 */
Vector v1, v2;
v1.AddAll(v2);

/* Thread 2 */
v2.AddAll(v1);
\end{lstlisting}
\end{center}
\end{enumerate}

\subsection{System Model}
我们对我们的系统建一个模型：

我们认为System有很多resources:$R_1, R_2, \cdots , R_m$；然后resource types包含CPU cycles, memory space, I/O devices。

同时每个resource type $R_i$有$W_i$个实例；每个process用这些资源的方式包括：request, use, release。

\subsection{Deadlock Characterization}
\textbf{下面的内容非常重要}，讲产生Deadlock的四个必要非充分条件：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Mutual exclusion：同时只能有一个process占用一个resource，也就是说资源是不可以共享的。
\item Hold and wait（一边占用，一边等待）：一个process至少占有一个resource，并且正在等待别的processes释放它们的资源。（也就是说，自己有自己的资源的同时，还要抢别人的资源，也就是说，如果请求资源之前必须释放自己已经占有的资源的话，就不会发生死锁）。
\item No preemption：process占用的资源只能自己释放，不能被系统强制夺走（如果一个resource可以被抢占的话，那就要么没死锁，要么它和deadlock没有关系）。
\item Circular wait：说的简单一点，就是processes互相等待彼此的resources，形成一个圈；当然如果形成两个圈的话，那就产生两个死锁啦！
\end{enumerate}

那么为什么是necessary而不是sufficient呢？看PPT吧，感觉dk讲的也不是非常清楚，举了一个反例是multiplex，还有一个反例可以看后面的内容。

\subsubsection{Resource Allocation Graph}
那么如何描述一个deadlock呢？可以用resource allocation graph。

\begin{definition}
Resource Allocation Graph：包含一些vertices和一些edges，其中vertices有两种（Processes、Resources），edges也有两种（Request edge和Assignment edge）。
\end{definition}

话不多说，我们来看几个例子：
\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{screenshot010}
\label{fig:screenshot010}
\end{figure}

首先看(a)图，图中没有形成环，所以没有deadlock；其次看(b)图，图中有环，“所以”形成deadlock；最后看(c)图，里面有环，好像要形成deadlock了？NOPE!!!在这张图中，我们可以看见R4是有3分的，所以分出去两份之后，正好留一份能够满足P3的需求，所以P3是可以make progress的，所以没有deadlock！另外再补充一些，这个graph表示的是当前时刻的资源分配情况，也就是说，当前时刻P3问R4要一份resources，那下一刻的箭头可能就是R4指向P3了。

所以我们现在就可以总结出一些basic facts：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 如果一个图没有cycle，那么肯定没有deadlock。
\item 如果一个图中存在cycle，这时候要分类讨论：如果说每个resource type只有一个实例，那么就会有deadlock；如果说每个resource type有多个实例，那就要看情况咯，就有deadlock的可能。
\end{enumerate}

\subsection{Methods for Handling Deadlocks}
我们有四个方法来应对deadlocks，分成两类：

一类是确保系统不会进入deadlock的状态：
\begin{itemize}
\item Deadlock prevention：把形成deadlock的4个必要条件一个个毙掉就可以嘞
\item Deadlock avoidance：例如银行家算法，始终保持系统处于一个安全状态
\end{itemize}

另一类是允许系统进入deadlock state但是之后会recover:
\begin{itemize}
\item Deadlock detection
\item Recovery from deadlock
\end{itemize}

但是吧，你看上去我们要讲这么多的算法，实际上大部分OS都默认没有死锁的存在，因为死锁更多是用户/程序员该干的事情。

\subsection{Deadlock Prevention}
\textbf{因为这块部分不是内核在做的事情，所以考试也就只会考考概念。}

\subsubsection{Circular Wait}
做的事情很简单，就是给所有资源类型一个全局编号，然后所有进程必须按照资源编号升序请求资源。（本质：从代码层面prevent deadlock）

当然这个升序可以是total ordering（全序），也可以是partial ordering（偏序），类似想法的还有哲学家问题。

\begin{example}
在给两个mutex lock加锁的时候，可以先给地址大的那个lock加锁。
\end{example}

\subsubsection{Hold and Wait}
核心思想就是保证当一个process申请资源的时候，它不能占有任何其他资源。那么也就是说，对于一个process来说，要么process在执行前申请和分配所有需要的资源（同时也可能拿走一些其实本身并不需要的资源），或者就是在运行过程中只允许没有占用任何资源的Process去申请资源。"All resources of none"

但是啊，这也会有两个问题，一个问题是low resource utilization（你很难搞到所有的资源），另一个问题是有可能starvation（同样是你可能没法拿到所有资源，所以饥饿）。

当然，这个solution也是有问题的，问题就出在：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 没法确保分装的函数能遵循上面的"rules"
\item 并发率降低（没有资源，那就没办法运行，吞吐量也会随之降低）
\end{enumerate}

\subsubsection{No preemption}
核心思想：如果一个process占用了一些资源，但是没办法运行，那就把它占用的这些资源都释放出去（在仿真允许preemption的感觉）。Process只有在它重新获得了旧的资源，并且也获得了它需要的新的资源的时候，这个process才会继续运行。

\begin{center}
\begin{lstlisting}[language=C]
top:
	lock(L1);
	if (trylock(L2) == -1) {
		unlock(L1);
		// 拿不到资源，那算了，我就把锁释放掉吧
		goto top;
	}
\end{lstlisting}
\end{center}

当然，这个solution也是有问题的：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 同样也是没办法封装的很好
\item 可能出现Livelock（活锁）：有可能两个线程都是占了一个锁，发现得不到另一个锁，就放弃，一直重复在干这个事情，最后卡住了。

解决方案：等一会就好啦
\begin{center}
\begin{lstlisting}[language=C]
top:
	lock(L1);
	if (trylock(L2) == -1) {
		unlock(L1);
		sleep(rand()%10);
		goto top;
	}
\end{lstlisting}
\end{center}

\end{enumerate}

\subsubsection{Mutual Exclusion}
对于一些可共享的资源（比如只读文件），它们本身就是可以共享的，那很好，就不需要锁了。\textbf{但是！}，还有一些资源它们不是可以共享的，它们必须要有锁来进行管理，也就是说，\textbf{互斥是不可避免的}。

但是！我们可以用compare and swap这个hardware support来写代码，因为compare and swap可以实现一个lock-free synchronization（当然，有lock，才可能会有deadlock，没有lock，是不是就不会发生deadlock了呢）

这招被称为无等待并发(wait-free concurrency)
\begin{center}
\begin{lstlisting}[language=C]
/* atomically increment a value by a certain amount */
int CompareAndSwap(int *adderess, int expected, int new) {
	if (*address == expected) {
		*address = new;
		return 1;
	}
	return 0;
}
void AtomicIncrement(int *value, int amount) {
	do {
		int old = *value;
	} while (CompareAndSwap(value, old, old + amout) == 0);
}
\end{lstlisting}
\end{center}

\subsection{Deadlock Avoidance}
我们之前说的是在代码上Prevent deadlock（这是程序员应该做的事情），现在OS通过\textbf{调度}来avoid deadlock。
\begin{example}
Assume we have two processors ($CPU_1, CPU_2$) and four threads ($T_1, T_2, T_3, T_4$) which must be scheduled upon them. Assume further we know that each thread will grab some locks $L_1$ or $L_2$ as follows. 
\begin{center}
\begin{tabular}{c r r r r}
&$T_1$&$T_2$&$T_3$&$T_4$\\
$L_1$&yes&yes&no&no\\
$L_2$&yes&yes&yes&no
\end{tabular}
\end{center}
\begin{center}
\begin{ganttchart}[hgrid, vgrid, x unit=5mm, y unit chart=.8cm, y unit title=0.5cm, title/.style={draw=none, fill=none}, include title in canvas=false, canvas/.style={draw=black, fill=black!20}, bar top shift=0, bar height=1, ]{1}{20} 
%\gantttitlelist{1,...,20}{1} \\
\ganttbar{$CPU_1$}{1}{1}
\ganttbar[inline]{$T1$}{1}{8}
\ganttbar[inline]{$T2$}{9}{18}\\
\ganttbar{$CPU_2$}{1}{1}
\ganttbar[inline]{$T3$}{1}{13}
\ganttbar[inline]{$T4$}{14}{20}
\end{ganttchart}
\end{center}

那么在这个例子中你可能会很奇怪，不对啊兄弟，你这个T1和T3难道不会抢资源嘛？不是这样的兄弟，对于T3来说，它不会等待任何锁，所以它也就不会阻塞T1的资源，也就是说，它们不满足deadlock的hold and wait和circular wait的条件。
\end{example}

\textbf{下面的内容非常非常非常重要！因为这些都是OS在做的事情！}

那么我们如何avoid呢？对于系统来说，它需要有一些先验信息，说白了，就是OS要看Thread/Process申请了内核资源之后，它是不是能够正常运行，大概步骤是这样的：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 最简单也是最有效的方法就是让每个process声明它们每种resource要的最多的数量
\item Deadlock-avoidance算法动态检查资源分配的情况，来保证不会出现circular-wait的情况，从而avoid deadlock。
\item 资源分配情况是根据可分配资源和已分配的资源，还有process要求的最大资源数量一起决定的。
\end{enumerate}

\subsubsection{Safe State}
当一个process想要一个空闲的资源的时候，系统需要看看资源如果分配给它之后，系统能不能处于一个"safe state"。

\begin{definition}
Safe State：如果能构建一个process序列，比如说$<P_1, P_2, \cdots , P_n>$，能够保证P1能够执行完，然后P1释放它的资源，再保证P2能够执行完，直到最后序列中的所有Process都能够顺利执行，那么说明这个system是处于一个safe state。（如果两个process没有资源冲突，那也就无所谓这两个process的顺序）
\end{definition}

\begin{pt}
我们找到的这个序列是在“最坏情况”下也能保证执行的序列，那么为什么会提到“最坏情况”呢？比方说一个Process，它有一个条件语句，例如if():L1()，那么它可能用到L1的资源，也可能用不到，但是我们的算法需要把这种情况考虑进去，L1的资源该给还是要给，用不用再说。
\end{pt}

\begin{example}
我们来看一个例子：Consider a system with 12 magnetic tape drives and three processes: $P_0, P_1, P_2$. 
\begin{table}[H]
\centering
\normalsize
\begin{tabular}{c c c}
&Maximum Needs&Current Needs\\
\hline
$P_0$&10&5\\
$P_1$&4&2\\
$P_2$&9&2
\end{tabular}
\end{table}
\begin{itemize}
\item At time $t_0$, the system is in a safe state. The sequence $<P_1, P_0, P_2>$ satisfies the safety condition.
\item \textcolor{blue}{Show by an example that a system can go from a safe state to an unsafe state.}
\item Suppose that, at time $t_1$, process $P_2$ requests and is allocated one more tape drive. The system is no longer in a safe state.
\end{itemize}

具体讲讲的话，现在$P_1, P_0, P_2$加起来用了5+2+2=9个资源，那么我们还剩3个，可以先给P1，完成后会剩下5个资源，再给P0运行，完成后会剩下10个资源，最后再让P2运行，就完成啦。

但是！如果说，一开始给P2多分配一个，也就是5，2，3。这种情况下，P1完成后，只有4个资源，P0和P2都没法完成，那就进入了unsafe state，就可能死锁咯。
\end{example}

\begin{pt}
有一件值得注意的事情是，如果一个系统处于safe state，那么它肯定不会有deadlock的可能；但是，如果一个系统处于unsafe state，它也只是有可能deadlock。而我们deadlock avoidance做的事情是，保证一个系统不会进入unsafe state，如果有可能进入unsafe state，就拒绝请求。
\end{pt}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{screenshot011}
\label{fig:screenshot011}
\end{figure}

那么有哪些avoidance algorithms呢？有下面这两种:
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 如果每个资源类型只有一个实例（比如说只有一个打印机，一个锁），这时候我们用resource-allocation-graph algorithm。
\item 如果每个资源类型有多种实例呢？这时候我们就要用银行家算法啦(Banker's algorithm)。
\end{enumerate}

\subsubsection{Resource-Allocation-Graph Algorithm}
在这里我们讲一个新的边：
\begin{definition}
Claim edge:$P_i \dashrightarrow R_j$表示Pi可能会请求资源Rj，但至少现在没有真的分配过去。
\end{definition}

那么到这里为止，我们就有三种边了：Request edge（P->R）、Assignment edge（R->P）、Claim edge（P->R）。

那么三条边之间怎么转换呢？
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 当一个process真的请求了一个resource时，claim edge会变成request edge。
\item 当资源真的被分配给一个process时，request edge会变成assignment edge。
\item 如果一个process把资源释放掉了，assignment edge会变成claim edge。
\item 进程必须提前声明可能会请求哪些资源（就是声明边）
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\linewidth]{screenshot012}
\label{fig:screenshot012}
\end{figure}

\begin{pt}
我们之前说，resource-allocation-graph中出现环的时候，就是unsafe state，这个时候claim edge是不会参与deadlock的判定的。但是只有确保resource allocation graph不会出现环的时候才能把一个request edge变成一个assignment edge。（这个感觉不是很清晰，但我觉得不是很重要）
\end{pt}

\textbf{下面这个算法非常非常重要！}

\subsubsection{Banker's Algorithm}
首先，银行家算法是针对多个实例的，然后我们做以下假定：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 每个process都必须实现声明它要的maximum use的资源
\item 当一个process需要一个资源的时候，它必须等待别人把资源给它
\item 当一个process获得了它要的所有资源，它必须在有限的时间里面还回去
\end{enumerate}

我们首先定义一下银行家算法当中要用到的量和数组：
\begin{itemize}
\normalsize
\item $n$ Processes的数量。
\item $m$ Resources有m个类型。 
\item \textcolor{blue}{$Available$}: 数组长度为 $m$. 如果$Available[j] = k$， 则说明有 $k$ 个 $R_j$ 是可以分配的。
\item \textcolor{blue}{$Max$}: $n \times m$ 的矩阵。如果 $Max[i,j] = k$， 说明 $P_i$ 最多会需要 $k$ 个 $R_j$。
\item \textcolor{blue}{$Allocation$}: $n \times m$ 的矩阵。 如果$Allocation[i,j] = k$ 那么说明 $P_i$ 已经被分配了 $k$ 个$R_j$了。
\item \textcolor{blue}{$Need$}: $n \times m$ 的矩阵。如果 $Need[i,j] = k$, 那么 $P_i$还需要$k$个$R_j$来完成它的任务。
\begin{center}
$Need[i,j] = Max[i,j] - Allocation[i,j]$
\end{center}
\end{itemize}

\textbf{同时，银行家算法又能分成下面两个部分，为了区分开来，额外用两个小标题来表示，但这两个部分都应归属银行家算法。}

\subsubsection{Safety Algorithm}
其实这个算法做的事情很简单，看下面的例子就可以了：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 首先我们有两个向量，一个$Work$向量，长度为m，就是$Available$向量还有一个是$Finish$向量，长度为n，它初始化的时候都为false，表示这个进程是否已经被完成了。
\item 之后，我们找一个i（也就是找那么一个process），它要满足Finish[i]=false，且它所需要的资源，也就是Need[i]≤Work（也就是Work能满足Process i的需求）->这个步骤就相当于在构建safe sequence。但是，如果没有这样的process i存在，那么跳到第4步。
\item 这时候我们已经确保process i的要求是可以被满足的，所以我们可以做一个Work=Work+Allocation[i]，相当于先把work的资源给process i，它完成它的工作之后，把Finish[i]置为True，同时，把资源全部还回去，在这之后跳到第2步重复进行。
\item 最后，如果所有的Finish[i]都为true，那么说明这个系统处于safe state，反之，就是unsafe的。
\end{enumerate}

\subsubsection{Resource-Request Algorithm}
这个算法做的事情就是利用safe algorithm算法来avoid deadlock，其基本逻辑就是在分配资源的时候看每次分配是否safe。

在这里我们引入一个向量$Request_i$，它表示process i需要的资源，如果 $Request_i[j] = k$ 那也就说明 $P_i$ 想要 $k$ 个 $R_j$。

\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item \textbf{如果$Request_i \leq Need_i$，就到第二步。否则就报错，因为process会超过它的maximum claim。}

\begin{pt}
这个地方非常容易出错，也是一个扣分点，我们来打一个比方，如果Availabel=[100,5]，Need=[10,1]，但是这个时候我申请的Request=[11,1]，虽然说不会产生死锁，但是这是unsafe的，因为procee要求的资源超过maximum claim了。
\end{pt}

\item 如果$Request_i \leq Available$，那就去第三步。否则$P_i$就得一直等着，因为资源不够它用的。

\item 最后做一个分配资源的假想状态：$Available = Available - Request_i$、$Allocation_i = Allocation_i + Request_i$、 $Need_i = Need_i - Request_i$。然后我们调用safety算法，如果结果是safe的，那么就可以把资源分配给$P_i$；如果结果是unsafe的，那就等着吧。
\end{enumerate}


\subsection{Deadlock Detection}
这里和之前的区别在于，Deadlock可能已经发生了，但是现在我想要把这样的deadlock给detect出来（虽然现在没有OS在搞这个事情，真的deadlock了还是重启吧）

当然这也能分成两种情况，一种是每种resource type有一个实例的情况，还有一种显然就是有多个实例的情况。

\subsubsection{Single Instance of Each Resource Type}
处理的方式和resource-allocation graph algorithm很像，我们在这里搞了一个wait-for graph（就是resource-allocation graph的变形），节点都是process，$P_i \rightarrow P_j$表达的意思就是$P_i$在等待$P_j$释放资源。

不过！有一个不一样的地方在于，如果wait-for graph有环的时候，\textbf{一定有一个deadlock}（是充分条件，不是possibility of deadlock）。

好，我们来看看如何从Resource-Allocation Graph变成Wait-for Graph（虽然感觉这样好像也没啥必要，完全可以从Resource-Allocation Graph判断）：
\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{screenshot013}
\label{fig:screenshot013}
\end{figure}

那么显然图中的有环的，所以有死锁。

\subsubsection{Several Instances of a Resource Type}
那有多个实例呢？我们有一个Detection Algorithm，然后和Safety Algorithm很关键的一个不同点在于Detection Alg.用的是Request数组而不是Need数组，且它更关注当下是不是发生deadlock，也就是保证当前的进程能够前进，未来怎么样是不管的：

书上用的是Finish数组，但是dk认为应该是Progress数组，所以这里都改成Progress数组：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 首先我们有两个向量，一个$Work$向量，长度为m，就是$Available$向量还有一个是$Progress$向量，长度为n。（对Progress的初始化和safety alg.已经开始不一样了）如果进程 Pi当前没有持有资源（Allocation[i] == 0），说明它不会阻塞死锁$\rightarrow$ Progress[i] = true，否则，Progress[i] = false
\item 之后，我们找一个i（也就是找那么一个process），它要满足Finish[i]=false，且它\textbf{当前申请的资源}，也就是\textbf{Request[i]≤Work}（当下我要的你能给，但不代表我以后要的你也能给）。如果没有这样的process i存在，那么跳到第4步。
\item 同样我们也做一个Work=Work+Allocation[i]，相当于先把work的资源给process i，把Progress为True，同时，把资源全部还回去（注意这个地方有点问题，因为对于process i来说，乐观情况下它会把资源还回去，但也有可能之后要的更多），在这之后跳到第2步重复进行。
\item 最后，如果有Progress[i]=false，那就陷入deadlock了。
\end{enumerate}

那么最后还有一个问题，我们什么时候，以怎样的频率调用这个算法呢？这取决于：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 一个deadlock发生的频率如何？
\item 出现deadlock的话回滚多少进程？
\end{enumerate}

如果检测地很随意，那很可能当你检测到的时候已经有很多环了，就会形成很多死锁，可以每次分配完资源后都detect一下，这下就能明确知道是哪步导致deadlock啦！


\subsection{Recovery from Deadlock}
那么，怎么解决Deadlock呢？有几种方式啊，一种是把所有deadlock的进程都杀了，还有一种方式是一个一个杀进程，直到死锁解开。被杀的那个进程叫做victim。

还有一个办法是资源抢占，我们首先选择一个能够minimize cost的victim，然后回退到safe state，再restart process。但是呢，有个问题，就是那个process可能很不幸啊，一直是victim，那就很糟糕了。




\newpage
\section{Exercise}
\begin{example}
A computer system has 3 types of resources A, B, and C with different numbers of instances. There are 4 running processes $P_1, P_2, P_3, P_4$. The total resources, the resource's $Allocation$ and $Max$ matrices for the four processes are shown as follows:
\end{example}
\begin{center}
\begin{table}[H]
\centering
\renewcommand\arraystretch{.9}
\normalsize
\begin{tabular}{c|c c c|c c c} 
& \multicolumn{3}{c|}{$Allocation$} & \multicolumn{3}{c}{$Max$} \\
\multicolumn{1}{c|}{Process} & $A$ & $B$ & $C$ & $A$ & $B$ & $C$ \\
\hline
$P_1$ & 1 & 3 & 1 & 6 & 5 & 3 \\
$P_2$ & 0 & 2 & 2 & 3 & 5 & 3 \\
$P_3$ & 2 & 0 & 0 & 3 & 5 & 2 \\
$P_4$ & 0 & 1 & 3 & 2 & 4 & 3 \\
\hline
total & 6 & 9 & 6 & & & 
\end{tabular}
\end{table}
\end{center}

\textbf{Keys to Q1:}
\begin{center}
\begin{tabular}{c|ccc}
& \multicolumn{3}{c}{$Need$} \\
Process & A & B & C\\
\hline
$P_1$ & 5&2&2\\
$P_2$ &3&3&1\\
$P_3$ &1&5&2\\
$P_4$ &2&3&0\\
\hline
$Available$ & 3& 3& 0\\
\end{tabular}
\end{center}


\textbf{Keys to Q2:}
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\normalsize
\item $Work=Available$, $Finish[i]=false, 0\leq i<n$.
\item Find an $i=4$.
\item $Work=Work+Allocation_i$, $Finish[4]=true$\\
\begin{tabular}{c|ccc}
& A & B & C\\
$Work$ & 3& 4& 3\\
\end{tabular}
\item Go to step 2.
\item Find an $i=2$.
\item $Work=<3,6,5>$, $Finish[2]=true$\\
\item Go to step 2.
\item Find an $i=3$.
\end{enumerate}

Safe sequence: $<P_4, P_2, P_3, P_1>$

\vspace{8pt}

\textbf{Keys to Q3:}
\begin{pt}
\textbf{很重要！考试的时候一定要查claim有没有超过max！}
\end{pt}
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\normalsize
    \item Pretend to grant.
    \item Current state:
\begin{center}
\begin{tabular}{c|c c c|c c c} 
& \multicolumn{3}{c|}{$Allocation$} & \multicolumn{3}{c}{$Need$} \\
\multicolumn{1}{c|}{Process} & $A$ & $B$ & $C$ & $A$ & $B$ & $C$ \\
\hline
$P_1$ & 1 & 3 & 1 & 5 & 2 & 2 \\
$P_2$ & 1 & 2 & 2 & 2 & 3 & 1 \\
$P_3$ & 2 & 0 & 0 & 1 & 5 & 2 \\
$P_4$ & 0 & 1 & 3 & 2 & 3 & 0 \\
\hline
Total & 6 & 9 & 6 & & & \\
Avai. & & & & 2 & 3 & 0
\end{tabular}
\end{center}
 \item Find a safe sequence:
 $(2,3,0) \rightarrow P_4 (2,4,3) \rightarrow P_2 (3,6,5) \rightarrow P_3 (5,6,5) \rightarrow P_1 (6,9,6)$
 \item Safe $\rightarrow$ Yes!
\end{enumerate}

\vspace{8pt}
\textbf{Keys to Q4:}
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
    \item Pretend to grant.
    \item Current state:
\begin{center}
\begin{tabular}{c|c c c|c c c} 
& \multicolumn{3}{c|}{$Allocation$} & \multicolumn{3}{c}{$Need$} \\
\multicolumn{1}{c|}{Process} & $A$ & $B$ & $C$ & $A$ & $B$ & $C$ \\
\hline
$P_1$ & 2 & 3 & 1 & 4 & 2 & 2 \\
$P_2$ & 0 & 2 & 2 & 3 & 3 & 1 \\
$P_3$ & 2 & 0 & 0 & 1 & 5 & 2 \\
$P_4$ & 0 & 1 & 3 & 2 & 3 & 0 \\
\hline
Total & 6 & 9 & 6 & & & \\
Avai. & & & & 2 & 3 & 0
\end{tabular}
\end{center}
 \item Find a safe sequence:
 $(2,3,0) \rightarrow P_4 (2,4,3) \rightarrow ?$
 \item Unsafe $\rightarrow$ No!
\end{enumerate}

\begin{example}
Consider the following system snapshot using the data structures in the Banker's algorithm, with resources $A$, $B$, $C$, and $D$, and processes $P_0$ to $P_4$:
\end{example}

\begin{center}
\begin{tabular}{c|cccc|cccc|cccc|cccc}
		& \multicolumn{4}{c|}{$Max$} & \multicolumn{4}{c|}{$Allocation$} & \multicolumn{4}{c|}{$Need$} & \multicolumn{4}{c}{$Available$} \\
		Processes & A & B & C & D & A & B & C & D & A & B & C & D & A & B & C & D\\
		\hline
		$P_0$ & 6 & 0 & 1 & 2 & 4 & 0 & 0 & 1 &&&& &&&& \\
		$P_1$ & 1 & 7 & 5 & 0 & 1 & 1 & 0 & 0 &&&& &&&& \\
		$P_2$ & 2 & 3 & 5 & 6 & 1 & 2 & 5 & 4 &&&& &&&& \\
		$P_3$ & 1 & 6 & 5 & 3 & 0 & 6 & 3 & 3 &&&& &&&& \\
		$P_4$ & 1 & 6 & 5 & 6 & 0 & 2 & 1 & 2 &&&& &&&& \\
		\hline
		Total &&&& &&&& &&&& & 3 & 2 & 1 & 1
		\end{tabular}
\end{center}
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
    \item How many resources of type A, B, C, and D are there? 
    \item What are the contents of the Need matrix? Fill in the above table.
    \item Is the system in a safe state? Provide your reasons.
    \item If a request from process $P_4$ arrives for additional resources of $(1,2,0,0)$, can the Banker's algorithm grant the request immediately? Show the new system state and other criteria.
\end{enumerate}


\textbf{Keys:}
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
    \item How many resources of type A, B, C, and D are there? 
    \begin{itemize}
        \item 9,13,10,11
    \end{itemize}
    \item What are the contents of the Need matrix? Fill in the above table.（太简单了，跳过）
    \item Is the system in a safe state? Provide your reasons.
    \begin{itemize}
        \item Yes, $P_0 (7,2,1,2) \rightarrow P_2 (8,4,6,6) \rightarrow P3 (8,10,9,9) \rightarrow \cdots$
    \end{itemize}
    \item If a request from process $P_4$ arrives for additional resources of $(1,2,0,0)$, can the Banker's algorithm grant the request immediately? Show the new system state and other criteria.
    \begin{itemize}
        \item This state is NOT safe. $P_0$ can be satisfied, but the available resources at that point $(6,0,1,2)$ cannot satisfy the needs of the remaining processes.
    \end{itemize}
\end{enumerate}


\newpage
\section{Concepts Organization}
\begin{enumerate}[label=\arabic*., leftmargin=3em]
\item Deadlock: A set of processes are all waiting for each other to release resources, and none of them can proceed.
\item Four necessary conditions: Mutual exclusion, hold and wait, no preemption, circular wait.
\item Resource-allocation graph: A directed graph that represents the allocation of resources to processes and the requests made by processes for resources in a system.
\item Deadlock prevention: A strategy that ensures the system will never enter a deadlock state by eliminating one or more of the necessary conditions for deadlock to occur.
\item Deadlock avoidance: A dynamic strategy that checks the current resource-allocation state before granting a request to ensure the system remains in a safe state.
\item Deadlock detection: A strategy that allows deadlocks to occur, but the system periodically checks for deadlock conditions and takes action when one is detected.
\item Deadlock recovery: A set of techniques used after a deadlock has been detected, to restore the system to a normal, non-deadlocked state.
\item Safe state: A state of a system in which there exists at least one sequence of process execution such that every process can finish execution.
\item Resource-allocation graph algorithm: A deadlock avoidance algorithm used in operating systems when there is only a single instance of each resource type.
\item The banker's algorithm: An algorithm that ensures that the system always remains in a safe state, where all processes can complete their execution without leading to a deadlock.
\item Wait-for graph algorithm: A deadlock detection method used in systems where each resource type has only one instance. 
\item Detection algorithm: A method used in operating systems to determine whether a system has entered a deadlocked state.
\end{enumerate}


\chapter{Main\_Memory}
\section{In-class Contents}
Address binding的功能是在把物理内存作为资源进行分割，保证process之间互不干扰，比如说每个process有自己的0号地址，这个是需要OS来管理的。那么如果说我的代码可以直接管理物理内存的内容，那这部分的代码完全不需要由OS管理。

同时有一个小插曲，我们现在绝大多数的OS用的都是Paging，那么是不是说其实完全不需要base and limit呢？不是这样的，因为在启动的时候会有一小段代码需要用base and limit做简单的内存管理，同时，页表也需要base and limit去查。

\subsection{Warm-up}
Memory有很多API，包括Stack Memory API和Heap Memory API。Stack和Heap是process仅有的两种使用内存的方式。
\vspace*{6pt}
\begin{lstlisting}[language=C]
void func() {
	int x;
	// Stack memory，有生命周期，由编译器隐式自动回收
	int *x = (int *) malloc(sizeof(int));
	// Heap memeory，需要使用free()等去释放这些内存（灵活但需要小心管理）
}
\end{lstlisting}

我们再来看一段代码：
\vspace*{6pt}
\begin{lstlisting}[language=C]
#include <unistd.h>
#include <stdio.h>
#include <stdlib.h>
#include "common.h"

int main(int argc, char *argv[]) {
 int *p = malloc(sizeof(int));
 assert(p != NULL);
 printf("(%d) memory address of p: %08x\n", getpid(), (unsigned) p);
 // (unsigned p表示p指向的地址)
 *p = 0;
 while (true) {
  Spin(1);
  *p = *p + 1;
  printf("(%d) p: %d\n", getpid(), *p);
 }
 return 0;
}

/* For this example to work, address space randomization should be disabled */
\end{lstlisting}

现在有两种情况，一种情况是说只有一个process在运行这个程序，那么结果是这样的：
\vspace*{6pt}
\begin{lstlisting}[language=sh]
prompt> ./mem
(2134) memory address of p: 00200000
(2134) p: 1
(2134) p: 2
(2134) p: 3
^C
\end{lstlisting}

还有一种情况是有两个process一起运行这个程序，那这个情况就比较复杂了：
\vspace*{6pt}
\begin{lstlisting}[language=sh]
prompt> ./mem &; ./mem &
[1] 24113
[2] 24114
(24113) memory address of p: 00200000
(24114) memory address of p: 00200000
(24113) p: 1
(24114) p: 1
(24113) p: 2
(24113) p: 3
(24114) p: 2
(24114) p: 3
(*@$\cdots$@*)
\end{lstlisting}

从上面这段代码中，我们可以从两个进程的memory address都相等可以知道，这是virtualization在发挥作用，CPU认为的地址其实是virtual address，表示虚拟地址中堆的起始地址都是相同的。另外，我们可以从两个进程的p的改变情况可以得知，p不是共享变量，两个process在改变各自的p的值。

\subsection{Background}
首先我们来讲一点背景知识：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 程序从硬盘中加载到内存里面，然后OS为这个程序创建一个进程，然后CPU才可以调度它
\item CPU只能访问Main Memory(only large storage)和Registers，其他的要借助OS等间接访问。
\item Memory会提供一些interfaces(read(),write()这种)，它做的事情只是按地址访问，只知道是读操作还是写操作，具体你做这个事情背后的原因它不管。
\item 在一个CPU cycle中我们就能完成register的访问，非常快对吧
\item 但是Main memory就不太行了，它相对比较慢，要很多CPU cycle，就会导致stall（滞留：CPU在等待期间没法做其他事情）
\item 啊哈，这时候我们就想着搞一个Cache吧，它的访问速度介于main memory和CPU registers之间，是不是就很棒呢。
\end{enumerate}

\subsubsection{Address Binding}
在程序的不同阶段，有三种方式来表示地址：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Symbolic（符号化）：在写源代码的时候，地址是符号化的，只是一个名字，没有具体的地址，比如说:"int count"，当我们对"count"做++操作的时候，本质是对地址上的这个value进行操作。在这种情况下，我们通过符号表达去表示内存当中的一个具体位置。
\item Logical（不知道具体的位置，但是知道逻辑地址）：这个时候，我们编译一下源代码，它就成为了可重定位的地址(relocatable addresses)，这时候就会有一个可执行文件，例如exe,img（镜像文件），当然可执行文件不是这里的重点。

比方说，我们再详细地说一下count的位置"14 bytes from beginning of this module"，那么我们拍脑袋一想就知道这是个相对地址，不是绝对地址。
\item Physical：那么最后，我们还是要找到physical memeory的对不对，其实也非常简单，当我们把image file或者是这个exe文件加载到内存的时候，count的位置现在就是Physical address，也就是绝对位置，比如说是"74014"。
\end{enumerate}

那么在这个过程中，我们可以知道每次发生bind的过程当中，就会有地址从一个地址空间映射到另一个地址空间(Symbolic->Logical->Physical)。


\subsubsection{Address Binding Time}
那么我们知道了什么是Address Binding之后，我们应该什么时候做Address Binding呢？实际上，我们有下面三种情况：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Compile time（编译时）：如果程序在编译时就已经知道将来会加载到内存中的具体地址，那么编译器可以\textbf{直接生成“绝对地址”的代码}。一旦内存位置发生变化（比如换个地方加载程序）就必须重新编译，因为地址已经写死在机器码里了。
\begin{example}
刚刚启动电脑的时候，会有些程序需要直接操作物理地址；比如说，在开机的时候，有一个"Hello，这是rgl的计组课"，那么这个内容是在固定的物理地址的。
\end{example}
\item Load time（加载时）：如果编译时还不知道程序将来会被加载到哪个内存位置，编译器会\textbf{生成可重定位代码（relocatable code）}。等程序被加载进内存时，由加载器（Loader）把这些相对地址转换成具体的绝对地址。\textbf{程序加载进去地址就不变了！}
\begin{example}
只要不需要dynamic loading的情况都是。
\end{example}
\item Execution time（运行时，最常见，也最灵活）：进程在运行时可能被移动到不同的内存位置，地址绑定必须等到程序真正运行时才完成。
\begin{example}
$cout<<"Hello,world";$
\end{example}

在这种情况下，可能需要硬件的支持来完成地址映射，比如(base and limit registers，基址寄存器、界限寄存器)

那么为什么会有在Execution time的时候才绑定地址呢？其实就是因为，\textbf{现代计算机只能load program的一部分进到内存当中，所以只有在执行阶段才能知道具体的地址。}
\end{enumerate}


\subsubsection{Dynamic Loading \& Linking}
首先补充一个知识，program的运行是compile->link->load的过程。

我们主要讲讲动态加载和动态链接之类的概念：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Dynamic loading:程序在启动时不会一次性把所有代码都加载进内存。某个函数或模块只有在被调用时才从磁盘加载到内存。

\item Static linking:编译链接时，所有用到的库文件和程序模块都被打包进一个最终的可执行文件里。一旦生成，就不依赖外部库文件了。缺点是文件体积大、更新库必须重新编译程序。

\item Dynamic linking:程序中调用的库函数不会在编译时直接包含。它们在程序运行时由操作系统在内存中动态链接。

比方说，Windows会弹出来一个错误，叫做系统缺少某个dll(dynamic link library)所以游戏不能玩，这个时候我们还会用到一个东西，叫做stub。
\end{enumerate}

\begin{definition}
Stub（存根）：一个很小的占位代码段。在使用动态链接时，程序中原本调用库函数的位置会先放一个 stub。程序运行时，这个 stub 会负责：查找库函数在内存中的位置；如果还没加载，就触发加载；然后跳转到真正的函数实现。
\end{definition}

\subsubsection{Address Space}
在现代操作系统中，程序运行时使用的地址（逻辑地址）和实际内存中的地址（物理地址）是分开的；所以说把这两者绑定（address binding）在一起的过程，是实现内存管理的核心。

我们的Address有下面两种：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Logical address(Virtual address):由CPU产生，是在程序当中看到的地址。（可以看warm-up example）
\item Physical address:内存芯片真正看到的、实际使用的地址。我们之前还说了Memory做的事情是按地址访问，执行读写操作，这个硬件接口在使用的时候，用的就是Physical address（也就是Absolute address）。
\end{enumerate}

与此同时，在compile-time address-binding和load-time address-binding的情况下，logical address和physical adress的完全一样的。只有在execution-time address-binding的情况下，两者的地址是不一样的，所以才会需要有一些算法来实现Logical和Physical Address之间的Translate。

\begin{ext}
为什么说Load-time binding的逻辑地址和物理地址也是一样的呢？因为它们加载到内存之后就不会发生改变了。
\end{ext}

因为有这两类的addresses，所以呢，也会有下面的两种address space：logical address space（程序可以生成的逻辑地址的集合，比如说我是32位的机器，那么理论上就能生成0到$2^32-1$的逻辑地址）和physical address space（实际上能访问的物理地址，如果说机器是8GB，那么物理地址空间就是0到8GB-1）

\subsection{Address Translation}
好，那么好，我们之前说当我们使用的是execution-time address-binding的时候，需要做address translation对吧，那么我们现在就来讲讲具体是怎么translation的。

我们先来看这样一个例子：
\vspace{6pt}
 
\begin{lstlisting}[language=C] 
void func() {
	int x = 3000;
	x = x + 3;
}
\end{lstlisting}

那么汇编语言是什么样的呢？（当然从计组当中的指令系统中我们会知道，这些在内存中都是一系列的01串）
\vspace{6pt}

\begin{lstlisting}[language={[x86masm]Assembler}] 
128:	mov 0x0(%ebx), %eax	; load 0+ebx into eax
132:	add $0x03, %eax		; add 3 to eax register
135:	mov %eax, 0x0(%ebx)	; store eax back to mem
\end{lstlisting}

我们现在来研究一下这背后具体一步步是怎么做的，假设x存在内存为0x1000处，\%ebx寄存器的值就是0x1000：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 获取地址为128的这个指令
\item 执行这个指令：将地址为\%ebx，也就是0x1000的数据加载到寄存器\%eax当中，所以现在\%eax寄存器的值为3000
\item 再获取地址为132的这条指令
\item 执行这个指令，把\%eax寄存器的值加3
\item 最后获取地址为135的这条指令
\item 执行这个指令，把3003写回地址为\%ebx，也就是0x1000的地方
\end{enumerate}

\subsubsection{Base and Limit}
书接上文，有一个问题啊，就是CPU只看到了三条指令的地址是128，132，135，但这些地址都是logical address，那我们怎么去找到物理地址，然后让process察觉不到呢？

那就是用base and limit registers（基址寄存器和界限寄存器），用他们来定义程序的“逻辑地址空间”在物理内存中的位置和大小。

\begin{pt}
那么每次在user mode访问内存的时候，出于protection的考虑，CPU会检查一下这样做是否合法，如果合法，那就可以通过下面的公式把物理地址算出来（这件事情是由硬件实现的，没有指令显示做这个事情）：
\begin{center}
$physical\ address = virtual\ address + base$
\end{center}
\end{pt}

\subsubsection{Memory Management Unit}
base和limit寄存器都是存在chip里面的寄存器，一般性每个CPU有一对。

\textbf{下面讲一个很重要的概念：}

\begin{definition}
MMU:负责完成地址翻译（逻辑地址->物理地址）以及检测是否越界（是否小于Limit）
\end{definition}

\subsubsection{Summary}
\textbf{这里又是一个很重要的重点！！！}

Base-and-limit能够有这些好处：Transparency（process不会感知物理地址的变化）、Protection（Limit会做保护，保证你不会出界）。那么有Efficiency吗？在时间上确实是这样的，因为全都由硬件完成，但是在空间是不是这样的，存在很多空间浪费，我们可以从下面这张图来看看。

这个问题被称为Internal fragmentation，也就是说内部存在无法被使用的空间碎片，因为要加载一个process的时候，必须要给它分配heap、stack的空间，但是因为heap、stack的大小不确定，这个时候就只能尽可能开更大的空间来保证process的运行，那这样就空间浪费了嘞。

\begin{figure}[H]
\centering
\includegraphics[width=0.2\linewidth]{internal_fragmentation}
\label{fig:internalfragmentation}
\end{figure}

\subsection{Segmentation}
由于时间原因，只能将下面的重点内容分条陈述：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 重要思想就是每个segment都有自己的base和limit，同时有自己的data, code, heap, stack这种，而不是在整个MMU里面只有一对base and bounds.

\item 如果你试图访问一个illegal的address，那你就会引发segmentation fault。

\item 当OS要做context switch的时候，base and limit的信息也要存起来

\item Segmentation能解决internal fragmentation的问题，但会产生external fragmentation的问题。
\end{enumerate}

\subsection{Free Space Management}
在分配空间的时候，我们会需要做malloc()和free()对吧，那么现在如果我有5MB的空间，申请了3MB，就会把空间分成3MB和2MB。如果那3MB的空间用完了，又会和边上的2MB重新合成5MB，这很好理解。

在分配空间时，我们有四种方式：
\begin{itemize}
\item Best-fit(smallest fit)：找满足条件最小的free space
\item Worst-fit:找满足条件的最大的free space
\item First-fit：找第一个满足条件的free space
\item Next-fit：从上一次被分配空间的地方开始，先看那个地方能不能分配，如果不能就到下一个free space找
\end{itemize}

\begin{pt}
First-fit和best-fit表现都比worst-fit好，不管从时间角度还是空间角度。

但是从空间利用角度，first-fit和best-fit是差不多的，而且first-fit还更快。
\end{pt}

\subsection{Paging}
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Paging:物理地址可以看作是一个数组，数组的内容是固定大小的slots，被称为page frame。
\item 有什么好处呢？Flexibility（虚拟地址和物理地址都可以是零散的）和Simplicity（找一个page frame非常快）。
\item Physical Frame Number(PFN)
\item Virtual Page Number(VPN)
\item Page Table就是VPN和PFN的映射，一般每个process有一个page table（里面的内容被称为page table entry，就是一堆二进制数）
\item Page Table不存在MMU里面，存在kernel physical memory里面
\item Page-talbe base register指向page table的为止，Page-table length register表示page table的大小。这俩玩意是在MMU里面的。
\item 那么PTE里面是什么东西呢？\textbf{注意，不需要VPN}，因为在virtual address本来就是顺序的，所以PTE里面会有PFN、valid bit（访问一个valid bit为0的页，可能就page fault了）、present bit（这个page是不是在memory里面）、dirty bit、reference bit(accedded bit)（这个page有没有被读/写过、LRU）、一些和缓存相关的bits
\end{enumerate}

\subsection{Translation Lookaside Buffer}
别告诉我这玩意你忘了！TLB！

现在的问题就是，pagign有点慢，因为对于segmantation来说，CPU直接用base and limit就解决了，paging要查表，表还在内存当中，这非常不好！

\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item TLB是MMU的一部分，它做的事情其实就是缓存，存最近的映射关系
\item 由操作系统来处理TLB missing-handling的事情，因为操作系统可以选择任何适合的页表数据结构（链表、树、哈希表等）。这样硬件不必固定为某种结构，系统设计更灵活。
\item TLB和page table不太一样，它必须要record VPN，原因也很显而易见。
\item 但是有一个问题啊，TLB服务很多processes，如果你做了context switch，理论上你的VPN和PFN就对不上了，有两个方式，一个方式是重写你的TLB，当然这很费时间，还有一种方式就是加一位ASID(Address space identifier)在辨认你的这条映射是属于哪个process的。
\item 当然，可能会出现两个process的某个VPN映射到同样的PFN上，这很好，可以共享代码。
\item Effective Access Time(EAT)：这东西很重要，但是它不考(LOL)
\end{enumerate}

\textbf{这个东西，very very very important!}

那么我们怎么去选择page size呢，我们希望它是大点好还是小点好呢（虽然说到底，一般的page size都是4KB）？
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 希望small page:

Fragmentation（很好理解，让internal fragmentation最少）、locality/resolution（page越小，越好刻画局部性和分辨率）

\item 希望large page:

Table size/page faults（很好理解，large page->less pages->less PTE->less possibility of page fault）、I/O overhead（这个不太好理解，首先I/O指的是内存和disk之间的数据交换，然后在disk当中，顺序读写的速度会远超随机读写，所以对于同样大小的空间，读一个大的page比读分散的小的page要快很多）、TLB reach/TLB size（TLB reach指能cover多少内存，page越大，cover的肯定也就更多、TLB size和table size同理）
\end{enumerate}

\subsection{Structure of the Page Table}
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 核心思想就是，本来一个process一个page table，现在根据它的逻辑段，一段一个page table，那可能这个process有code section、stack啥的，最后一个process就对应了几个page tables。
\item 可以理解为，一个process里面的每个segment都有自己的base and limit，还有自己的page table。
\item 但是现在问题也出现了，因为每个segment大小不一样，所以分配的page table的大小也不一样，那么其实提高了OS管理这些page table的难度。
\item 那么为了解决这个问题，我们提出了multi-level page tables，也就是用paging and paging，核心思想就是，把有用的entry留下，没用的“扔掉”（也就是说明它是not allocated的）
\item Page directory table(outer page table):包含一系列page directory entries(PDE)，每个page有这样一个entry，PDE里面含有PFN和一个valid bit，那个valid bit为1就表示page table中至少有一个page是有内容的。
\item 在page套page的模型当中，一个页是4KB，对应了1024个PTE。同时，base and limit不再直接指向Linear Page Table，而是指向Page Directory Table。
\item Page directory table的一个entry指向一整个page，也就是1024个PTE。
\item 但是这里有个问题，如果说我们没有在TLB找到，就要先访问外部表，再访问内部表，最后找到对应的内存：Time-Space Trade-off。
\end{enumerate}








\newpage 
\section{Exercise}




\newpage
\section{Concepts Organization}

\begin{enumerate}[label=\arabic*., leftmargin=3em]
\item Program loading and linking: The process of bringing a program into memory (loading) and resolving symbol references (linking) to make it executable.
\item Address binding: The mapping of symbolic or logical addresses to physical memory addresses.
\item Logical address space: The address space as perceived by a process.
\item Physical address space: The actual locations in main memory where data and instructions reside.
\item Swapping:
\item Base and limit:
\item MMU:
\item Address translation:
\item Free space management:
\item Segmentation:
\item External fragmentation: Total memory space exists to satisfy a request, but it is not contiguous.
\item Paging:
\item TLB:
\item Page size issues:
\item Internal fragmentation: Allocated memory may be slightly larger than requested memory; this size difference is memory internal to a partition, but not being used.
\item Paging and Segments:
\item Hierarchical page tables:
\item Hashed page tables:
\item Inverted page tables:
\end{enumerate}










\chapter{Virtual Memory}

    \section{In-class Contents}
    \subsection{Warm-up}
    \begin{definition}
    Segmentation fault:访问了不该访问的内存而引发的错误
    \end{definition}
    \begin{enumerate}[label=（\arabic*）, leftmargin=3em] % 调整左侧缩进
\item free list用于追踪哪些内存还没有被使用过
\item OS为了启动process，需要先为其分配内存，再set base/limit寄存器
\item 硬件用base and limit寄存器来翻译虚拟地址来取指令和完成load/store操作
\item 如果要从一个process A切换到另一个process B，OS会把A的数据（包括base/limit寄存器的内容）存到proc-struct(A)当中，并将B中的内容都取出来
\item 如果说process访问内存的时候出现out-of-limit（由硬件检查，一般是MMU），那么OS就会执行trap handler，并回收B的空间
\end{enumerate}

    \subsection{Background}
    一个现实问题：王者荣耀的容量的很大的，而且其中可能会有不常用的路径和大型数据结构（梦境大乱斗、人机对战），这个时候我只是想玩排位赛，可以让其他的内容自己去下载。\\
    \indent \textbf{Benefits:}
\begin{enumerate}[label=（\arabic*）, leftmargin=3em] % 调整左侧缩进
\item 我的程序不会因为物理内存小而被限制
\item 每个程序需要的内存更少->我就可以加载更多的程序->增加CPU利用率和吞吐量，同时响应时间并没有增加
\item 更少的I/O切换操作（因为在物理内存中放的都是常用的部分，而且量比较少，所以不需要很多切出来切出去） 
\end{enumerate}
\begin{definition}
Virtual Memory：介于Logical Memory(由CPU生成)和Physical Memory之间的存储层级，与Logical Memory很相似
\end{definition}
\begin{vs}
区分Virtual Memory和Logical Memory：
\begin{enumerate}[label=\arabic*., leftmargin=3em] % 调整左侧缩进
\item Logical Memory是CPU看到的，用户程序使用的；而Virtual Memory是硬件向下实现的部分，也就是Logical Memory的底层实现
\item Logical Address = Virtual Address（因为他们都用的是Hardware Interface）
\end{enumerate}
\end{vs}

\begin{enumerate}[label=（\arabic*）, leftmargin=3em] % 调整左侧缩进
\item 我每次只需要加载程序的一部分
\item \textbf{逻辑地址空间远大于物理地址空间}
\item 多个程序共享地址空间
\item 允许更多程序并行运行
\item 需要更少的I/O
\end{enumerate}
\qquad 那我这时候想要swap呢？（通过Backing store，也就是disk）这样我就可以玩王者荣耀的其他模式了。

\begin{definition}
    Memory Map：将Virtual Memory映射到Physical Memory->Address Translation
\end{definition}

\subsection{Swapping Mechanisms}
\begin{definition}
    Swap Space：预留给backing store的空间，一般情况下，物理内存如果是4G，swap space要是物理内存的两倍，也就是8G
\end{definition}
\textbf{OS可以按照page的大小来访问（读/写）swap space}，同时需要记住每一页对应的disk中的地址（这样才可以进行swap操作）

\begin{definition}
    Present Bit（在Page Table Entry里面）：如果是1，那么page就在physical memory，如果是0，就在disk里面
\end{definition}

\begin{definition}
    Page Fault：要找的page不在physical memory里面
\end{definition}

\begin{vs}
Present bit = 0 只表示page不在memory（call page fault->invoke Page fault handler， handler来swap）；但是Valid bit = 0， 都不在backing store里面（call segmentation fault）
\end{vs}

Page fault handler具体做什么事情呢？
\begin{enumerate}[label=（\arabic*）, leftmargin=3em] % 调整左侧缩进
\item 找一个空的帧（frame）
\item 把我要的page换进去（OS是知道page在disk中的位置的）
\item 将Present bit置为1
\item 重新运行导致Page fault的那个指令
\end{enumerate}
\subsection{Page Replacement}
OS要关注的两个事情：为每个process要分配多少frame；当frame都用完了，应该怎么去做Page replacement，使得Page fault能够降到最低？

\begin{definition}
    Modify(Dirty) bit：Page被修改过了，那么在调出frame的时候需要写回disk，如果没有被修改过，直接丢弃就可以了
\end{definition}
Page replacement能够做到通过swapping，将很大的Virtual memory映射到较小的Physical memory。\\
\indent Basic Page Replacement思想：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em] % 调整左侧缩进
\item 找到disk里面我要的page的位置
\item 找到一个空闲的frame（如果有那正好，如果没有，就要想办法替换；如果被替换的frame是dirty的，那还要先写回disk一下）
\item 把我要的页面放进腾出来的空frame，更新页面和frame tables
\item 重新运行导致中断的指令
\end{enumerate}
\begin{pt}
    一次Page fault会牵扯到两个pages
\end{pt}
\subsubsection{Page-replacement Algorithm}
目标：给定了每个process的frames，想要降低page fault
\begin{definition}
    Reference string：
    \begin{itemize}
        \item 只是page number，不是address！！！
        \item 如果string连续两个相同的字符，要删掉一个
        \item 算法page fault的结果和frames的数量有关（注意：并不是frame越多越好，后面会提到）
    \end{itemize}
\end{definition}
\begin{example}
7,0,1,2,0是一个reference string，同时7,0,1,2,0和7,0,0,1,1,2,0是一样的，算page fault的时候记得先把重复的数字删掉
\end{example}
\subsubsection{Optimal Algorithm}
也被成为是Belady's MIN algorithm，核心思想是替换掉最长时间才会用到的那个page number（check the future）。\\
\indent 但是实际上我们并不能预知未来，所以没法用Optimal Algorithm，我们更多用这种算法作为一种benchmark或者baseline。

\subsubsection{FIFO Algorithm}
核心思想：先来的先被替换，用一个队列来追踪要被换到的pages
\begin{pt}
    Belady's Anomaly（异常）：增加更多的frame反而可能导致page faults更多。（我们学过的算法里面，只有FIFO有这个问题）
\end{pt}
\begin{example}
    1,2,3,4,1,2,5,1,2,3,4,5 for 3 frames（9 faults） vs. for 4 frames（10 faults）
\end{example}
\subsubsection{LRU Algorithm}
核心思想：替换掉最远使用的那个；是most common use 的algorithm

两种实现方法：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em] % 调整左侧缩进
\item Counter implementation：每个page entry有一个计数器，来记录page被换入到frame的时间，如果需要替换，找到那个数据最小的数（Confusing，我的理解是一开始给一个大的数，然后一点点减这个数）去替换掉

Problem：每次找到我想要替换的page很慢，因为要把每个counter遍历一遍
\item Stack implementation：用一个双向链表来记录放进去的数

核心思想：每当有一个page被refer了，将这个page放在链表head的位置，替换掉的是原链表Tail位置的page。

Problem：每次有page进来，要改变6个指针的值，那么维护stack是比较慢的，但是找那个page很快。
\end{enumerate}
\begin{exam}
LRU和OPT算法是基于stack algorithms的算法，不会出现Belady's Anomaly。（也就是说，只要是基于栈实现的，都不会有这个异常问题）
\end{exam}
\begin{pro}
The set of pages in memory for n frames is always a subset of the set of pages that would be in memory with n+1 frames.
\end{pro}

\subsubsection{LRU Approximation Algorithms}
想要解决的问题：LRU需要特殊的Hardware support并且还是比较慢，所以希望用近似算法来做。
\begin{definition}
    Reference Bit：表示这个page是否被访问过，0就是“没有”被访问，1就是被访问过了
\end{definition}

在进行替换操作的时候，将reference bit = 0的替换掉，新加入的page，设置其 reference bit = 1，问题在于这个方法没办法知道具体是谁更早被访问。

\begin{pt}
Reference bit = 0只是表示其近期没有被访问过，具体怎么变化的见Second-chance 算法
\end{pt}

\begin{definition}
Additional-Reference-Bits Algorithm：用8个bit来表示Reference，这样就能知道对于一个页面，谁更早被访问，谁更晚被访问
\end{definition}

每次操作，将所有的bits往右移动一位，如果这个page被访问了，最高位置1，否则为0；在替换的时候，每次替换掉真值最小的那个page。
\begin{example}
    P1:00100100,P2:00100000；Obviously, P1比P2对应的值更大，所以要替换的话就替换P2。
\end{example}

\begin{exam}
    Second-chance (Clock) Algorithm：FIFO的思想，但是会循环遍历pages去找我要替换的page，首先看Reference bit是否为0，如果是，直接替换掉；如果为1，将其至为0，相当于给它第二次机会。
\end{exam}

\begin{exam}
Enhanced Second-Chance Algorithm：相比second-chance 只用一位的reference bit， 这里用一对bits（reference, modify）

替换顺序：(0,0)->(0,1)->(1,0)->(1,1)

核心思想：越不常用、且没有被修改的页面，越适合被替换掉
\end{exam}

\begin{pt}
    为什么会有(0,1)的被替换，因为reference bit = 0代表没有被访问，modify bit = 1表示被修改了。

    Answer：只是最近没有用，modify bit从1变为0了，不代表说这个page从未被使用过。
\end{pt}

\subsubsection{Other Algorithms (Less Important)}
\begin{enumerate}[label=（\arabic*）, leftmargin=3em] % 调整左侧缩进
\item Counting Algorithms（Least Frequently Used \& Most Frequently Used）
\item Page-buffering Algorithms：

核心思想：一直保留空的frame，这样可以直接把page读入，同时\textbf{方便的话，换出一个}

改进1：记录哪些页面被修改过，这样当backing store空闲的时候，可以写回disk，并将页面设为未被修改过的。

改进2：不立即把空白帧的内容给清空掉，记录它装的页面内容
\end{enumerate}

\subsubsection{Conclusion \& Comparision}
所有的OS都会去“猜”未来可能的page，但是可能猜错；而数据库能够帮助优化页面管理\\
\begin{definition}
    Double Buffering：OS有一份page的内容用于I/O，同时Application有一份page的内容用于自己的工作->不必要的内存占用和冗余操作。
\end{definition}
\begin{example}
    在放视频的时候，OS为播放器分配内存缓冲区，存放视频数据，而应用程序（播放器）也需要将当前帧放在bufferA，下一帧放在bufferB->同一份数据被放在两个不同的地方。
\end{example}
\begin{definition}
    Raw Disk Mode：Applications 不经过OS，直接获取page的内容。
\end{definition}

两个Benchmark：一个之前提到的，是OPT，另一个是Random Algorithm（随便换）。

比较不同的Page replacement 算法：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em] % 调整左侧缩进
\item The No-Locality Workload（page 随机到来）：OPT > RAND = LRU = FIFO
\item The 80-20 Workload（80\%的访问集中在process里20\%常用的pages，另外20\%的空间给process80\%不那么常用的pages）：OPT > LRU > CLOCK(和LRU差不太多)> FIFO = RAND
\item The Looping-Sequential Workload（循环访问页面）：如果frame不够多，OPT > RAND > FIFO = LRU = 0；如果frame足够多，OPT = RAND = FIFO = LRU = 100\%
\end{enumerate}

\subsection{Allocation of Frames}
我们要解决的问题：我们要给每个process分配多少frames。

每个process有最少需要的frames数量，比如IBM 370至少要6个pages才能实现MVC指令；frames的最大值是整个系统所有的frames。

\subsubsection{Fixed Allocation}
\begin{enumerate}[label=（\arabic*）, leftmargin=3em] % 调整左侧缩进
\item Equal Allocation：均分
\item Proportional Allocation：按process的大小的比例来分
\end{enumerate}

\subsubsection{Priority Allocation}
根据process的优先级来分配，本质也是一种proportional allocation，只是一个根据优先级来，一个根据process的size来。

\subsubsection{Replacement Strategy}
\begin{enumerate}[label=（\arabic*）, leftmargin=3em] % 调整左侧缩进
\item Global replacement：从所有的frame里面去找一个frame替换，也就是不同process之间可以抢frame来用

Pros：更大的吞吐量

Cons：Process自身无法控制自己的Page fault
\item Local replacement：每个process只能从分配给自己的frame里面去替换

Pros：表现更加consistent

Cons：有的时候不需要那么多帧，那就有内存没被用上
\end{enumerate}

\subsubsection{NUMA:Non-Uniform Memory Access}
CPU访问Memory的速度也会有优先级，设置lgroups（内存低延迟组），这样“离CPU更近”的内存会被更快访问。

\subsection{Thrashing}
\begin{definition}
    Thrashing（抖动、颠簸）：A process is busy swapping pages in and out.
\end{definition}

\begin{example}
    如果一个Process没有足够的page，那么page fault会一直很高，所以就需要不停的从disk里面拿page来做replacement，然而replacement的动作是一个I/O操作，这就导致了本来一个CPU-Bound的进程，因为频繁的I/O，导致CPU一直空闲，操作系统认为它是一个I/O-Bound的进程，就会增加multiprogramming的程度（Completed by long-term scheduler）。那么系统添加的process变多了，每个process分配的frame更少了，那page fault又变多了，\textbf{凉性循环}。
\end{example}

所以说，当多道程序的程度增加，CPU Utilization一开始是会增加的，直到发现没有enought frames，CPU利用率就减少了，开始\textbf{颠簸}。

\begin{definition}
    Locality Model（局部性模型）：Process从一个局部性迁移到另一个局部性，且局部性之间可能有data和statements的重叠。
\end{definition}
\begin{example}
    程序执行到函数A，访问A中的数据，之后执行到B，就要访问B中的数据，这个过程就说局部性被迁移的过程。

    理解：Process的执行不是均匀访问整个内存，而是阶段性访问内存其中的一部分数据和指令。随着程序逻辑推进，不断迁移访问新的局部区域。

    For Example：王者荣耀把游戏分为场景模块和战斗模块，这就是两个Locality。
\end{example}

那么Thrashing是怎么发生的呢？说白了就是Locality的大小总和超出了系统的物理内存->我们怎么做？用local or priority page replacement（个人理解：local对应PFF，priority对应working set）。

\subsubsection{Working-Set Model}
核心思想：看最近$\Delta$时间的页面访问（被称为WSS：working set size of Process $P_i$），来判断一个进程目前真正需要哪些页面。（$\Delta$：working-set window）

如果$\Delta$很小，那可能没法包含所有的局部性信息；$\Delta$很大，可能包含多个局部性；$\Delta$是无穷，那么包含整个program，我要它也没用，失去局部性的概念了，所以$\Delta$要合理选取。
\begin{definition}
    Total Demand Frames(D)：把所有的WSS加起来，表征$\Delta$时间内系统所需要的页面需求量。
\end{definition}

如果D>m（物理内存大小）->寄了孩子，你要thrashing了

好，那么好，怎么解决这个问题？

Working-Set Model给出了一个方案：如果D>m，就把一些process给挂起来或者换出，来减少the degree of multi-programming（这个事情是由medium-term scheduler完成的），同时，被换出去的process的什么状态（比如说是ready）状态，之后换进来的时候也是什么状态。

现在我们遇到了第二个问题：精确记录每个页面是否在$\Delta$时间内被访问的开销是很大的，所以我们希望找一个近似的算法来计算WSS，同时不要很大的开销。我们采取的方法是通过interval timer interrupt和reference bit来做这个事情。

\begin{example}
    假如$\Delta$=10000，我们可以每5000和时间单元中断一次，我们为每个page分配两个bit，一个bit表示前5000时间单元它有没有被访问，另一个bit表示后5000时间单元它有没有被访问。那么我怎么来确定bit是1还是0呢？看page的reference bit，如果前5000时间单元发现reference bit为1，那么第一个bit就被置为1->(1,0)，再把page的reference bit置0；如果在后5000时间单元，reference bit还为1，那么第二个bit也为1->(1,1)。不管最后这两个bit有几个1，只要有一个，那么就可以说明这个page是在working set的。
\end{example}
\begin{pt}
    这个方法是不够精确的，因为每5000时间单元采样一次，分辨率是比较低的(resolution)，那么我们可以改进一下，比如说每1000时间单元采样一次（同时为每个page分配10个bits），这样分辨率会高一些。
\end{pt}

\subsubsection{Page-Fault Frequency}
核心思想：我去算一个process的page-fault frequency(PFF)，然后设定一个upper bound和一个lower bound，用local replacement policy来调整这个process分配的frame。

这种方法比WSS更加直接，因为如果PFF很高，说明process缺页；如果PFF很低，说明frame很多，那么内存资源就被浪费了，系统的并发程度不高，那就减少frame的数量。

\subsubsection{Conclusion}
对于PFF方法来说，它通过调整单个进程的frame数量来尽量避免Thrashing，而WSS则是通过调整系统运行的process数量来尽量避免Thrashing。

他们之间也是有联系的，如果working set大，那么活跃的页面就多，很可能frame会不够，那就需要分配更多的frame，如果分配多到能够覆盖working set，PFF就会下降（同时注意不能分配太多frame，会导致concurrency下降的）。

\subsection{Other Concepts \& Issues}
\subsubsection{Demand Paging}
常规情况下，我们是要把整个process加载到内存的，但是为了节省空间，我们使用“按需分页”的方式，当我们需要的时候，把一个页拿进内存。

这样做的好处有：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em] % 调整左侧缩进
\item 会减少I/O，不会有没用的I/O（这里听上去会容易疑惑，因为一个一个page搞进来会发生很多page fault，导致更多的I/O，但是，实际上如果把整个process搞进来，用的I/O更多）
\item 需要更少的内存
\item 响应更快（程序更早开始执行，不用全都加载进来）
\item 容纳更多的用户（显然，并发率提高）
\end{enumerate}

当我需要一个page，首先看是不是无效的reference，如果是无效的，直接abort the request；如果是有效的，并且这个page不在memory（很可能是因为这个page第一次被需要），这个时候就page fault并把这个page搞到内存中。

\begin{definition}
Lazy Swapper：从来不会把page swap进内存，除非这个page被需要

似乎会增加一些page fault，但可以大大减少不必要的内存加载和I/O操作。
\end{definition}
\begin{definition}
Pager：Swapper that deals with pages.
\end{definition}

我们再深入探讨一下这种“swap”方式：

除了交换内容之外，Demand paging有三个主要的工作，一个是需要page-fault handler来处理中断，这个操作相对是比较快的；把page搞进来，这是一个I/O操作，\textbf{是Demand paging最耗时间的一步}；还有一步是重新启动process，也就是运行那个引起中断的语句，也不要啥时间。

在这种情况下，设p是page fault rate，我们再来算一下这样情况下的Effective Access Time：
\begin{equation}\begin{aligned}EAT=&(1-p)\times T_{memory\_access}+p\times(T_{page\_fault\_overhead}+T_{swap\_page\_out}+T_{swap\_page\_in})\end{aligned}\end{equation}

我们看PPT60页，会发现这样做的EAT还是很大的，所以我们希望能够改进一下这个算法。

这时候我们做一件事情，就是Prepaging（预分页），先加载一些页面进来（当然不会把所有的process加载进来），目的是减少page faults的数量（可能加载一些关键的page吧）

同样也会有问题，如果预先准备的page没有被用到，那么同样I/O的时间被浪费了，空间也会被浪费。

那么我们要做的就说一件Trade-off的事情，我们要看到底是不用Prepaging时的page fault花的时间多，还是预先准备“没用”的page会花费更多时间。

\textbf{下面的部分dk几乎很快带过去了，可能没有那么重要。}

了解完Demand Paging的算法之后，我们尝试做一个优化：
\begin{definition}
    Swap Space：也是disk里面的存储，但是它的I/O速度比普通的file system I/O的速度要快，用大的块来存储，需要更少的管理
\end{definition}

在进程启动，被加载到内存（可能是Prepaging）的时候，这个进程的内容会复制一份到swap space里面，这样做的好处就是，我的page in和page out都可以在swap space里面做，就不用通过disk了。除此之外，如果要释放frame的时候，可以直接丢弃内存里page的内容（因为我有备份），但是注意丢弃之前要看一下是不是要写回swap space（比如堆栈这种和文件内容本身无关的内容或者是page被改过的内容）。

对于现在的系统来说，一般没有swapping的机制，用的是闪存。

现在我们再做一个优化，我们连页表都不全部加载进来，需要的时候再加载进来。

\begin{definition}
Inverted Page Table（倒排页表）：不会有完整的page table，完整的在disk里面，需要了再去找
\end{definition}

\begin{example}
    进程A有虚拟页号0,1,2,3，这时候内存只加载了页号0,1（说明倒排页表也只会有0,1页号到物理地址的映射），这个时候我要访问虚拟页号2，首先2肯定不在physical memory（$Page fault\times1$），再去倒排页表里面找对应的映射，发现倒排页表里面也没有（$Page fault\times1$），那只能去disk（也就是backing store）里面找有全部信息的那个页表。
\end{example}

\subsubsection{Copy-on-Write}
\textbf{这是一个相对比较重要的概念，同时涉及memory management和CPU management。}

背景：有一个process，它想要另一个process的page，但是嫌copy太复杂太麻烦，就想让这两个进程指向同一个物理页。

\begin{definition}
    Copy-on-Write(COW)：这当OS需要将一个页面从一个地址空间copy到另一个地址空间的时候（典型情况：fork()），我们不是真正做一个复制，而是让两个进程指向同一个目标地址，并且让这个页面对于两个进程来说都是\textbf{只读}的。
\end{definition}

但是啊，有个问题，如果有一个process的地址空间试图修改这个page，那么OS就会意识到，并且只能真正地分配一块空间给你去修改。

那么COW有什么应用呢：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em] % 调整左侧缩进
\item 共享库：多个process共享一份只读数据
\item 在UNIX系统，COW允许父子进程共享内存的page，分为两种情况：

fork()：有COW；

vfork()：没有COW，也就是子进程会直接共享父进程的地址空间，效率更高，但也更危险。

vfork是fork的变种，它做的事情就是，在子进程生成之后，父进程挂起，子进程要立刻调用exec()，把自己替换成全新的process，一个和父子进程都没关系的process，这样子才能保证父子进程的内容是不被修改的。
\end{enumerate}
\begin{example}
    UNIX的Shell在输入命令后，会调用vfork()在创建子进程。
\end{example}

\subsubsection{Memory-Mapped Files}
\begin{definition}
    Memory-Mapping（内存映射）：和swapping很像，本质做的事情就是使用mmap()来将一个已经打开的文件（例如stdin,stdout,stderror）映射到process的虚拟内存，再返回一个指向文件在虚拟内存起始位置的指针p
\end{definition}

那它有什么好处呢？我们常规的例如调用read()，write()来对文件进行读写，本质输入file I/O,也就是和磁盘进行交互，那肯定是很慢的。现在我们用Memory-mapping，把文件从disk映射到内存当中（一般是把一个disk block映射到内存的一个page），这时候我们做的读写操作就是在和内存交互。

\begin{pt}
    文件的读使用demand paging策略。
\end{pt}

那么我们还可以知道，如果说我们要访问一个没有放进memory的文件的一部分，就会发生page fault，之后我们就可以把对应的disk block swap in到内存，变成一个新的page。同样，当我们要把这个file全部swap out的时候，再把内存映射的内容写回disk。

\subsubsection{Allocating Kernel Memory}
背景：kernel不知道用户是怎么使用physical memory的，只知道把空间分配出去就结束了，但是对于kernel memory，kernel是知道内存什么时候释放，是不是需要物理连续的。kernel memory的空间一般来自free-memory pool，也就是系统未被使用的内存区域（包括碎片去雨）。kernel会申请不同大小的空间存放它的data structures。同时有些kernel memory可能需要连续空间（例如I/O，因为需要速率固定），这样不用tranverse就能直接定位到我要的内容。

现在，我们有两种方法来分配内核内存。

\begin{definition}
    Buddy Allocator（是所有内核内存分配的底层机制）：分配固定大小的段，里面是连续物理空间的页。
\end{definition}

分配的大小只会是2的幂，具体怎么分配看PPT68页就行，非常容易理解。优势：能够快速合并小的块合成大的块。缺点：空间碎片化，多出来的会被浪费。

好，那么为了解决这样的空间碎片化的问题，我们提出了另一种内核空间的分配方式。

\begin{definition}
    Slab Allocator（现在更多说Slub Allocator）：这种分配方法不会像buddy allocator给每个数据结构对应的object分配一块内存，而是按层级分成object->slab->cache。
\end{definition}

\begin{definition}
    Slab：是一个或者多个物理连续的页。
\end{definition}

\begin{definition}
    Cache（只是一个名字，和CPU层级结构的Cache没有关系）：由一个或多个slab组成。
\end{definition}

每个Cache存放一种data structure，同时这个Cache存放的是这类data structure的所有实例。

\begin{example}
    比如说这个data structure是PCB（进程控制块），那么对应的Cache就会有系统所有的PCB。
\end{example}

当cache被创建，里面填满空闲的对应的objects；而当这个数据结构被存进来，对应的object被标记为used。如果slab放满了used objects，它就会开一个新的slab放新进来的object，如果slab不够了，会分配新的slab。

这么做的好处是不会存在碎片化空间，同时满足更快的内存请求（因为kernel知道每个kernel data structure的大小，在物理地址当中又是连续存放，可以用index来找，所以快）。

\begin{pt}
    slab是连续的，cache不一定，也就是说slab之间不一定物理连续；同时，一般情况下，slab有空的地方优先填objects。
\end{pt}

那么把两个方法融合一下呢？就是原先是为每个object分配一块内存，现在是为每个slab分配一块内存，可以减少空间碎片化的程度。

\begin{definition}
    I/O Interlock：当执行I/O的时候，不能把正在执行的页面给swap out到disk里面去，需要把这些页面固定(Pin)在内存当中。
\end{definition}

\subsubsection{The Linux Address Space}
分为user portion和kernel portion，里面都是一些code、heap、stack。kernel portion里面就说一些handler、system call之类的。

那么为什么这么做呢？因为所有进程是共享一个内核空间（kernel portion）的，所以在切换进程的时候，user portion的地址是要变的，但是kernel portion是不变的。

如果说我想要访问kernel virtual pages时，我需要trap into the kernel（比如read()，write()）然后将CPU切到特权模式。

两种kernel virtual addresses（Difficult but useful）：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em] % 调整左侧缩进
\item Kernel logical addresses（内核最常用的虚拟地址），有以下这些特点：

使用kmalloc来分配这种类型的空间（类似用户态的malloc()）

\textbf{大部分}kernel数据结构在这里（page tables, per-process kernel stacks），对于系统是很重要的数据结构。

\textbf{不能被swap到disk里面}（因为不能接收磁盘换入换出的延迟）

能够直接从虚拟地址映射到逻辑地址，有两个好处：1.容易进行翻译；2.方便用DMA。
\item Kernel virtual addresses：不是直接映射，使用vmalloc来分配空间，分配的量比较大，但是一般在物理上不连续，和用户层面的虚拟地址很像。
\end{enumerate}

\subsubsection{Page Table Structure}
我们所知道的page都是4KB大小的，在64位操作系统当中，需要4次翻译才能找到目标page，这是非常慢的。所以现在我们的电脑里面是有4KB的page，也会有2MB的page，这样子能让TLB表现更好，也需要更少的page table entries。

\subsubsection{Page Cache}
和kernel data structure里的Cache很像，核心思想是把数据放在内存里面，这些实体被放在page cache的哈希表当中。

Page cache的内容主要来自以下三个地方：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em] % 调整左侧缩进
\item memory-mapped files
\item file data和metadata（元数据）（Chapter12,13详细说明）
\item 堆和栈
\end{enumerate}

\begin{pt}
    \textbf{后面的内容都不是很重要，有个概念就行。}
\end{pt}

\subsubsection{Page Replacement}
问题：标准的LRU是比较有效的，但是有的时候表现会比较糟糕，比如说面对循环访问的大型文件的时候，可能比RAND糟糕。

这个时候，Linux打算用两个队列来解决这个问题（本质上还是LRU的近似算法）。一个队列是不活跃队列，当一个page是第一次访问，就放在这个队列；另一个队列是活跃队列，当一个page被再次访问，那就放到这个队列里面。那么我们在做替换的时候，就会替换掉不活跃队列的内容。同时，Linux也会定期把page从活跃队列里面搞到不活跃队列，保持活跃队列的大小大概是整个page cache的2/3。

\subsubsection{Security And Buffer Overflows}
这里介绍了三种缓冲区溢出攻击和对应的解决方案。\\
\begin{lstlisting}[language=C]
int some_function(char*input){
  char dest_buffer[100];
  strcpy(dest_buffer,input);
}
\end{lstlisting}
\begin{example}
    第一类问题：
    
    问题声明：从上面的代码可以看到，input的大小要是超过100，那么buffer后面的内存区域就会被input的内容覆盖，恶意的程序员就可以在input里面加恶意数据来攻击。

    应对方案：NX(No-eXecute) bit in AMD。这样做可以规定一块区域的代码是不可运行的（例如stack）。
\end{example}
\begin{example}
    第二类问题：return-oriented programming(ROP：返回导向编程，是很有名的攻击)

    问题声明：这类问题源自return-to-libc（libc指的是C标准库）攻击。恶意的程序员做的事情是修改栈来改变return的地址，指向一个他们想要的指令（这些指令被称为gadgets，和C Library相关）。

    应对方案：ASLR(address space layout randomization)。具体来说，我们把代码、栈、堆的地址空间随机化，这样就能避免被攻击。
\end{example}
\begin{definition}
    Speculative Execution：CPU会猜后面会执行的指令，并且会提前执行，那么在猜测的时候会用到cache，不管有没有猜中，cache的内容都不会被清理。
\end{definition}
\begin{example}
    第三类问题：Meltdown and Spectre（熔断和幽灵，也是很有名的攻击）

    问题声明：正因为cache的内容不会被清理，所以这导致内存的内容会很脆弱，容易受到攻击。

    应对方案：KPTI(kernel page-table isolation)：我们不会让每个process都能看到kernel的所有代码和数据结构，我们只会把最基本的放在process里面，这样做虽然能够解决一些安全问题（不能全部解决），但是会牺牲电脑的性能。因为当我们要进到kernel的时候，需要switch到kernel page table，这是花时间的。
\end{example}

\newpage
\section{Concepts Organization}
\begin{enumerate}[label=\arabic*., leftmargin=3em]
\item Demand paging: Loads pages into memory only when they are needed during execution.
\item Page fault: An event that occurs when a program accesses a page that is not currently in memory.
\item Effective access time: The average time to access memory.
\item Copy-on-write: Parent and child processes initially share the same memory pages, and a copy is made only when a write occurs.
\item Modify/dirty bit: A bit that indicates whether a page has been written
\item Reference string: A sequence of memory page numbers accessed by a process, used to evaluate page replacement algorithms.
\item Belady's anomaly: Increasing the number of page frames results in more page faults.
\item Page replacement algs: OPT, LRU, FIFO.
\item Equal allocation: A strategy that gives each process an equal number of page frames regardless of its size.
\item Proportional allocation: A strategy that allocates page frames to processes in proportion to their memory size or usage. 
\item Priority allocation: A strategy that allocates page frames based on process priority levels.
\item Global replacement: A page replacement policy where any page in memory can be replaced, regardless of which process owns it.
\item Local replacement: A page replacement policy where only pages belonging to the faulting process can be replaced.
\item Thrashing: A process is busy swapping pages in and out.
\item Working-set model: A model that defines the set of pages a process needs to keep in memory to avoid page faults.
\item Page-fault frequency: Controlling the allocation of frames to avoid thrashing.
\item Memory-Mapped Files: A technique that allows files to be mapped into virtual memory so they can be accessed like regular memory.
\item Page size issues: Fragmentation, table size, page faults, I/O overhead, resolution, locality, TLB reach, TLB size.
\end{enumerate}


\chapter{Mass-Storage Structure}
\section{In-class Contents}
\subsection{Disk Structure}
\subsubsection{Moving-head Disk Mechanism}
移动磁头磁盘机制一般由以下几个部分组成：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em] % 调整左侧缩进
\item platter：盘片
\item spindle：转轴
\item track(t)：磁道，是数据逻辑组织的最基本单位（也就是在整个组织结构中，第一个有规律划分的结构。具体来说，在进行读写操作的时候，是\textbf{磁道->柱面->扇区}；类比我要找一本书，是\textbf{书架->书架的一列->书架具体的隔间}）
\item sector(s)：扇区，是读写的最小单元
\begin{pt}
    sector和disk block有很大关联但是不一样，一个disk block由多个sector组成。
\end{pt}
\item cylinder(c)：柱面，物理意义：磁盘同时对一个cylinder进行读写操作，所以说，把文件分布在不同platter上时，读写是最快的。
\item read-write head：读写头
\item arm：磁盘臂，通过前后移动定位到不同的track。
\item arm assembly
\end{enumerate}
\subsubsection{Disk Interface}
磁盘接口包含以下四个：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
    \item Disk drives是按照一个一位数组来分配地址的，数组元素是大小为512B的logical block，\textbf{logical block是最小的transfer单位。}所以8个logical blocks就是4KB，就能和内存(disk block)对齐。

    低级格式化（Low-lebel formatting）在物理媒介上创建logical blocks。
    \item 那么这个一维数组也是顺序地映射到sector上，也就是说，1 sector = 1 logical block = 512B。
    \item 多扇区操作是可行的，许多文件系统会一次性读写4KB或更多的内容。
    \item 512B的写操作的原子操作（也就是在单个扇区上操作），所以说这样的写操作要么全部完成，要么一点都没有完成。
\end{enumerate}
\subsubsection{Disk I/O}
如何去计算一次I/O操作的时间和效率呢？

我们认为，一次磁盘的读写操作包括seek（找到正确的track）、rotation（等磁盘转到正确的sector）和transfer（读写操作）。所以定义$T_{I/O}=T_{seek}+T_{rotation}+T_{transfer}$，同时定义$R_{I/O}=\frac{Size_{Transfer}}{T_{I/O}}$，常用来比较不同的drives。

其中$Size_{Transfer}$表示对多少数据进行了读写操作。对于$T_{I/O}$来说是越小越好，同时对于$R_{I/O}$来说是越大越好。

现在呢，我们先来detail一下什么是seek time(Not important)。

seek一共有四个阶段：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
    \item 加速：disk arm加速动。
    \item 匀速(coasting)：arm全速前进。
    \item 减速：disk arm开始减速。
    \item 停止：最后read-write head要到正确的轨道上。（是最重要的一部）
\end{enumerate}

在这个过程中，加速和减速占大部分时间，不过为了简化问题，我们一般认为加速、减速、匀速都能认为是匀速阶段。

\textbf{这个内容不考。}
\begin{definition}
    Seek Time：将disk arm移动到正确轨道的时间。平均的disk-seek time大约是1/3的full seek time（遍历所有的track）。
\end{definition}

好像很奇怪，为什么不是1/2是1/3呢？
\begin{example}
    假设有track 1\textasciitilde track 10，那么full seek time为9。如果从track 1开始遍历，遍历到track $R_x$，那么确实average time为9/2。但是实际上，是从track $R_y$遍历到track $R_x$，结果大概就是1/3啦。
\end{example}

\begin{definition}
    Rotational delay：等待我要的sector转到disk head底下，平均的rotational delay是full rotational delay的一半。
\end{definition}

\textbf{Rotation 和 seek（不包括transfer）是disk operations里最耗时间的。}

\begin{example}
    我们现在可以解决一个之前没有说得很明白的问题，为什么I/O更喜欢large page size呢？

    我们以40KB的page和4KB的page举一个例子来对比，这两种的rotation和seek的时间是不会变的，但是40KB的page的传输量要增加，也就是说，它的transfer time要增加，听上去不那么好？那么再考虑一下如果这40KB包含了2个要读写的4KB，那么访问时间会是什么样的呢？

    对于40KB的那个，只需要1 seek + 1 rotation + transfer(40KB)。

    对于两个4KB，则需要2 seek + 2 rotation + transfer(8KB)。

    但是我们之前又说，transfer其实不怎么占时间，所以综合来看40KB的I/O所花的时间是要少很多的。
\end{example}
\subsubsection{Workload Assumptions}
我们现在有两种workload，一种是Random workload,就是每次随机访问disk上的4KB（应用场景：类似数据库的频繁随机访问）；另一种是Sequential workload，会连续读取一系列的sectors（应用场景：读取类似视频的大量扇区数据）。

在这之后，我们对两种disks进行比较，一种是Barracuda，其特点是Average Seek较长，但是Cache大（常作个人电脑、家庭存储，更适用Sequential workload）。另一种是IronWolf，它的特点和Barracuda相反，Average Seek更快，但是Cache较小（常作为服务器，更适用Random workload）。

我们再讨论一下在Sequential workload的情况下，$T_{I/O}=T_{transfer}$，因为这个过程当中是省略掉seek和rotation的，head不需要重新定位，所以sequential workload的表现会比random workload的表现好得多。

\begin{definition}
    Track skew（磁道倾斜）：具体见PPT15页，核心思想是偏移几个block，让head从外圈切进内圈的时间和rotate的时间可以相同，让顺序读取更加流畅。
\end{definition}
\subsubsection{Some Other Details}
\begin{definition}
    Multi-zoned disk drives：在磁盘旋转时，外圈的磁道线速度要比内圈磁道线速度快（在const angular velocity,CAV的情况下），所以外圈的磁道可以容纳更多的sector，内圈的相对少一点。但是呢，每个cylinder的区域的sector数量还是要统一的。

    当然，现在也有了constant angular velocity(CAV)和constant linear velocity(CLV)两种形式。
\end{definition}
\begin{definition}
    Cache(track buffer)：内存有一块小的地方，通常大概是64MB或者256MB，这些地方可以被drive用来暂时存储被读和被写的data。
\end{definition}
\begin{example}
    一般来说，Random workload要的Cache少，Sequential workload要的Cache大一些。
\end{example}
\subsection{Disk Scheduling}
Operating system负责更有效地使用硬件，那么对于disk drives来说，这意味着能有更快的access time和disk bandwidth（硬盘带宽）。

那么我们怎么做到最快完成这个任务呢？也就是minimize $T_{I/O}$？其实如果我们给定了一个disk，那么I/O操作的rotation delay和transfer time都是固定的，只有seek time是会变化的，所以我们的目标就变成了minimize seek time。

我们再做一个假设，我们不考虑seek过程的加速减速变化，认为他们都是匀速的，所以seek time可以用seek distance来代替表示。

\begin{definition}
    Disk bandwidth=传输的总字节数/完成I/O操作的总时间
\end{definition}

Disk的I/O request可能来自以下几个方面：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
    \item OS
    \item System processes
    \item Users processes
\end{enumerate}

这里想要表达的意思是，磁盘访问请求不只是来自用户，系统本身也会频繁进行I/O操作。

I/O请求包括输入或输出模式、磁盘地址、内存地址和需要transfer的sector数量。OS做的事情是为每个disk或者device管理请求队列。

\begin{pt}
空闲的disk可以直接完成I/O请求，只有忙的disk才会有等待队列。也就是说，优化算法只有太多请求排队的时候才有意义。
\end{pt}
\subsubsection{First Come First Serve(FCFS)}
核心思想：很简单，根据request queue，那个先来先seek哪个就行。
\subsubsection{Shortest Seek Time First}
核心思想：还是很简单，对于当前节点来说，哪个近就访问哪个。

But!有一个问题，可能会出现starvation，你一直在0\textasciitilde50访问，那么100多，200多的就饿死了。
\subsubsection{SCAN Algorithm(the elevator algorithm)}
核心思想：我们先定一个方向，一条路走到黑（也就是走到边界），然后返回回来把路径上的request都完成。

\begin{pt}
    别名电梯算法要知道，然后需要知道一开始的seek方向。
\end{pt}

\subsubsection{C-SCAN Algorithm(Circular-SCAN)}
核心思想：在SCAN的基础上，先一条路走到黑，再走到另一个头（中间就算有request也不管），然后再回头seek所有的request。

\begin{pt}
    记得跨越两个边界的那条路要算上。
\end{pt}

这么做有什么物理意义吗？有的兄弟，有的。
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
    \item 跨越两个边界的那条路实际上是加速再减速的过程，所以说不会太耗时间。
    \item C-SCAN相比SCAN的响应更加平滑，新来的request也能更快被响应。(provide a more uniform wait time than SCAN)
\end{enumerate}
\subsubsection{LOOK \& C-LOOK}
核心思想：两个边界设置为queue里面最大的和最小的request就行了，不用一条路走到黑。

\subsubsection{Other Issues}
\begin{pt}
    有个问题，两个SCAN算法和LOOK算法并不是最好的算法，因为他们只优化了seek time，完全忽略了rotation delay。
\end{pt}
\begin{definition}
    Short Positioning Time First(SPTF)：positioning time包括了seek + rotation，这才是真正最优的（当然和disk drive本身的实现有关，因为涉及rotation）。
\end{definition}
\begin{exam}
    操作系统把requests打包给disk drive，让disk drive完成调度，所以对于OS来说，它是没什么工作的。
\end{exam}
所以说，SSTF(or SPTF)确实是最常见的算法，也很有吸引力，但问题是会有饥饿的问题。那么对于SCAN和C-SCAN来说，他们更加擅长解决有heavy load的情况，因为他们不太会有饥饿的问题（虽然一般不会有很重的I/O）。

还有一个小问题，谁来完成这个调度呢？

在老的系统当中，OS完成全部的调度。但是在现代系统当中，OS只会做I/O的merging，也就是把requests合并起来，而由disk drive来做例如SPTF这样的调度。

\subsection{RAID Structure}
RAID在生活当中是很有用的。\textbf{（但是考试只考基本的概念）}
\begin{definition}
    RAID：一种技术去将几个磁盘搞在一起来构成一个更快、更大、更可靠的磁盘系统。
\end{definition}

从外部来看，RAID就像是“1”个disk，里面是一系列可供读写的block，这种透明性增加了RAID的可部署性。从内部看过去，RAID由许多disks，内存（包括易失性的，也有非易失性的）以及一个或更多的处理器来管理这个系统。

那么为什么我们用RAID而不是用单独的一个Disk呢？
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
    \item Perfoemance：几个disk并行读写，可以大幅加速I/O操作。
    \item Capacity：打个比方5000RMB可能可以买到5张4GB的disk，但是买不到1张20GB的disk。
    \item Reliability：在disk system中添加一些冗余，这样RAIDs可以接收磁盘内容的丢失并继续运行。
\end{enumerate}
\subsubsection{RAID Level 0}
特点：条带化，完全没有reduntant（所以本质并不是一个RAID，更多作为一个baseline）

RAID-0是performance和capacity的上限，但是如果一个磁盘坏了，那么整个RADI-0的数据就木的了。

我们来对RAID-0做一个分析：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
    \item Perfoemance：Perfect，并行读写利用到了所有的disks。
    \item Capacity：Perfect，存储效率为100\%，假如有N个disk，每个disk能存储B个数据块，那么capacity为N×B。
    \item Reliability：没有冗余，可靠性较差，反而比单个disk更加糟糕。
\end{enumerate}

现在我们来分析一下RAID-0的performance：
\begin{example}
    Single-request latency（单次请求延时）：如果说我每次只访问一个block，那对不起，这种情况下的访问速度和single disk没啥区别，performance没有明显提升。
\end{example}
\begin{example}
    Steady-state throughput（持续吞吐量）：But！如果说我们有N个disk，同时是sequential workload，传输速率为S MB/s，那么RAID的总传输速率就是N×S MB/S。如果我们用的是random workload，传输速率为R MB/S（R一般远小于S），这时候RAID的总传输速率就是N×R MB/S。
\end{example}

还有一个变体，按照默认的说法，stripe是一行一行这么来的，这里可以改变striping的宽度，如果宽度为2，就是每次分配两个数据块给每个disk。

\begin{definition}
    Chunk size：表示striping的宽度。
\end{definition}

chunk size小的话，可以增加一个文件读写的并发程度，但是代价就是在每个disk上都会需要做positioning，那么这对于整个读写系统来说可能更花时间。

\subsubsection{RAID Level 1}
这是最简单的RAID，核心思想是\textbf{"Mirroring"}，也就是说我们会在一个磁盘系统中把每个block都拷贝一下，同时每份拷贝都会被安排在不一样的disk，来应对disk failure的问题（增加reliable）。

一种常见的分配方式是RAID-10(RAID 1+0)，它做的事情就是先镜像，再让他们按照stripes去排列，也就是先做RAID-1，再做RAID-0。当然对应的也有RAID-01(RAID 0+1),其物理盘上的数据和RAID-10是一样的，但是从表现来说，RAID 1+0会更好一些，具体看PPT的解析。

同样的，我们对RAID-1进行一个分析：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
    \item Perfoemance：见后文，\textbf{不考}。
    \item Capacity：比RAID-0贵很多，因为我们相当于只用了一半的capacity。
    \item Reliability：还是不错的，可以最多接受N/2个disk failure（在RAID 1+0的情况下，要是一个disk failure了，可以去镜像盘里找）。
\end{enumerate}

\textbf{以下内容不考！不考！不考！}
\begin{example}
    Single-request latency：Read和在一个disk上是一样的，但是Write的时候要注意虽然是并行地写，但是写的时间取决于两个里面时间更长的那个。
\end{example}
\begin{example}
    Steady-state throughput：如果是Sequential，读和写都是N×S/2 MB/s，如果安排好的话理论上是能达到N×S的。Random情况下读是N×R MB/s，写是N×R/2 MB/s。
\end{example}

\subsubsection{RAID Level 4}
我们发现RAID-1好像是可以解决reliable的问题，但是他需要的空闲位太多了，所以RAID-4提出的方法就是在RAID-0的基础上，加一个校验位(parity block)，然后校验位的计算由其余非校验位通过异或运算得到。
\begin{center}
\normalsize
\begin{tabular}{c c c c c}
Disk 0&Disk 1&Disk 2&Disk 3&Disk 4\\
\hline
0&1&2&3&P0\\
4&5&6&7&P1\\
8&9&10&11&P2\\
12&13&14&15&P3
\end{tabular}
\end{center}

同样的，我们对RAID-4做一个分析：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
    \item Perfoemance：见后文。
    \item Capacity：N-1，因为有一个作为校验位了。
    \item Reliability：最多可以承受一个disk failure。
\end{enumerate}

现在再来讨论一下performance：
\begin{example}
    Single-request latency：Read和single disk一样；write是single disk的两倍（因为每次写不仅要改变当前位置的数据，还要改变校验位）。
\end{example}
\begin{example}
    Steady-state throughput：Sequential workload的读和写都是(N-1)×S MB/s；Random workload的读是(N-1)×R MB/s，写是R/2 MB/s，因为每次写只有两个disk可以并行操作，同时要两次读两次写，所以是R/2。
\end{example}

对于random writes，其实有两种方法对校验位进行计算，一种是additive parity，也就是不看原来的parity，更新了数据之后重新计算parity，那这个显然是没那么高效的。另一种方法是Subtractive parity，具体来说是只需要用被修改的block和parity就可以进行操作了，一共需要两次读和两次写。

RAID-4有一个问题，就是每次并行操作的只能有两个disk，假设disk 4里面都是parity，那么disk 4会忙死，这样的效率是比较低的。

\subsubsection{RAID Level 5}
这个时候，我们就想到了RAID-5，他和RAID-4也很像，做的事情就是让每个disk都有一个block作为parity block。
\begin{center}
\normalsize
\begin{tabular}{c c c c c}
Disk 0&Disk 1&Disk 2&Disk 3&Disk 4\\
\hline
0&1&2&3&\textcolor{blue}{P0}\\
5&6&7&\textcolor{blue}{P1}&4\\
10&11&\textcolor{blue}{P2}&8&9\\
15&\textcolor{blue}{P3}&12&13&14\\
\textcolor{blue}{P4}&16&17&18&19
\end{tabular}
\end{center}

同样的，对RAID-5进行分析\textbf{（啊，要疯掉了，这么多）}，Capacity和Reliability和RAID-4是一模一样的，现在来看performance：
\begin{example}
    Single-request latency：还是和RAID-4一模一样。
\end{example}
\begin{example}
    Steady-state throughput：Sequential workload，和RAID-4一样，读和写都为(N-1)×S MB/s。Random情况就不一样了：

    Read：N×R MB/s，因为pairty是均匀分布的，所以有的时候一个disk在工作，也有的时候这个disk充当parity，所以是N不是N-1。

    Write：(N/4)×R MB/s，除以4是因为每个写操作的背后都是4个读写，同时这四个读写是可以一起并行的。
\end{example}

\subsubsection{Conclusion}
\begin{tabular}{l c c c c}
& RAID-0 & RAID-1 & RAID-4 & RAID-5 \\
\hline
Capacity&$N\times B$&$N\times B/2$&$(N-1)\times B$&$(N-1)\times B$\\
\hline
Reliability&0&1 (for sure)&1&1\\
&&$N/2$ (if lucky)&&\\
\hline
Throughput&&&&\\
\quad Sequential Read&$N\times S$&$(N/2)\times S$&$(N-1)\times S$&$(N-1)\times S$\\
\quad Sequential Write&$N\times S$&$(N/2)\times S$&$(N-1)\times S$&$(N-1)\times S$\\
\quad Random Read&$N\times R$&$N\times R$&$(N-1)\times R$&$N\times R$\\
\quad Random Write&$N\times R$&$(N/2)\times R$&$(1/2)\times R$&$(N/4)\times R$\\
Latency&&&&\\
\quad Read&T&T&T&T\\
\quad Write&T&T&2T&2T\\
\end{tabular}

\textbf{上面内容都不考喔！}
\chapter{File-System Interface}
\section{In-class Contents}
\subsection{Files and Directories}
我们到现在学过了很多abstractions，包括process、thread、page等，这里再额外介绍两个文件系统用到的：
\begin{definition}
    File：是一个线性字节数组，可供读写，每个文件都有一个low-level name，被称为inode number（索引节点编号，int类型数据，不是file id）（类似process有pid，thread有tid），这样OS就能知道你是哪根葱了。
\end{definition}
\begin{definition}
    Directory（目录）：包含一个列表的entries，每个entry是一个pair，pair的内容是用户可读的名称和一个low-level name（类似page table做的translation:logical->physical），可以指向一个文件，也可以指向其他目录（子目录），所以说每个目录也是会有inode number的。
\end{definition}
\begin{definition}
    Directory tree(hierarchy)：在目录层级结构中，从根目录开始，用分隔符（一般为"/"）将父目录和子目录分开，直到找到我要的文件。
\end{definition}
\begin{example}
    home/user/hjm/pressure.doc
\end{example}

这样的话一个文件就可以通过它的绝对路径被访问到。所以说File system提供了一个非常好的东西就是他的命名很方便，用户容易理解。

那么对于一个文件来说，通常是x.y的格式，前面是文件的名字，后面\textbf{通常}是它的扩展名，比如：rgl.ppt。

\begin{pt}
    当然这只是一个约定俗成，应由用户决定怎么打开这些文件，main.c文件也不一定真的是C source code，不过修改扩展名一般会出现“改变扩展名可能导致文件无法正常打开”的提示。
\end{pt}

\subsection{File Interface}
本质就是一些System calls，那么我们一点一点来分析和认识。
\subsubsection{Creating Files}
首先在创建文件的时候，会用到open()这个system call。
\vspace{6pt}
\begin{lstlisting}[language=C]
int fd = open("foo", O_CREAT | O_WRONLY | O_TRUNC);
\end{lstlisting}

这里做的事情其实很简单，就是在当前工作目录中创建一个名为“foo”的文件，O\_CREAT指的是我要创建一个文件，O\_WRONLY指的是这是个只能写的文件，当然也可以设置为read only，O\_TRUNC指的是如果这个文件已经存在了，那就会给它截断(Truncate)，把里面的东西全部清空，当然也可以设置为在现有文件的基础上进行编辑。最后，open()会返回一个int类型的数据fd，也就是file descriptor，是一个low-level name，一般会被read()和write()的syscall调用到。
\begin{pt}
    所有文件在进程里都表现为file descriptor。
\end{pt}

\subsubsection{Reading Files}
这里我们举一个例子，来读一个文件中的内容：
    \vspace{6pt}
    \begin{lstlisting}[language=sh]
prompt> echo hello > foo
prompt> cat foo
hello
prompt>
\end{lstlisting}

这里做的事情就是将hello写在一个名为foo的文件当中，然后读取foo的内容并且打印出来。

现在我们来仔细分析一下cat：
\vspace{6pt}
\begin{lstlisting}[language=sh]
prompt> strace cat foo
(*@$\cdots$@*)
# 打开foo这个文件，这里foo是一个只读文件，同时支持大文件的读。
open ("foo", O_RDONLY | O_LARGEFILE)	= 3
# 那么为什么返回的fd是3呢？因为0代表stdin，1代表stdout，2代表stderr。
# 现在开始读文件
read(3, "hello\n", 4096)		= 6
# read里面有三个参数，第一个参数指的是读取的文件的fd；第二个参数是一个buffer，里面存放的是读取的内容，这里是hello和一个换行符；第三个参数是buffer的大小，设置的是4096B，也就是4KB，那就是page size，为什么这么设置？因为方便swap in和out
# read的返回值是读取的数据的大小，这里hello加换行符一共是6个Byte。
# 现在我们把读取的内容写到控制台当中
write(1, "hello\n", 6)			= 6
# 第一个参数1表示stdout，这样我们才能把写的东西放到控制台中，返回的6同样表示内容的大小。
# 控制台输出hello
hello
# 因为我们一次性只读了4KB，还要进行下一次读写，发现buffer里面没有东西了，那说明我们已经读完了，就可以终止操作了。
read(3, "", 4096)			= 0
close(3)				= 0
(*@$\cdots$@*)
prompt>
\end{lstlisting}

其实，说白了cat用到的syscall也就read、write、open、close。

\subsubsection{Write Files}
写文件和读文件差不多，唯一不同的就是和stdout没啥关系了。

\subsubsection{Reading/Writing NOT Sequentially}
现在我想实现一个功能，我的读写不再从头开始，而是说能够访问文件中的特定位置并进行读写。

这时候就要请出我们的lseek()这个Syscall了：
\vspace{6pt}
\begin{lstlisting}[language=C]
off_t lseek(int fildes, off_t offset, int whence);
\end{lstlisting}

第一个参数指的是file descriptor，也就是我们要操作的文件；第二个参数是偏移量offset，可以是正的可以是负的；第三个参数表示lseek怎么去perform，具体分为以下三种：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
    \item SEEK\_SET：从头开始，加上一个偏移量
    \item SEEK\_CUR：从当前位置开始，加上一个偏移量
    \item SEEK\_END：从文件末尾开始，加上一个偏移量，那这里的偏移量大概率就是负的了
\end{enumerate}

我们注意到SEEK\_CUR是从当前位置开始，那么这个当前位置的更新又有两种方式：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
    \item 隐式更新：前面提到的read()和write()Syscall，假设读/写了N个bytes，那当前的位置就是上一个位置＋N（那么一般性是4KB的读和写）。
    \item 显示更新：就像lseek说的那样。
\end{enumerate}

\subsubsection{Writing Immediately}
大部分情况下呢，我们要把数据写回disk并不是立刻执行的，write请求什么时候响应取决于disk，对于file system来说，他做的事情就是先把要写入的数据缓存在内存当中，把几个write()操作合并起来，等时机合适了再把它们全部写回disk。

那么，万一我想立刻写回去，我不想等怎么办？

别急，我们有一个Syscall叫做fsync()(file synchronize)。它可以强制把当前文件指向的文件中的所有数据立刻写回去，就相当于写word文档的时候ctrl+S保存一下。

\begin{pt}
在创建一个新文件的时候，注意要确保这个新文件是在目录里面的，这时候也要调用fsync()。
\end{pt}

那么问题来了，万一我有些数据没有写入磁盘，电脑寄了，怎么办？

\subsubsection{Renaming Files}
我知道你很急，但是你先别急，我们先来讲讲rename这个syscall，它是一个原子操作，可以改一个文件的名字，原子操作有什么好处呢？要么就改名成功，要么就失败，不会出现我断电了，都变成乱码了。

所以我们依据这个特性，可以用这串代码来解决断电数据丢失的问题：
\vspace{6pt}
\begin{lstlisting}[language=C]
// 创建一个叫foo.txt.tmp的临时文件，当然本来也有一个foo.txt文件的
int fd = open("foo.txt.tmp", O_WRONLY | O_CREAT | O_TRUNC);
write(fd, buffer, size);
fsync(fd);
close(fd);
// 运行到这里崩溃是没事的，东西至少还在foo.txt.tmp里面，但是之后记得要去恢复或者改名。
rename("foo.txt.tmp", "foo.txt");
// 完成保存操作
\end{lstlisting}

\begin{example}
    我在写word文档，可以发现如果没有保存的话，隐藏的项目里面会有个副本，一旦我点了保存，那个副本就不见了，那个副本就是临时文件。
\end{example}

\subsubsection{Geting Information about Files}
File system除了保存着文件的内容，还会保存关于文件本身的一些信息，这些信息被称为元数据(metadata)，而这些信息会被存在inode里面（和pcb很像，被称为file control block）。

如果说我们要看这些metadata，可以用stat()或者fstat()Syscall。

\subsubsection{Permission Bits}
我们用9个bits来表示一个文件能够被谁访问以及能够被如何访问，看下面的例子：
\vspace{6pt}
\begin{lstlisting}[language=sh]
prompt> echo hello > file
prompt> ls -l foo.txt
-rw-r--r-- 1 kai kaigroup 0 Jul 13 16:29 foo.txt
# kai为文档的作者，rw-表示kai有读写权限；r--r--表示kaigroup和其他用户只有读的权限；1表示硬链接数；0表示文件大小，Jul 13 16：29表示最后修改的时间；最后一个是文档的名字
\end{lstlisting}

那么如何修改权限呢？
\vspace{6pt}
\begin{lstlisting}[language=sh]
prompt> chmod 777 test.sh
\end{lstlisting}

首先我们看一个7，7=4（读）+2（写）+1（执行）；那么三个777，就对应的是kai的权限、kaigroup的权限和others的权限。所以这个代码的作用就是让所有人有所有的权限。

\subsubsection{Removing Files}
当我们删除一个文件的时候，我们调用rm()这个UNIX中的指令，\textbf{不是一个Syscall}！它的作用是调用一个unlink()的Syscall，把我要删除的文件的名字给扔掉。那么为什么是这么做呢？我们这里留一个伏笔。

\subsection{Directory Interface}
\subsubsection{Making Directories}
在创建目录的时候，我们调用mkdir()Syscall：
\vspace{6pt}
\begin{lstlisting}[language=sh]
prompt> strace mkdir foo
(*@$\cdots$@*)
# 这里做的事情是创建foo目录，并且设置权限为0777，也就是所有人都能rwe。
mkdir("foo", 0777)			= 0
(*@$\cdots$@*)
prompt> 
\end{lstlisting}

在目录创建好之后，并不完全是空的，会默认有一个"."（表示当前目录的引用）和一个".."（表示父目录，也就是上一级目录的引用）

\subsubsection{Reading Directories}
简单来说，就是用ls()Syscall展示一堆东西。

\subsubsection{Deleting Directories}
调用rmdir()Syscall，把目录下的所有文件都删除之后，再把这个目录删除（所有OS都是这么干的）。

\subsection{Links}
\subsubsection{Hard Links}
link()Syscall需要两个参数，一个是旧的路径名字，一个是新的，即link(oldpath, newpath)，这样可以多创建一个entry，指向同一个inode，也就是说，文件并没有被复制，只是增加了一个新名字来引用相同的内容（很像浅拷贝）。

同样的，如果删了其中一个名字，只要另一个名字还在，内容就不会被删掉；如果改其中一个文件的内容，另一个也会改变，因为它们指向的是一样的文件内容。

现在我们来看一个例子：
\vspace{6pt}
\begin{lstlisting}[language=sh]
prompt> echo hello > file
prompt> cat file
hello
prompt> ln file file2
prompt> cat file2
hello
prompt> ls -i file file2
67158084 file
67158084 file2
prompt> 
\end{lstlisting}

下面我们来解释为什么我们在删除的时候用的是unlink()而不是remove()或者delete()。

在创建一个文档的时候，我们首先会创建一个inode，里面有这个文件的内容，包括大小、blocks、\textbf{链接计数(reference count)}（来跟踪有多少个文件名指向这个inode）；其次，我们会链接一个面向用户的human-readable名字在目录下。

那么当我们unlink一个文档的时候，file system会首先检查reference count是不是0，如果不是0，那没事，如果是0，那很抱歉，兄弟你被“销户”了，文件被彻底删除掉。

\textbf{所以这也就解释了为什么所谓的“文档删除”是unlink()，而不是remove()，也不是delete()。}

\subsubsection{Symbolic Links}
符号链接，也就是Soft Links。

Hard links是有一些限制的，因为我们没办法为一个目录创建hard link，比方说，我们首先创建了一个dir，又在dir这个目录下创建了一个link，硬链接到dir，这样就循环了，就坏了。

不仅如此，由于每个文件系统的inode都是独立的，所以说，我们没办法将链接链接到别的disk部分->找一个文件首先要找它所属的文件系统。

这时候，我们就引入了Symbolic Link，见以下代码：
\vspace{6pt}
\begin{lstlisting}[language=sh]
prompt> echo hello > file
prompt> ln -s file file2
prompt> cat file2
hello
prompt> 
\end{lstlisting}

这个时候呢，file2就指向了file，而不是像硬链接那样指向file对应的inode。

Symbolic link的本质是一个文件，它有自己的inode，是独立于原文件的，就比如：\textbf{快捷方式}。

当我们运行ls的时候：
\vspace{6pt}
\begin{lstlisting}[language=sh]
prompt> ls -al
drwxr-x--- 2 kai kai 6 Jul 13 19:10 ./
drwxr-x--- 26 kai kai 4096 Jul 13 16:17 ../
-rw-r----- 1 kai kai 6 Jul 1319:10 file
lrwxrwxrwx 1 kai kai 4 Jul 1319:10 file2 -> file
# 我们来看这四行的最前面一个字母，"-"表示regular files，"d"表示directories，"l"表示soft links。在这个例子当中，symbolic link的大小是4B。
\end{lstlisting}

\begin{definition}
    Dangling Reference（悬空引用）：如果我们把file2指向的file给删了，就像我有一个游戏本体和快捷方式，我们把本体删了，快捷方式虽然还在，但是它指向不了任何东西。不过，如果我们删快捷方式，对游戏本体是没有影响的。
\end{definition}
\subsection{File System Interface}
最后我们来提一下怎么创建file system，在这里我们用的是mkfs()指令：
\vspace{6pt}
\begin{lstlisting}[language=sh]
prompt> mkfs -t ext3 /dev/sda1
# 创建一个ext3类型的文件系统，并将其写入/dev/sda1设备（一个磁盘分区）
\end{lstlisting}

\begin{definition}
    Mount（挂载）：一种命令，将现有目录作为目标装载点，并在该点将新的文件系统粘贴到目录树上。
\end{definition}
\vspace{6pt}
\begin{lstlisting}[language=sh]
prompt> mount -t ext3 /dev/sda1 /home/users
# 任何访问/home/users的操作都会访问/dev/sda1上的内容，不再是原本的/home/users的内容，直到被卸载。
\end{lstlisting}

\chapter{File-System Implementation}
\section{In-class Contents}
\subsection{Warm-up}
那当我们知道了有哪些interface之后，我们要开始implement。那么既然要implement，那肯定还要考虑performance。

我们首先了解一下File system的特征，可以总结为：小文件多，但平均文件大小一直在增加；一小部分的大文件占据了绝大部分的空间；系统里会有成千上万个文件；即使你扩容了磁盘容量，也会有大约50\%的使用率；目录不会有太多。

这时候我们来看一个例子：
\begin{table}[h]
\footnotesize
    \centering
    \begin{tabular}{ccc|cc}
        Filename & Content & Description & \textbf{Size} & \textbf{Space} \\
        \hline
        test1 & - & - & 0 & 0 \\
        test2 &  ``This is a test file.$\backslash$r$\backslash$n'' & $\times 1$ (line) & 22 bytes & 0 \\
        test3 &  ``This is a test file.$\backslash$r$\backslash$n'' & $\times 40$ (lines) & 878 bytes & 4 KB \\
        test4 &  ``This is a test file.$\backslash$r$\backslash$n'' & $\times (40-39)$ (lines) & 22 bytes & 4 KB
    \end{tabular}
\end{table}

我们来验证一下：
\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.35\textwidth}
    \includegraphics[width=\linewidth]{test1.png}
    \caption{test1}
  \end{subfigure}
  \begin{subfigure}[t]{0.35\textwidth}
    \includegraphics[width=\linewidth]{test2.png}
    \caption{test2}
  \end{subfigure}
  \begin{subfigure}[t]{0.35\textwidth}
    \includegraphics[width=\linewidth]{test3.png}
    \caption{test3}
  \end{subfigure}
  \begin{subfigure}[t]{0.35\textwidth}
    \includegraphics[width=\linewidth]{test4.png}
    \caption{test4}
  \end{subfigure}
\end{figure}

诶？好像还真是这样，那为什么test2的空间是0，但是test4的空间是4KB呢？

在这个章节中，我们会描述实现本地文件系统和目录结构的细节；描述远程文件系统的实施；讨论块分配和自由块算法以及权衡；最后讲一些固态盘上的文件系统。

\subsection{Typical File System}
\subsubsection{Very Simple File System}
我们先讲一个VSFS，是UNIX file system的简单版本，我们需要理解的有：Data structures（用了什么数据结构来管理数据和元数据）和Access methods（进程发出的syscall如何映射到底层的数据结构）。

我们来举一个例子，我们找了一个disk，有64个块，每个块的大小都是4KB：

\begin{figure}[H]
\begin{center}
\tiny
\tikz{
    \path
    (0,0) node {32}
    (2.8,0) node {39}
    (3.4,0) node {40}
    (6.2,0) node {47}
    (6.8,0) node {48}
    (9.6,0) node {55}
    (10.2,0) node {56}
    (13,0) node {63};

    \foreach \j in {0,1,2,...,7}
    {
       \draw (0+\j*0.4,0.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (3.4+\j*0.4,0.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (6.8+\j*0.4,0.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (10.2+\j*0.4,0.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (0+\j*0.4,0.5) node {\texttt{}};
       \draw (3.4+\j*0.4,0.5) node {\texttt{}};
       \draw (6.8+\j*0.4,0.5) node {\texttt{}};
       \draw (10.2+\j*0.4,0.5) node {\texttt{}};
    }
    
    \path
    (0,1) node {0}
    (2.8,1) node {7}
    (3.4,1) node {8}
    (6.2,1) node {15}
    (6.8,1) node {16}
    (9.6,1) node {23}
    (10.2,1) node {24}
    (13,1) node {31};
    \foreach \j in {0,1,2,...,7}
    {
       \draw (0+\j*0.4,1.5) node [minimum height=0.4cm,minimum width=0.4cm,draw] {};
       \draw (3.4+\j*0.4,1.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (6.8+\j*0.4,1.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (10.2+\j*0.4,1.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (0+\j*0.4,1.5) node {};
       \draw (3.4+\j*0.4,1.5) node {\texttt{}};
       \draw (6.8+\j*0.4,1.5) node {\texttt{}};
       \draw (10.2+\j*0.4,1.5) node {\texttt{}};
    }
    
    \draw (0,1.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=white] {};
    \draw (0.4,1.5) node [minimum height=0.4cm,minimum width=0.4cm,draw] {};
    \draw (0.8,1.5) node [minimum height=0.4cm,minimum width=0.4cm,draw] {};
    \draw (0,1.5) node {};
    \draw (0.4,1.5) node {};
    \draw (0.8,1.5) node {};
}
\end{center}
\end{figure}

我们把最后56个blocks用来放data，标记为D，而前面的8个blocks用来放各种data structures，这也就说明了为什么我买了一个T的盘，但是只有900G可以用：
\begin{figure}[H]
\centering
\tiny
\tikz{
    \path
    (0,0) node {32}
    (2.8,0) node {39}
    (3.4,0) node {40}
    (6.2,0) node {47}
    (6.8,0) node {48}
    (9.6,0) node {55}
    (10.2,0) node {56}
    (13,0) node {63};

    \foreach \j in {0,1,2,...,7}
    {
       \draw (0+\j*0.4,0.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (3.4+\j*0.4,0.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (6.8+\j*0.4,0.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (10.2+\j*0.4,0.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (0+\j*0.4,0.5) node {\texttt{D}};
       \draw (3.4+\j*0.4,0.5) node {\texttt{D}};
       \draw (6.8+\j*0.4,0.5) node {\texttt{D}};
       \draw (10.2+\j*0.4,0.5) node {\texttt{D}};
    }
    \draw [|-|] (-0.2,1)--node[above=0] {Data Region} (13.2,1);
    
    \path
    (0,2) node {0}
    (2.8,2) node {7}
    (3.4,2) node {8}
    (6.2,2) node {15}
    (6.8,2) node {16}
    (9.6,2) node {23}
    (10.2,2) node {24}
    (13,2) node {31};
    \foreach \j in {0,1,2,...,7}
    {
       \draw (0+\j*0.4,2.5) node [minimum height=0.4cm,minimum width=0.4cm,draw] {};
       \draw (3.4+\j*0.4,2.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (6.8+\j*0.4,2.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (10.2+\j*0.4,2.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (0+\j*0.4,2.5) node {};
       \draw (3.4+\j*0.4,2.5) node {\texttt{D}};
       \draw (6.8+\j*0.4,2.5) node {\texttt{D}};
       \draw (10.2+\j*0.4,2.5) node {\texttt{D}};
    }
    
    \draw (0,2.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=white] {};
    \draw (0.4,2.5) node [minimum height=0.4cm,minimum width=0.4cm,draw] {};
    \draw (0.8,2.5) node [minimum height=0.4cm,minimum width=0.4cm,draw] {};
    \draw (0,2.5) node {};
    \draw (0.4,2.5) node {};
    \draw (0.8,2.5) node {};
    
    \draw [|-|] (3.2,3)--node[above=0] {Data Region} (13.2,3);
}
\end{figure}

现在我们聚焦前面8个blocks，回顾一下第十一章，想想我们要放些什么数据结构。

首先要放的是inodes，inodes组成了inode table，存在了前面8个blocks的后面五个：
\begin{figure}[H]
\centering
\tiny
\tikz {
    \path
    (0,0) node {32}
    (2.8,0) node {39}
    (3.4,0) node {40}
    (6.2,0) node {47}
    (6.8,0) node {48}
    (9.6,0) node {55}
    (10.2,0) node {56}
    (13,0) node {63};

    \foreach \j in {0,1,2,...,7}
    {
       \draw (0+\j*0.4,0.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (3.4+\j*0.4,0.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (6.8+\j*0.4,0.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (10.2+\j*0.4,0.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (0+\j*0.4,0.5) node {\texttt{D}};
       \draw (3.4+\j*0.4,0.5) node {\texttt{D}};
       \draw (6.8+\j*0.4,0.5) node {\texttt{D}};
       \draw (10.2+\j*0.4,0.5) node {\texttt{D}};
    }
    \draw [|-|] (-0.2,1)--node[above=0] {Data Region} (13.2,1);
    
    \path
    (0,2) node {0}
    (2.8,2) node {7}
    (3.4,2) node {8}
    (6.2,2) node {15}
    (6.8,2) node {16}
    (9.6,2) node {23}
    (10.2,2) node {24}
    (13,2) node {31};
    \foreach \j in {0,1,2,...,7}
    {
       \draw (0+\j*0.4,2.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=red!20] {};
       \draw (3.4+\j*0.4,2.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (6.8+\j*0.4,2.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (10.2+\j*0.4,2.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (0+\j*0.4,2.5) node {I};
       \draw (3.4+\j*0.4,2.5) node {\texttt{D}};
       \draw (6.8+\j*0.4,2.5) node {\texttt{D}};
       \draw (10.2+\j*0.4,2.5) node {\texttt{D}};
    }
    
    \draw (0,2.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=white] {};
    \draw (0.4,2.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=white] {};
    \draw (0.8,2.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=white] {};
    \draw (0,2.5) node {};
    \draw (0.4,2.5) node {};
    \draw (0.8,2.5) node {};
    
    \draw [|-|] (1,3)--node[above=.5mm] {Inodes} (3,3);
    \draw [|-|] (3.2,3)--node[above=0] {Data Region} (13.2,3);
}
\end{figure}

现在我们再假设一个inode要256B，那么5个4KB的blocks一共能够存5×4×1024/256=80个inodes，这也就代表了这个文件系统最多可以有80个文件（或者说是有80个有inode的东西）。

现在我们又有一个问题了，我们怎么知道哪个inode是空的或者哪个data block是空的呢？

我们引入了inode bitmap和data bitmap，放在整个系统的1号block和2号block，它们可以知道一个inode有没有被分配或者一个data block有没有被分配，这样的数据结构可以是一个free list。
\begin{figure}[H]
\centering
\tiny
\tikz  {
    \path
    (0,0) node {32}
    (2.8,0) node {39}
    (3.4,0) node {40}
    (6.2,0) node {47}
    (6.8,0) node {48}
    (9.6,0) node {55}
    (10.2,0) node {56}
    (13,0) node {63};

    \foreach \j in {0,1,2,...,7}
    {
       \draw (0+\j*0.4,0.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (3.4+\j*0.4,0.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (6.8+\j*0.4,0.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (10.2+\j*0.4,0.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (0+\j*0.4,0.5) node {\texttt{D}};
       \draw (3.4+\j*0.4,0.5) node {\texttt{D}};
       \draw (6.8+\j*0.4,0.5) node {\texttt{D}};
       \draw (10.2+\j*0.4,0.5) node {\texttt{D}};
    }
    \draw [|-|] (-0.2,1)--node[above=0] {Data Region} (13.2,1);
    
    \path
    (0,2) node {0}
    (2.8,2) node {7}
    (3.4,2) node {8}
    (6.2,2) node {15}
    (6.8,2) node {16}
    (9.6,2) node {23}
    (10.2,2) node {24}
    (13,2) node {31};
    \foreach \j in {0,1,2,...,7}
    {
       \draw (0+\j*0.4,2.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=red!20] {};
       \draw (3.4+\j*0.4,2.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (6.8+\j*0.4,2.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (10.2+\j*0.4,2.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!20] {};
       \draw (0+\j*0.4,2.5) node {I};
       \draw (3.4+\j*0.4,2.5) node {\texttt{D}};
       \draw (6.8+\j*0.4,2.5) node {\texttt{D}};
       \draw (10.2+\j*0.4,2.5) node {\texttt{D}};
    }
    
    \draw (0,2.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=white] {};
    \draw (0.4,2.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=red!80] {};
    \draw (0.8,2.5) node [minimum height=0.4cm,minimum width=0.4cm,draw,fill=black!80] {};
    \draw (0,2.5) node {\texttt{S}};
    \draw (0.4,2.5) node {\textcolor{white}{\texttt{i}}};
    \draw (0.8,2.5) node {\textcolor{white}{\texttt{d}}};
    
    \draw [|-|] (1,3)--node[above=.5mm] {Inodes} (3,3);
    \draw [|-|] (3.2,3)--node[above=0] {Data Region} (13.2,3);
}
\end{figure}

这时候我们发现0号block是空的，它被称为Superblock。什么意思呢？它有整个文件系统的整体信息，包括文件系统有多少inodes、data blocks以及inode table的地址在哪里。所以当挂载这个文件系统的时候，OS会首先去读超级块。

现在我们来聚焦一下前八个blocks，看看里面都有什么：
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{1.png}
\end{figure}


我们之前也说了inode在文件系统中被称为File Control Block(FCB)，是index node的简称。每个inode有256B的内容，那么为了在逻辑上能够更好表示inode，我们给每个inode一个编号，被称为inumber(0\textasciitilde79)。从这个图中我们也可以知道，i-bmap会有80个bits，来表示这0\textasciitilde79的inodes有没有被使用；而d-bmap则有56个bits，因为一共有56个data blocks，来表示对应的data block有没有被使用。

现在还有一个问题，给定一个inumber，怎么去读这个inumber对应的inode里面的metadata呢？

很简单，我们首先找到inode table的首地址，那么在这里就是12KB，假设我们要读inumber=39位置的数据，那么它在磁盘当中的地址就是12KB+39×256B。

那inode里面有什么呢？以Ext2 Inode为例，有一坨东西，最重要的是一共有15个pointers，一个pointer4B，所以总共占据60B的空间，指向disk上的不同位置，也就是指向disk block。

那么，我们来深入分析一下，一个inode如何去找到对应的data blocks呢？有一个很简单的方法，就是用pointers\textbf{直接}去指向data blocks，但这是有问题的，因为15个data blocks也就只有60KB的大小，这对于大型文件肯定是不能接受的。

所以我们的一个solution就是去使用\textbf{间接}指针，什么意思呢？就是用pointer指向一个存满了pointers的data block。打个比方，一个data block 4KB，那么就可以存1024个pointers，再用这些pointers每个指向一个data block，所以说最后可以用一个pointer指向1024个data blocks，1024×4KB=4MB，15个指针就可以指向60MB的空间，这是可以接受的。

那有了这样的一级的非直接指针，就会有二级间接指针(4GB)、三级间接指针(4TB)。或者说，我们用extents(ext4)来代替pointers。
\begin{definition}
    Extent：本质还是一个指针，只是比方说指针指向的是第16个data block，它要额外设置一个长度，假设为6，那么这个pointer或者说这个extent最终指向的就是16，17，18，19，20，21这些blocks。
\end{definition}

这样子看上去还不错，节省一些空间，但是问题在于他不灵活，因为每个extent的length可能是不一样的，而且你只能连续访问blocks，中间可能出现没有内容的情况。

那么在讲完了VSFS之后呢，我们来关注一下其他File System。

\subsubsection{FAT}
这个文件系统将File-Allocation Table作为File Control Block，不支持大文件，但实现起来较为简单。

核心思想：类似链表，通过start block指向下一个block中的内容；在下一个block的最后又会有一个指针去指向再下一个block的内容。这样的话有一个问题，就是如果文件太大，要经过的blocks很多，效率是很低的。又况且访问的是disk，效率更低。

\subsubsection{NTFS}
\textbf{NTFS被经常用在Windows操作系统下。}

核心思想：用MFT(master file table)作为File Control Block，和inode差不多，里面有文件的各种属性，例如文件名、文件的数据等。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{screenshot001}
\end{figure}

我们来看一下这个“表格”，它本质是一个entry，也就是说应该存在一行里面，这里为了方便展示，把它画成了一个表格。

\begin{pt}
Data里面既可以存放小的数据，也可以存放disk pointers。
\end{pt}

这时候我们再回顾一下warm-up的例子，做一个分析：
\begin{table}[h]
\footnotesize
    \centering
    \begin{tabular}{ccc|cc}
        Filename & Content & Description & \textbf{Size} & \textbf{Space} \\
        \hline
        test1 & - & - & 0 & 0 \\
        test2 &  ``This is a test file.$\backslash$r$\backslash$n'' & $\times 1$ (line) & 22 bytes & 0 \\
        test3 &  ``This is a test file.$\backslash$r$\backslash$n'' & $\times 40$ (lines) & 878 bytes & 4 KB \\
        test4 &  ``This is a test file.$\backslash$r$\backslash$n'' & $\times (40-39)$ (lines) & 22 bytes & 4 KB
    \end{tabular}
\end{table}

我们先明确一下，Space在这里指的是占用的data block的大小，和文件大小无关。

首先我们来解决第一个问题，test1虽然没有内容，但是它有文件名，为什么Size和Space都为0呢？因为文件名是存放在MFT里面的Attr.2中间的，就不用占文件的存储。

再来解决一个问题，为什么test2的Space是0呢？因为test2的内容只有22B，远小于1KB，那就可以直接存在MFT的Attr.8里面。

那么为什么到test3的时候Space就是4KB了呢？因为878B已经很接近1KB了，MFT不太够放了，只能给文件分配一个大小为4KB的data block。

最后一个问题，test4的Space和test2的Space为什么不一样呢？是因为data block已经分配出去了，只有文件全部删除完，才能够把那4KB收回来（但是噢，如果我分配的是12KB，我的Size只有3KB了，那多分配的8KB是可以收回来的）。

\begin{exam}
非常中要的三个Allocation Method：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item Contiguous allocation：把文件分配在disk的连续blocks上。（现在已经没有纯的contiguous allocation了，因为这样做的问题太大了，没有那么多连续空间给你）

举例：ext4(Linux)、ntfs(Windows)，注意啊，它们两个本质是indexed allocation和contiguous allocation的结合，做的事情是尽可能连续，但也需要disk pointers去指向data block。
\item Linked allocation：就是链表找文件存放的内容，效果比较糟糕。

举例：fat
\item Indexed allocation：由disk pointer指向data block。

举例：ext2,ext3(Very Simple File System)
\end{enumerate}
\end{exam}

\subsubsection{Directory Organization}
Directory的本质就是一个translation table，里面有很多entry name和inode number的配对，现在我们来举一个例子：
\begin{center}
\normalsize
\begin{tabular}{c c c c}
\multicolumn{1}{c|}{inum}&\multicolumn{1}{c|}{reclen}&\multicolumn{1}{c|}{strlen}&name\\
5&4&2&.\\
2&4&3&..\\
12&4&4&foo\\
13&4&4&bar\\
24&8&7&foobar
\end{tabular}
\end{center}

我们先看一下这张表，strlen表示的是文件名的长度，reclen(record length)一般是2的多少次幂，表示\textbf{entry在目录文件中占的字节数量}。

那么如果我们要删掉一个文件(unlink())，也就是把某个entry删了之后，它占用的空间还是存在的，因此我们需要做一些标记告诉directory这个entry没了，用的方法是把inode number置为0。同时这也是为什么要用reclen的原因，reclen指明了entry的长度，这样在扫描目录的时候可以跳过空的entry，且要是插入一个新的entry时，可以复用空闲的空间。

最后，目录也有一个inode，被存放在inode table里面，但是会对这个inode进行区分，会标记"directory"和"regular file"。

\subsubsection{Access Paths: Reading}
现在呢，我想要打开一个文件，读一下，然后关闭它：
$$open(''/foo/bar'', O\_RDONLY)$$

对于file system来说，它必须遍历整个路径来定位到我要的inode，所以读的过程大致为：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 读根节点的inode
\item 在inode中找到指向data blocks的pointers，里面包含着根目录的内容
\item 找到foo的入口（也就是对应的inum）
\item 一个个文件夹往下找，直到找到目标文件的inode，读取里面的block的内容
\item 上面做的工作本质是在打开文件，那么读取文件还需要记住一个最近的访问时间变了，是需要在inode里面进行修改的
\end{enumerate}

现在我们用一个流程图来理解一下（希望真的能理解吧）：

\begin{center}
\small
\begin{tabular}{c|c c|c c c| c c c c c}
&data&inode&root&foo&bar&root&foo&bar&bar&bar\\
&bitmap&bitmap&inode&inode&inode&data&data&data[0]&data[1]&data[2]\\
\hline
\multirow{5}{*}{$open(bar)$}&&&read&&&&&&&\\
&&&&&&read&&&&\\
&&&&read&&&&&&\\
&&&&&&&read&&&\\
&&&&&read&&&&&\\
\hline
\multirow{3}{*}{$read()$}&&&&&read&&&&&\\
&&&&&&&&read&&\\
&&&&&write&&&&&\\
\hline
\multirow{3}{*}{$read()$}&&&&&read&&&&&\\
&&&&&&&&&read&\\
&&&&&write&&&&&\\
\hline
\multirow{3}{*}{$read()$}&&&&&read&&&&&\\
&&&&&&&&&&read\\
&&&&&write&&&&&\\
\end{tabular}
\end{center}

为什么在read()的过程中还有write呢？是因为最近的访问时间改变了。

\subsubsection{Access Paths: Writing}
写和读就不太一样了，写要更复杂一些，因为写的时候可能要allocate一个block，那我们现在来看看writer()的流程：
\begin{enumerate}[label=（\arabic*）, leftmargin=3em]
\item 首先读data bitmap，找一个0，也就是空的地方
\item 找到这个空的地方，并把这个0写成1
\item 现在又要去inode里面搞一个disk pointer来指向这个新的data block
\item 最后要把东西写到这个data block里面
\end{enumerate}

那么如果说我们现在要创建一个新的文档呢？那就不仅和文件有关了，还和目录有关了：
\begin{enumerate}[]
\item 首先我们要去inode bitmap里面找一个空的inode
\item 找到这个空的inode，将0置为1
\item 初始化inode
\item 链接inode和user能读的名字
\item 对于目录的inode还有一次读写
\end{enumerate}

这个就比较复杂了，下面的流程图看看就行了：

\centering
\footnotesize
\begin{tabular}{c|c c|c c c| c c c c c}
&data&inode&root&foo&bar&root&foo&bar&bar&bar\\
&bitmap&bitmap&inode&inode&inode&data&data&data[0]&data[1]&data[2]\\
\hline
&&&read&&&&&&&\\
&&&&&&read&&&&\\
&&&&read&&&&&&\\
&&&&&&&read&&&\\
$create$&&read&&&&&&&&\\
$(/foo/bar)$&&write&&&&&&&&\\
&&&&&&&write&&&\\
&&&&&read&&&&&\\
&&&&&write&&&&&\\
&&&&write&&&&&&\\
\hline
\multirow{5}{*}{$write()$}&&&&&read&&&&&\\
&read&&&&&&&&&\\
&write&&&&&&&&&\\
&&&&&&&&write&&\\
&&&&&write&&&&&\\
\hline
\multirow{5}{*}{$write()$}&&&&&read&&&&&\\
&read&&&&&&&&&\\
&write&&&&&&&&&\\
&&&&&&&&&write&\\
&&&&&write&&&&&\\
\hline
\multirow{5}{*}{$write()$}&&&&&read&&&&&\\
&read&&&&&&&&&\\
&write&&&&&&&&&\\
&&&&&&&&&&write\\
&&&&&write&&&&&\\
\end{tabular}

\textbf{好的小猪猪，文件系统理论上要考试的内容就到这里啦，不过有时间的话记得往下学喔！}

\newpage
\section{Exercise}
\begin{example}
Imagine a file system which uses inodes to manage files on disk. Each inode consists of a file name (4 bytes), user id (2 bytes), three timestamps (4 bytes each), protection bits (2 bytes), a reference count (2 bytes), a file type (2 bytes), and the file size (4 bytes). Additionally, the inode contains 13 direct indices, 1 index to a single indirect block, 1 index to a double indirect block, and one index to a triple indirect block. Each of these indices (block pointer) is 4 bytes. The file system also stores the first 356 bytes of each file in the inode.
\end{example}
\begin{enumerate}[label=(\arabic*), leftmargin=2em]
\item Three major methods of allocating disk space are introduced in our textbook. What are these three allocation methods? Which one is used in the previous file system?
\item Assume a disk sector is 512 bytes and that each indirect block fills a single sector. What is the maximum file size for this file system? Show your work clearly. You need not do the arithmetic to get full credit.
\item Is there any benefit to including the first 356 bytes of the file in the inode? If so, what is the reason? If not, why not?
\end{enumerate}

\textbf{Keys：}
\begin{enumerate}[label=(\arabic*), leftmargin=2em]
\item Contiguous Allocation、Linked Allocation、Indexed Allocation；Indexed Allocation.
\item $(512/4)^3*512+(512/4)^2*512+(512/4)^1*512+13*512+\textbf{356}$（这里356很容易漏）
\item Yes, Efficiency in both spatial and temporal. Most files are small.（在最开始的时候提到过，说大部分文件都是2KB左右的） For small files (≤356 bytes), do not need to access disk twice. save disk space (internal fragmentation within blocks).
\end{enumerate}

\chapter{IO\_Systems}
\section{In-class Contents}

\begin{flushleft}
\subsection{I/O Hardware}

\begin{enumerate}[label=(\arabic*), leftmargin=3em]
\item Port(端口)：用过端口确定I/O设备是什么
\item Bus:电路
\item Polling:轮询
\item Polling, DMA and interrupt are three main interaction models between I/O controllers and CPUs to accomplish a complete I/O transfer.
\end{enumerate}

\subsection{Application I/O Interface}

\subsection{Kernel I/O Subsystem}

有一些相对重要的概念：

\begin{definition}
I/O Scheduling:优化 I/O 请求的顺序以提升性能（尤其是磁盘 I/O，可以使用更好的调度算法）
\end{definition}
\begin{definition}
Buffering:当数据生产者和消费者速度不一致时（例如 CPU 和慢速磁盘），用缓冲区过渡。网络每次发送 1500 字节，而程序只写 1 字节，缓冲可以合并或拆分数据。
\end{definition}
\begin{definition}
Caching:与缓冲不同：缓存用于重用数据，缓冲用于数据传输中转。
\end{definition}
\begin{definition}
Spooling and Device Reservation:防止设备使用的冲突，比如说打印机同时只能处理一个文档。
\end{definition}
\end{flushleft}


\end{document}
